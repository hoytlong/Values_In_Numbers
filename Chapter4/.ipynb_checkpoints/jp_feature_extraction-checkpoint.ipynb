{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Code for Chapter 4\n",
    "\n",
    "<p style='text-align: justify;'> This notebook is used to extract specific passage-level features from Japanese language corpora used in Chapter 4. In contrast to the previous chapter, this code extracts features across discrete-sized chunks of text. This includes all of the passages designated as stream-of-consciousness passages, as well as all passages across two larger corpora, storing the results as data frames for further analysis in R.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Import libraries\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from __future__ import division\n",
    "import nltk, re, pprint\n",
    "import pandas as pd\n",
    "import operator\n",
    "from pandas import Series, DataFrame\n",
    "from scipy import stats\n",
    "from numpy.random import permutation, shuffle\n",
    "import string\n",
    "\n",
    "import sys, os\n",
    "import MeCab  #CHECK \"MECABRC\" FILE TO SEE WHICH DICTIONARY YOU ARE USING\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import word lists for use in feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#import stopwords list\n",
    "#######################\n",
    "raw = open(\"VocabLists\\jp_stopwords.txt\", encoding=\"utf-8\")\n",
    "text = raw.read()\n",
    "stopwords = text.split('\\n')  #split on newline and turn into a list\n",
    "\n",
    "##############################\n",
    "#build onomatopoeia word list\n",
    "##############################\n",
    "#grab the tags for these words from JEdict, build a list, and then do a simple check\n",
    "#grab the xml file, and specify which tag you want\n",
    "# context = etree.iterparse('c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT_JAPAN\\JMdict_e.xml', events=('end',))#, tag='entry')\n",
    "# #open a text file for writing\n",
    "# out = open(r'c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT_JAPAN\\onom.txt', 'w')\n",
    "# #iterate through the tree, taking only the elements you need, and deleting everything else along the way\n",
    "# for event, elem in context:\n",
    "#     if elem.tag == 'reb':\n",
    "#         word = elem.text\n",
    "#     if elem.tag == 'misc' and elem.text[0:4] == 'onom':\n",
    "#         out.write('%s\\n' % word.encode('utf-8'))\n",
    "#     elem.clear()\n",
    "#     #while elem.getprevious() is not None:\n",
    "#     #    del elem.getparent()[0]\n",
    "# out.close()\n",
    "\n",
    "#import the onomatopoeia list from already created text file\n",
    "raw = open(r\"VocabLists\\onom.txt\", encoding=\"utf-8\")\n",
    "text = raw.read()\n",
    "onom_list = text.split('\\n')  #split on newline and turn into a list\n",
    "onom_list = list(set(onom_list))\n",
    "#sort the list for efficient lookup\n",
    "onom_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to normalize quotation marks\n",
    "def normalize_quotation(text):\n",
    "    text = re.sub(r'『', r'「', text)   #replace all 『 with 「\n",
    "    text = re.sub(r'』', r'」', text)   #replace all 』 with 」\n",
    "    return text\n",
    "\n",
    "def bracket_cleaner(raw):\n",
    "    raw = re.sub(r'［[^］]+］', '', raw)   #replace annotations in brackets ([#...])\n",
    "    raw = re.sub(r'\\s+', ' ', raw)                         #get rid of double spaces\n",
    "    return raw\n",
    "\n",
    "#list of punctuations marks to exclude if needed\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','-','─','<',\n",
    "         '＃','△','※','＊','〔','〕']\n",
    "\n",
    "#remove punctuation marks from text with word boundaries indicated\n",
    "def remove_punc(text):\n",
    "    for punc in puncs:\n",
    "        text = re.sub(punc, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)    #get rid of double spaces\n",
    "    return text\n",
    "\n",
    "#remove stopwords from text with word boundaries indicated; return in same form\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = text.split(' ')\n",
    "    new_list = [token for token in tokens if token not in stopwords]\n",
    "    return ' '.join(new_list)\n",
    "\n",
    "#basic type/token ratio using all words in chunk\n",
    "def tt_ratio(chunk):\n",
    "    chunk = remove_punc(chunk)   #gets rid of punctuation and numbers\n",
    "    chunk = chunk.split(' ')\n",
    "    return len(set(chunk))/len(chunk)   #compute TTR (unique types over all word tokens)\n",
    "\n",
    "#returns type/token ratio without stopwords\n",
    "def tt_ratio_no_stopwords(chunk):\n",
    "    chunk = remove_punc(chunk)\n",
    "    chunk = remove_stopwords(chunk, stopwords)\n",
    "    chunk = chunk.split(' ')\n",
    "    return len(set(chunk))/len(chunk)\n",
    "\n",
    "#returns type/token ratio without proper nouns or stopwords (as determined by MeCab tags)\n",
    "#takes a chunk that has already been pos-tagged\n",
    "def tt_ratio_no_pn(pos_chunk):\n",
    "    new_text = []\n",
    "    #extract list of tokens from chunk, excluding proper nouns\n",
    "    for sent in pos_chunk:    \n",
    "        #inspect each element\n",
    "        for item in sent[:-1]:             #exclude last element, which is EOS\n",
    "            if item[2] != \"固有名詞\":\n",
    "                new_text.append(item[0])   #only keep items that are not proper nouns\n",
    "    new_text = ' '.join(new_text)\n",
    "    chunk = remove_punc(new_text)\n",
    "    chunk = remove_stopwords(chunk, stopwords)\n",
    "    chunk = chunk.split(' ')\n",
    "    return len(set(chunk))/len(chunk)\n",
    "\n",
    "#identify onomatopoeia in chunk based on Jedict list\n",
    "def onom(chunk):\n",
    "    onom_in_chunk = 0\n",
    "    total_words = 0\n",
    "    chunk = remove_punc(chunk)\n",
    "    chunk = chunk.split(' ')\n",
    "    for word in chunk:\n",
    "        if bi_contains(onom_list, word) == True:\n",
    "            onom_in_chunk += 1\n",
    "        total_words += 1\n",
    "    if total_words == 0:    #in case we get a dud sentence\n",
    "        return 0.0\n",
    "    else:\n",
    "        return onom_in_chunk/total_words\n",
    "\n",
    "#function to return median sentence length of chunk\n",
    "def median_sent_length(chunk, genre):\n",
    "    chunk = re.sub(r'\\s', '', chunk)\n",
    "    #chunk_sents = re.findall(r'([^！？。(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)', chunk)\n",
    "    #chunk_sents = re.findall(r'([^！？。」(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*(」[^をと])*)', chunk)\n",
    "    chunk_sents = re.findall(r'((「)*[^！？。(――)(——)\\(\\)]+([^」{5,}?]」[^とを])*(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」\\\n",
    "    と[^。]*)*[！？。」(……)]*)', chunk) \n",
    "    if genre != \"SOC\":\n",
    "        chunk_sents = chunk_sents[1:-1]    #first and last sentences are likely fragments\n",
    "    sent_length = []\n",
    "    for sent in chunk_sents:\n",
    "        sent_length.append(len(sent[0]))  #just inspect first element in each tokenized sentence\n",
    "    return np.median(sent_length)\n",
    "\n",
    "#function to find ellipses in passage\n",
    "def find_ellip(chunk):\n",
    "    count = len(re.findall(r\"…\", chunk))\n",
    "    if len(chunk) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return count/len(chunk)    #ratio expressing # of ellipses as function of length of passage\n",
    "\n",
    "#function to find potential neologisms, here defined as any word appearing in katakana or english\n",
    "def neo(chunk):\n",
    "    neologisms_in_chunk = []\n",
    "    chunk = remove_punc(chunk)\n",
    "    chunk = chunk.split(' ')\n",
    "    for word in chunk:\n",
    "        if len(word) > 2:   #limit to longer words, to avoid things like サ、ホラ、ヶ\n",
    "            if re.search(r'[゠-ヿＡ-Ｚａ-ｚA-Za-z]', word):\n",
    "                neologisms_in_chunk.append(word)\n",
    "    #return ratio of unique katakana and romaji words over all unique words\n",
    "    #this avoids over-counting character names (esp. in translated texts)\n",
    "    return len(set(neologisms_in_chunk))/len(set(chunk))\n",
    "\n",
    "#this function returns pos-tagged chunks as a list of sentence-level triples (the word + first two POS tags) \n",
    "def pos_tagger(chunk):\n",
    "    chunk_sents = []\n",
    "    #segment the chunk into sentences\n",
    "    chunk = re.sub(r'\\s', '', chunk)\n",
    "    #sent_list = re.findall(r'([^！？。(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)', chunk)\n",
    "    #sent_list = re.findall(r'([^！？。」(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*(」[^をと])*)', chunk)\n",
    "    sent_list = re.findall(r'((「)*[^！？。(――)(——)\\(\\)]+([^」{5,}?]」[^とを])*(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」\\\n",
    "    と[^。]*)*[！？。」(……)]*)', chunk)\n",
    "    \n",
    "    #now pos tag by sentence and store as triples\n",
    "    for i in range(len(sent_list)):\n",
    "        sent_triples = []\n",
    "        sent = sent_list[i][0]  #grab sentence from list\n",
    "        sent = re.sub(r'[\\(\\)]', '', sent)      #eliminate parantheses\n",
    "        sent = re.sub(r'…', '', sent)           #eliminate ellipses\n",
    "        sent = re.sub(r'[「」〔〕]', '', sent)   #eliminate quotation marks\n",
    "        node = mecab.parseToNode(sent)          #parse the sentence\n",
    "        node = node.next\n",
    "        while node:\n",
    "            pos_tags = node.feature.split(',')  #turn string of features into a list\n",
    "            #append the triples for each element of this chunk \n",
    "            sent_triples.append([node.surface, pos_tags[0], pos_tags[1]])\n",
    "            node = node.next\n",
    "        chunk_sents.append(sent_triples)\n",
    "    return chunk_sents\n",
    "\n",
    "#returns the percentage of sentences in chunk that end with noun\n",
    "def noun_ending(pos_chunk, genre):\n",
    "    if genre != \"SOC\":\n",
    "        pos_chunk = pos_chunk[1:-1]  #eliminate first and last items, since these are likely fragments\n",
    "    \n",
    "    #a list of sentence endings that are marked as nouns, but are actually grammatical function words\n",
    "    exception_list = [\"候\", \"つた\", \"ところ\", \"もの\", \"サ\", \"ナア\", \"こと\", \"事\", \"時\", \"とき\", \"けた\", \"云々\", \"だい\"]\n",
    "\n",
    "    noun_endings = 0   #counter for noun_ending sentences\n",
    "    total_sents = 0    #counter for total sentences\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:           #don't look at obvious non-sentences\n",
    "            tag_index = -1          #we want to start from the end of the sentence\n",
    "            final_tag = sent[tag_index]   \n",
    "            #while tag is EOS or punctuation..need to look at third item in triple to check for this\n",
    "            while (final_tag[1] == 'BOS/EOS' or final_tag[2] == '句点') and abs(tag_index) != len(sent):\n",
    "                tag_index = tag_index - 1                                          #step back one element\n",
    "                final_tag = sent[tag_index]\n",
    "            if final_tag[1] == '名詞':                                 #keep track of nouns\n",
    "                if final_tag[0] not in exception_list:       #make sure word is not an exception\n",
    "                    noun_endings += 1\n",
    "            total_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return noun_endings/total_sents               #normalize \n",
    "    \n",
    "#function to check for verbless sentences    \n",
    "def verbless_sents(pos_chunk, genre):\n",
    "    if genre != \"SOC\":\n",
    "        pos_chunk = pos_chunk[1:-1]  #eliminate first and last items, since these are likely fragments\n",
    "    \n",
    "    verbless_sents = 0\n",
    "    total_sents = 0\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:          #don't bother with non-sentences\n",
    "            verb_counter = 0\n",
    "            for tag in sent:\n",
    "                if tag[1] == '動詞':\n",
    "                    verb_counter += 1\n",
    "            if verb_counter == 0:\n",
    "                verbless_sents += 1\n",
    "            total_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return verbless_sents/total_sents\n",
    "\n",
    "#returns ratio of personal pronouns (daimeishi) per sentence, averaged over entire chunk\n",
    "def per_pronoun_use(pos_chunk, genre):\n",
    "    if genre != \"SOC\":\n",
    "        pos_chunk = pos_chunk[1:-1]  #eliminate first and last items, since these are likely fragments\n",
    "\n",
    "    sent_ratios = []\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:\n",
    "            per_pronouns = 0\n",
    "            total_tags = 0\n",
    "            for tag in sent:\n",
    "                if tag[1] == \"代名詞\" or tag[0] == \"自分\":\n",
    "                    per_pronouns += 1   \n",
    "                #don't include punctuation in your tag totals\n",
    "                if tag[1] != \"BOS/EOS\" and tag[1] != \"補助記号\":\n",
    "                    total_tags += 1\n",
    "            if total_tags != 0:\n",
    "                sent_ratios.append(per_pronouns/total_tags)       \n",
    "    if np.isnan(np.mean(sent_ratios)):\n",
    "        #print(\"here\")\n",
    "        #print(pos_chunk)\n",
    "        return 0.0\n",
    "    elif np.mean(sent_ratios) != 0:\n",
    "        return np.mean(sent_ratios)          \n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "#function that calculates number of sentences per chunk starting with person pronoun (daimeishi)\n",
    "def per_pronoun_head(pos_chunk, genre):\n",
    "    if genre != \"SOC\":\n",
    "        pos_chunk = pos_chunk[1:-1]  #eliminate first and last items, since these are likely fragments\n",
    "\n",
    "    prp_sents = 0\n",
    "    total_sents = 0\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:\n",
    "            tag_index = 0\n",
    "            #while first tag not punct\n",
    "            while sent[tag_index][1] == \"補助記号\":\n",
    "                tag_index += 1   #step forward one element\n",
    "            total_sents += 1\n",
    "            if sent[tag_index][1] == \"代名詞\" or sent[tag_index][0] == \"自分\":\n",
    "                prp_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return prp_sents/total_sents    \n",
    "\n",
    "#function to identify likely FreeIndirectDiscourse passages, or interior monologue \n",
    "#takes a chunk that has already been tokenized (spaces inserted)\n",
    "def fid_finder(chunk):\n",
    "    #create a list of all likely FID sentence ending phrases\n",
    "    fid_phrases = [\"ない。\",\"れ。\",\"め。\",\"ろ。\",\"け。\",\"よ。\",\"か。\",\"な。\",\"のだ。\",\"なんだ。\",\"んです。\",\"んだ。\",\"のだった。\",\n",
    "               \"何だ。\",\"ところだ。\",\"べきだ。\",\"からだ。\",\"のだから。\",\"ものだ。\",\"のでしょう。\",\"らう。\",\"ろう。\",\"のです。\",\"い。\",\n",
    "               \"所だ。\",\"筈だ。\",\"はずだ。\",\"筈です。\",\"はずです。\",\"ことだ。\",\"らしい。\",\"忘れぬ。\",\"にちがいなかった。\",\"わ。\",\n",
    "               \"に違いなかった。\",\"ことにした。\",\"らしかった。\",\"ながら。\",\"かしら。\",\"らうか。\",\"ろうか。\",\"気もする。\",\"たら。\",\n",
    "               \"てみる。\",\"のやうに。\",\"のように。\",\"かもしれません。\",\"かも知れません。\",\"心地もする。\"]\n",
    "\n",
    "    chunk = re.sub(r'\\s', '', chunk)                     #de-tokenize the chunk so we can search for strings later\n",
    "    chunk = normalize_quotation(chunk)\n",
    "    chunk = re.sub(r'「[^」]*」', '', chunk)         #eliminate all dialogue passages\n",
    "    \n",
    "    #chunk = re.sub(r'[\\(\\)〔〕]', '', chunk)          #eliminate all parantheses and brackets (usually indicate inter mono)              \n",
    "\n",
    "    #sent tokenize\n",
    "    #chunk_sents = re.findall(r'([^！？。(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)', chunk)    \n",
    "    #chunk_sents = re.findall(r'([^！？。」(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*(」[^をと])*)', chunk)\n",
    "    chunk_sents = re.findall(r'((「)*[^！？。(――)(——)\\(\\)]+([^」{5,}?]」[^とを])*(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」\\\n",
    "    と[^。]*)*[！？。」(……)]*)', chunk)\n",
    "    \n",
    "    total_sents = len(chunk_sents)\n",
    "    \n",
    "    num_fid_sents = 0  #initialize counter\n",
    "\n",
    "    for sent in chunk_sents[:-1]:   #check all but the last sentence (might be a fragment for non-SOC texts)\n",
    "        #first check for sentences ending with quotation or exclamation\n",
    "        if sent[0][-1:] == '?' or sent[0][-1:] == '！':\n",
    "            num_fid_sents += 1\n",
    "        #now check for all possible FID strings and adjust counter if any are found\n",
    "        if any(substring in sent[0] for substring in fid_phrases):\n",
    "            num_fid_sents +=1\n",
    "        \n",
    "    #check last sentence if chunk has content in it\n",
    "    if len(chunk) != 0:\n",
    "        if chunk_sents[-1][0][-1:] == '。':   #check last item in sentence\n",
    "            if any(substring in chunk_sents[-1][0] for substring in fid_phrases):\n",
    "                num_fid_sents += 1\n",
    "            else:\n",
    "                total_sents = total_sents - 1   #last sentence is fragment, so discount from total num of sentences\n",
    "            \n",
    "    #calculate and return the ratio of FID sentences vs. total number of non-dialogue sentences\n",
    "    if total_sents == 0:    #in case we get a dud sentence\n",
    "        return 0.0\n",
    "    else:\n",
    "        return num_fid_sents/total_sents\n",
    "    \n",
    "#an efficient way to look up strings in a large list\n",
    "from bisect import bisect_left\n",
    "#the list here needs to be sorted\n",
    "def bi_contains(lst, item):\n",
    "    \"\"\" efficient `item in lst` for sorted lists \"\"\"\n",
    "    # if item is larger than the last its not in the list, but the bisect would \n",
    "    # find `len(lst)` as the index to insert, so check that first. Else, if the \n",
    "    # item is in the list then it has to be at index bisect_left(lst, item)\n",
    "    return (item <= lst[-1]) and (lst[bisect_left(lst, item)] == item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prepare Data Frames to Analyze all Text Chunks for a given Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in corpus information for SOC texts and store as DataFrame\n",
    "df = pd.read_excel(r'Data\\SOC_TEXTS_METADATA.xlsx', sheetname='Sheet1')\n",
    "df = df.dropna(subset=['WORK_ID'])   #drop any empty rows at end of csv\n",
    "df['chunk'] = Series('',index=df.index)    #add column for text chunks\n",
    "df['chunk_id'] = Series('1',index=df.index)    #not needed, but keeping for consistency with realism corpus\n",
    "#Label SOC as the \"0\" class -- all other classes will be labeled \"1\"\n",
    "df['class_labels'] = Series('0',index=df.index)\n",
    "\n",
    "#generate filepaths for each text we have and load in the texts\n",
    "corpus_path = \"Texts\\\\\"\n",
    "\n",
    "for i in df.index:\n",
    "    filepath = corpus_path + str(df.WORK_ID[i]) + \".txt\"  #assign filepath\n",
    "    text = open(filepath, encoding=\"utf-8\")            \n",
    "    raw = text.read()\n",
    "    raw = re.sub(r'[\\u3000\\ufeff]', '', raw)\n",
    "    #now tokenize the chunk and store as tokenized text\n",
    "    node = mecab.parseToNode(raw)\n",
    "    node = node.next\n",
    "    tokens = []\n",
    "    while node:\n",
    "        tokens.append(node.surface)\n",
    "        node = node.next\n",
    "    df.at[i, 'chunk'] = ' '.join([t for t in tokens])   #store tokenized text as tokens joined by space\n",
    "\n",
    "#drop the columns we don't need\n",
    "df.drop(['TRANS', 'SOURCE', 'NATIONALITY','PUBLISHER'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'可視 的 な もの の 避け 難き 形態 。 それ 以上 で ない に し て も 、 少く とも さう 俺 の 眼 を 通し て 考へ た 。 俺 が 此 處 で 讀む ところ の 總て の もの の 署名 、 海 の 產物 、 海 の 漂流 物 、 近よる 上げ潮 、 あの 錆び た ボオト 。 靑洟 色 、 靑 い 銀色 、 錆 。 彩ら れ た サイン 。 透明 なる もの の 限界 。 然し 彼 は つけ 加へる 、 肉 體 に 於 て 。 で は 彼 は 彩ら れる 前 に それ 等 肉 體 を 知 つ て ゐ た の だ 。 如何に し て ？ 彼 の 頭 を それ 等 に 打付ける こと に よつ て 確か に 。 氣 安く 進む が よい 。 禿頭 で そして 百 萬 長者 で 彼 は あつ た 、 顏 の 識れ た 例 の 先生 。 透明 な もの の 限界 が 來る 。 なぜ 來る か ？ 透明 な 透明 に 。 若し 君 が 五 本 の 指 を 通す こと が 出來る なら 、 それ は 戶 で ない に し て も 、 確か に 門 だ 。 眼 を 閉 ぢ て 、 そして 見ろ 。 ステ イヴン は 彼 の 靴 が 漂流 物 と 貝 殼 を 踏み 碎く 音 を 聞く ため に 眼 を 閉 ぢ た 。 君 は 兎に角 、 それ を 步き とほし て ゐる 俺 は 一 度 に 一 跨ぎ だ 。 空間 の 極く 短い 間 を 通し て の 時間 の 極く 短い 間 を 。 五、六 。 順繰り に 。 正確 に 。 そして それ が 聞える ところ の もの の 避く べから ざる 形態 な の だ 。 眼 を 開け 。 否 。 イエス よ 。 若し 俺 が 海洋 の 上 に 突き出し て ゐる 所 の 斷崖 から 落ちる なら ば 、 不可避 的 に 相 並ん で だ 。 俺 は 闇 の 中 を うまく 進ん で ゆく 。 俺 の アツシユ の 劍 は 俺 の 腰 に 下 つ て ゐる 。 それ で 叩け 。 それ で 宜い 。 靴 を 穿い た 俺 の 二 つ の 足 は 相 並ん で 脚 の 端 に ある 。 創造 の 神 の 槌 に よつ て 造ら れ た 、 この 堅固 な 響 。 俺 は サンデイ マウント の 濱 に 沿 ふて 永遠 の 中 に 步 み 入らう と し て ゐる の か 。 クラツシユ 、 クラツク 、 クリツク 、 クリツク 。 荒々しい 海 の 貨幣 デイズイ 校長 は それ を みな 知 つ て ゐる 。 サンデイ マウント へ 行か ない か 、 牝馬 の マデラ イン よ 。 そら 律動 が 始まる 。 俺 に 聞える 。 短長 格 の 不完 な 四 音 步句 が 進軍 し て ゐる 。 いや 、 ギヤロツプ で 、 「 牝馬 の マデラ イン 」 よ 。 さあ お前 の 眼 を 開け 。 俺 は さう する 。 ちよつと 待て 。 それ 以來 總て は 消滅 し た か 。 若し 俺 が 眼 を 開け て 、 永久 に 透明 な 暗 黑 の 中 に 居る と し たら もう 澤山 だ 。 見える か どう か 試し て みよう 。 さあ 見ろ 。 お前 が 無く て も 、 何時 も 在り 、 永久 に 在る で あらう 、 終り ない 世界 は 。 彼女 等 は リイイ の テラス から の 階段 を 用心 し ながら 降り て 來 た 、 女 ども が 、 そして 段々 に なつ た 岸 を 降り て 來 た 、 彼女 等 の 飛沫 の かか つた 足 は 泥 だらけ の 砂 の 中 に だらしなく 埋り ながら 。 俺 の やう に 、 オオルヂイ の やう に 、 我々 の カ 强 い 母 の ところ へ 降り て 來 ながら 。 第 一 號 は 彼女 の 產婆 用 の 鞄 を 重々しく 振りまはし た 。 もう 一人 の 女 の 傘 は 濱 を 突 剌 し た 。 營業 所 から の 一日 の 外出 。 フロ オレンス • マツケイブ 夫人 、 ブライド • ストリイト の 、 深く 追悼 さ れ たる 故 パツク • マツケイブ 氏 の 未亡人 。 彼女 の 同 業 者 の 一人 が 、 悲鳴 を 擧げる 俺 を 、 生命 に まで 無理矢理 に 連れ込ん だ 。 虚無 から の 創造 。 彼女 は 鞄 の 中 に 何 を 持つ て ゐる か 。 赤い 木綿 に 包ん だ 、 蜒蜿 たる 臍 緖 を 持つ た 生れ 損 ひ 。 背後 へ の あらゆる 連鎖 を なす 紐 、 總て の 肉 體 の 沿岸 を 繫ぎ 絡める ケイブル 。 これ が 神秘 な 修道 院 の 存在 する 理由 だ 。 汝 等 は 神 と し て 在る つもり か 。 汝 等 の 臍 を 凝視 せよ 。 おおい 、 キンチ 、 此 處 だ 。 イイデンヴイル へ 俺 を 行かし て くれ 。 アレフ 、 アルフア 、 零 、 零 、 一 。 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[75].chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7497, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#create dataframe for JUNBUNGAKU, INOVEL, and POPULAR corpora chunks\n",
    "#####################################################################\n",
    "\n",
    "temp_df = pd.read_excel(r'Data\\ALL_TEXTS_METADATA.xlsx', sheetname='Sheet1')\n",
    "temp_df = temp_df.dropna(subset=['WORK_ID'])   #drop any empty rows at end of spreadsheet\n",
    "#temp_df['num_chunks'] = Series('',index=temp_df.index)   #add column to store number of text chunks\n",
    "\n",
    "temp_df.shape\n",
    "\n",
    "#set your chunk_length (in characters)\n",
    "chunk_length = 1500\n",
    "\n",
    "#path where all files are kept\n",
    "corpus_path = r\"C:\\Users\\Hoyt\\Dropbox\\JapanCorpusTokenized\\\\\"\n",
    "\n",
    "k = 0\n",
    "    \n",
    "#iterate through and build a df for each text that contains all of the text chunks\n",
    "for i in temp_df.index:\n",
    "    #need to read in file, get number of chunks, and store that information first\n",
    "    filepath = corpus_path + str(temp_df.WORK_ID[i]) + \".txt\"\n",
    "    raw = open(filepath, encoding=\"utf-8\")\n",
    "    text = raw.read()         \n",
    "    text = re.sub(r'[\\u3000\\ufeff]', '', text)\n",
    "    text = normalize_quotation(text)\n",
    "    text = bracket_cleaner(text)\n",
    "    text = re.sub(r'」 「', '」 。 「', text)  #insert period for back-to-back quotes to help with sent tokenization\n",
    "    num_chunks = int(len(text)/chunk_length)   #calculate the number of chunks\n",
    "    text = text[:(chunk_length*num_chunks)]     #get rid of trailing text at the end\n",
    "        \n",
    "    #make lists for metadata items\n",
    "    work_id = [temp_df.WORK_ID[i]]*num_chunks  \n",
    "    title = [temp_df.TITLE[i]]*num_chunks\n",
    "    auth_last = [temp_df.AUTH_LAST[i]]*num_chunks\n",
    "    auth_first = [temp_df.AUTH_FIRST[i]]*num_chunks\n",
    "    publ_date = [temp_df.PUBL_DATE[i]]*num_chunks\n",
    "    genre = [temp_df.GENRE[i]]*num_chunks\n",
    "    chunk_ids = range(num_chunks)\n",
    "    class_labels = [1]*num_chunks\n",
    "    chunk_list = []       #initiate a master list to store novel chunks\n",
    "        \n",
    "    #slice and dice and tokenize!\n",
    "    for j in range(num_chunks):\n",
    "        text_chunk = text[(j*chunk_length):((j+1)*chunk_length)]\n",
    "        #now tokenize the chunk and store as tokenized text\n",
    "        node = mecab.parseToNode(text_chunk)\n",
    "        node = node.next\n",
    "        tokens = []\n",
    "        while node:\n",
    "            tokens.append(node.surface)\n",
    "            node = node.next\n",
    "        tokenized_chunk = ' '.join([t for t in tokens])\n",
    "        chunk_list.append(tokenized_chunk)         #add to master list\n",
    "        \n",
    "    #now put all these lists into a dictionary and create a data frame\n",
    "    data_f = {'WORK_ID': work_id, 'TITLE': title, 'AUTH_LAST': auth_last, 'AUTH_FIRST': auth_first,\n",
    "            'PUBL_DATE': publ_date, 'GENRE': genre, 'chunk': chunk_list, 'chunk_id': chunk_ids, 'class_labels': class_labels}\n",
    "    chunked_df = pd.DataFrame(data_f, columns=data_f.keys())\n",
    "        \n",
    "    #if first time through loop, then initialize a master data frame, else merge with the master\n",
    "    if k == 0:\n",
    "        chunked_texts = chunked_df\n",
    "    else:\n",
    "        chunked_texts = pd.concat([chunked_texts, chunked_df], ignore_index=True)\n",
    "    k+=1    \n",
    "\n",
    "#clean any empty or null chunks\n",
    "mask = chunked_texts[\"chunk\"].isin([''])\n",
    "chunked_texts = chunked_texts[~mask]\n",
    "\n",
    "mask = chunked_texts[\"chunk\"].isnull()\n",
    "chunked_texts = chunked_texts[~mask]\n",
    "\n",
    "#merge these chunked texts with the SOC chunks\n",
    "all_df = pd.concat([df, chunked_texts], ignore_index=True)\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32626, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################\n",
    "#create dataframe for 1925-1940 Fiction corpus\n",
    "##############################################\n",
    "\n",
    "fic_df = pd.read_excel(r'Data\\ALL_FIC_METADATA.xlsx', sheetname='Sheet1')\n",
    "fic_df = fic_df.dropna(subset=['WORK_ID'])   #drop any empty rows at end of spreadsheet\n",
    "#temp_df['num_chunks'] = Series('',index=temp_df.index)   #add column to store number of text chunks\n",
    "\n",
    "fic_df.shape\n",
    "\n",
    "#set your chunk_length (in characters)\n",
    "chunk_length = 1500\n",
    "\n",
    "#path where all files are kept\n",
    "corpus_path = r\"C:\\Users\\Hoyt\\Dropbox\\JapanCorpusTokenized\\\\\"\n",
    "\n",
    "k = 0\n",
    "    \n",
    "#iterate through and build a df for each text that contains all of the text chunks\n",
    "for i in fic_df.index:\n",
    "    #need to read in file, get number of chunks, and store that information first\n",
    "    filepath = corpus_path + str(fic_df.WORK_ID[i]) + \".txt\"\n",
    "    raw = open(filepath, encoding=\"utf-8\")\n",
    "    text = raw.read()         \n",
    "    text = re.sub(r'[\\u3000\\ufeff]', '', text)\n",
    "    text = normalize_quotation(text)\n",
    "    text = bracket_cleaner(text)\n",
    "    text = re.sub(r'」 「', '」 。 「', text)  #insert period for back-to-back quotes to help with sent tokenization\n",
    "    num_chunks = int(len(text)/chunk_length)   #calculate the number of chunks\n",
    "    text = text[:(chunk_length*num_chunks)]     #get rid of trailing text at the end\n",
    "        \n",
    "    #make lists for metadata items\n",
    "    work_id = [fic_df.WORK_ID[i]]*num_chunks  \n",
    "    title = [fic_df.TITLE[i]]*num_chunks\n",
    "    auth_last = [fic_df.AUTH_LAST[i]]*num_chunks\n",
    "    auth_first = [fic_df.AUTH_FIRST[i]]*num_chunks\n",
    "    publ_date = [fic_df.PUBL_DATE[i]]*num_chunks\n",
    "    genre = [fic_df.GENRE[i]]*num_chunks\n",
    "    chunk_ids = range(num_chunks)\n",
    "    class_labels = [1]*num_chunks\n",
    "    chunk_list = []       #initiate a master list to store novel chunks\n",
    "        \n",
    "    #slice and dice and tokenize!\n",
    "    for j in range(num_chunks):\n",
    "        text_chunk = text[(j*chunk_length):((j+1)*chunk_length)]\n",
    "        #now tokenize the chunk and store as tokenized text\n",
    "        node = mecab.parseToNode(text_chunk)\n",
    "        node = node.next\n",
    "        tokens = []\n",
    "        while node:\n",
    "            tokens.append(node.surface)\n",
    "            node = node.next\n",
    "        tokenized_chunk = ' '.join([t for t in tokens])\n",
    "        chunk_list.append(tokenized_chunk)         #add to master list\n",
    "        \n",
    "    #now put all these lists into a dictionary and create a data frame\n",
    "    data_f = {'WORK_ID': work_id, 'TITLE': title, 'AUTH_LAST': auth_last, 'AUTH_FIRST': auth_first,\n",
    "            'PUBL_DATE': publ_date, 'GENRE': genre, 'chunk': chunk_list, 'chunk_id': chunk_ids, 'class_labels': class_labels}\n",
    "    chunked_df = pd.DataFrame(data_f, columns=data_f.keys())\n",
    "        \n",
    "    #if first time through loop, then initialize a master data frame, else merge with the master\n",
    "    if k == 0:\n",
    "        chunked_texts = chunked_df\n",
    "    else:\n",
    "        chunked_texts = pd.concat([chunked_texts, chunked_df], ignore_index=True)\n",
    "    k+=1    \n",
    "\n",
    "#clean any empty or null chunks\n",
    "mask = chunked_texts[\"chunk\"].isin([''])\n",
    "chunked_texts = chunked_texts[~mask]\n",
    "\n",
    "mask = chunked_texts[\"chunk\"].isnull()\n",
    "chunked_texts = chunked_texts[~mask]\n",
    "\n",
    "#merge these chunked texts with the SOC chunks\n",
    "all_df = chunked_texts\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract features from all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#iterate through each chunk in the dataframe and calculate values for all features\n",
    "\n",
    "#Add columns for each feature to be extracted\n",
    "all_df['sent_length'] = Series('',index=all_df.index)\n",
    "all_df['noun_ending'] = Series('',index=all_df.index)\n",
    "all_df['verbless_sents'] = Series('',index=all_df.index)  \n",
    "all_df['per_pronoun_use'] = Series('',index=all_df.index)\n",
    "all_df['per_pronoun_head'] = Series('',index=all_df.index)\n",
    "all_df['tt_ratio'] = Series('',index=all_df.index)\n",
    "all_df['tt_ratio_no_stopwords'] = Series('',index=all_df.index)\n",
    "all_df['tt_ratio_no_pn'] = Series('',index=all_df.index)\n",
    "all_df['onomatopoeia'] = Series('',index=all_df.index)\n",
    "all_df['neologisms'] = Series('',index=all_df.index)\n",
    "all_df['ellipses'] = Series('',index=all_df.index)\n",
    "all_df['fid_ratio'] = Series('',index=all_df.index)\n",
    "\n",
    "#Iterate through all texts and extract features\n",
    "for k in all_df.index:\n",
    "    #create a POS tagged version of the chunk\n",
    "    pos_chunk = pos_tagger(all_df.chunk[k])\n",
    "    \n",
    "    #pass each chunk through all the feature extraction functions\n",
    "    all_df.at[k, 'sent_length'] = median_sent_length(all_df.chunk[k], all_df.GENRE[k])\n",
    "    all_df.at[k, 'noun_ending'] = noun_ending(pos_chunk, all_df.GENRE[k])\n",
    "    all_df.at[k, 'verbless_sents'] = verbless_sents(pos_chunk, all_df.GENRE[k])\n",
    "    all_df.at[k, 'per_pronoun_use'] = per_pronoun_use(pos_chunk, all_df.GENRE[k])\n",
    "    all_df.at[k, 'per_pronoun_head'] = per_pronoun_head(pos_chunk, all_df.GENRE[k])\n",
    "    all_df.at[k, 'tt_ratio'] = tt_ratio(all_df.chunk[k])\n",
    "    all_df.at[k, 'tt_ratio_no_stopwords'] = tt_ratio_no_stopwords(all_df.chunk[k])\n",
    "    all_df.at[k, 'tt_ratio_no_pn'] = tt_ratio_no_pn(pos_chunk)\n",
    "    all_df.at[k, 'onomatopoeia'] = onom(all_df.chunk[k])\n",
    "    all_df.at[k, 'neologisms'] = neo(all_df.chunk[k])\n",
    "    all_df.at[k, 'ellipses'] = find_ellip(all_df.chunk[k])\n",
    "    all_df.at[k, 'fid_ratio'] = fid_finder(all_df.chunk[k])\n",
    "\n",
    "    print(\"Processed \" + str(k+1) + \" of \" + str(len(all_df)) + \" chunks...\", end=\"\\r\")\n",
    "\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#r'((「)*[^！？。(――)(——)\\(\\)]+([^」{5,}?]」[^とを])*(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)'\n",
    "#this does a better job than what you had, but still can't handle quote directly followed by another quote (e.g., 1822)\n",
    "\n",
    "#((「)*[^！？。」(――)(——)\\(\\)]+([^」{5,}?]」[^とを])*(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)\n",
    "#this works for 1822--need to test on others\n",
    "\n",
    "#the solution to this is to replace every instance of back to back quotes with quote, period, quote -- this will make\n",
    "#the sent_tokenizer far more accurate on the whole, and doesn't impact downstream measurements\n",
    "\n",
    "#we should also calculate median sentence length after removing dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'」。「いや、まあお聞きなさい、別れる時に熊谷が少し気の毒になったんで、「今夜は何処へ泊るんだい」ッてそう云うと、「泊る所なんか幾らもあるわよ。あたしこれから横浜へ行くわ」ッて、ちっともショゲてなんかいないで、そのままスタスタ新橋の方へ行くんだそうです。―――」。「横浜と云うのは、誰の所なんです？」。「そいつが奇妙なんですよ、いくらナオミさんが顔が広いッて、横浜なんかに泊る所はないだろうから、ああ云いながら多分大森へ帰ったんだろうと、そう熊谷が思っていると、明くる日の夕方電話が懸って、「エルドラドオで待っているから直ぐ来ないか」と云う訳なんです。それで行って見ると、ナオミさんが目の覚めるような夜会服を着て、孔雀の羽根の扇を持って、頸飾りだの腕環だのをギラギラさせて、西洋人だのいろんな男に囲まれながら、盛んにはしゃいでいるんだそうです」浜田の話を聞いているとあたかもビックリ箱のようで、「おやッ」と思うような事実がピョンピョン跳び出して来るのです。つまりナオミは、最初の晩は西洋人の所へ泊ったらしいのですが、その西洋人はウィリアム・マッカネルとか云う名前で、いつぞや私が始めてナオミとエルドラドオへダンスに行った時、紹介もなしに傍へ寄って来て、無理に彼女と一緒に踊った、あのずうずうしい、お白粉を塗った、にやけた男がそれだったのです。ところが更に驚くことには、―――これは熊谷の観察ですが、―――ナオミはあの晩泊りに行くまで、そのマッカネルと云う男とは何もそれほど懇意な仲ではなかったのだと云うのです。尤もナオミも、前から内々あの男に思し召しがあったらしい。何しろちょっと女好きのする顔だちで、すっきりとした、役者のような所があって、ダンス仲間で「色魔の西洋人」と云う噂があったばかりでなく、ナオミ自身も、「あの西洋人は横顔がいいわね、何処かジョン・バリに似てるじゃないの」―――ジョン・バリと云うのは亜米利加の俳優で、活動写真でお馴染のジョン・バリモーアのことなのです。―――と、そう云っていたくらいだから、確かにあれに眼を着けていたのだ。或はちょいちょい色眼ぐらいは使ったことがあるかも知れない。それでマッカネルの方でも、「此'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chunked_texts.loc[(chunked_texts['TITLE'] == 'Chijin no ai') & (chunked_texts['chunk_id'] == 160)]\n",
    "re.sub(r'\\s', '', chunked_texts.ix[3214].chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export dataframe to excel file for analysis in R\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "all_df.drop(['chunk'], axis=1, inplace=True)\n",
    "writer = pd.ExcelWriter(r'Results\\AllChunkFeatures.xlsx', engine='xlsxwriter')\n",
    "all_df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and Output 1929 \"SOC\" Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in passage metadata\n",
    "soc_1929_df = pd.read_excel(r'Results\\SOC_CHUNKS_1929.xlsx', sheetname='Sheet1')\n",
    "soc_1929_df = soc_1929_df.sort_values(['TITLE', 'chunk_id'], ascending=[True, True])\n",
    "\n",
    "#open file for output\n",
    "f = open(\"1929_SOC_Passages.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "#grab chunks from master data frame and write to file\n",
    "for i in soc_1929_df.index:\n",
    "    title = soc_1929_df.ix[i].TITLE\n",
    "    chunk_id = soc_1929_df.ix[i].chunk_id\n",
    "    score = soc_1929_df.ix[i].score\n",
    "    auth_last = soc_1929_df.ix[i].AUTH_LAST\n",
    "    auth_first = soc_1929_df.ix[i].AUTH_FIRST\n",
    "    \n",
    "    #get the passage\n",
    "    chunk = all_df.loc[(all_df['TITLE'] == title) & (all_df['chunk_id'] == chunk_id)].chunk.iloc[0]\n",
    "    chunk = re.sub(r'\\s','',chunk)\n",
    "    \n",
    "    f.write(title + '(id= ' + str(chunk_id) + '), ' + auth_last + ' ' + auth_first + ': ' + str(score) + '\\n\\n')\n",
    "    f.write(chunk + '\\n\\n\\n')\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
