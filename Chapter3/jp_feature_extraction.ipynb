{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Code for Chapter 3\n",
    "\n",
    "<p style='text-align: justify;'> This notebook is used to extract specific features from Japanese language corpora used in Chapter 3. These include things like the number of pronouns, number of connective words, etc. It also includes basic code for extracting the most distinctive words between two corpora.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys, operator, os, re\n",
    "from shutil import copyfile\n",
    "import collections\n",
    "from collections import Counter\n",
    "from scipy.stats import mannwhitneyu\n",
    "import MeCab  #CHECK \"MECABRC\" FILE TO SEE WHICH DICTIONARY YOU ARE USING\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get tokens from a corpus\n",
    "def get_tokens(dataframe, path):\n",
    "    all_tokens = []\n",
    "    for k in dataframe.index:\n",
    "        #get the tokenized text\n",
    "        source_text = path + str(dataframe.WORK_ID[k]) + \".txt\"\n",
    "        raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "        tokenized = raw_text.read()\n",
    "        tokenized = normalize_quotation(tokenized)\n",
    "        text_tokens = re.split(r'\\s', tokenized)    #split on white space\n",
    "        all_tokens.extend(text_tokens)              #add tokens to cumulative list\n",
    "    return all_tokens\n",
    "\n",
    "#function to normalize quotation marks\n",
    "def normalize_quotation(text):\n",
    "    text = re.sub(r'『', r'「', text)   #replace all 『 with 「\n",
    "    text = re.sub(r'』', r'」', text)   #replace all 』 with 」\n",
    "    return text\n",
    "\n",
    "#list of punctuations marks to exclude if needed\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','-','─','<',\n",
    "         '＃','△','※','＊']\n",
    "\n",
    "#function to clean punctuation marks from text\n",
    "def remove_punc(text):\n",
    "    for punc in puncs:\n",
    "        text = re.sub(punc, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)    #get rid of double spaces\n",
    "    return text\n",
    "\n",
    "#function to compute percentage of text made up of dialogue\n",
    "def percent_dialogue(text):\n",
    "    no_quotes = re.sub(r'「[^」]*」', '', text)   #eliminate all dialogue passages\n",
    "    text_per_diag = (len(text)-len(no_quotes))/len(text)\n",
    "    return text_per_diag\n",
    "\n",
    "#this function calculates the proportion of non-dialogue sentences that contain a first/third person singluar pronoun\n",
    "#must feed it a text that has been tokenized\n",
    "def pronouns(text):\n",
    "    terms = ['私','自分','僕','俺','わたくし','あたし','わたし','己','吾','余','おれ','わし','我輩','吾輩','我','わが',\n",
    "             'ぼく','予','彼','彼女','かれ','彼奴','彼れ']\n",
    "\n",
    "    no_dia = re.sub(r'「[^」]*」', '', text)\n",
    "\n",
    "    #eliminate bigrams that contain 彼 but which are not of interest\n",
    "    no_dia = re.sub(r'彼\\s[ら等方]','',no_dia)\n",
    "    \n",
    "    sents = re.findall(r'([^！？。(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)', no_dia)\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    #get intersection of sentence tokens and the pronoun terms; any intersection means a pronoun is found\n",
    "    for sentence in sents:\n",
    "        sent_tokens = re.split(r'\\s', sentence[0])\n",
    "        if len(list(set(sent_tokens) & set(terms))) > 0:\n",
    "            counter += 1\n",
    "        \n",
    "    return(counter/len(sents))\n",
    "    \n",
    "#this function calculates the proportion of thought/feeling words in all lemma of non-dialogue part of text; \n",
    "#pass to it a text with spaces removed\n",
    "def thought(text):\n",
    "    terms = ['感ずる','考える','心持ち','気分','心配','気持ち','思う']  #define lemma to search for\n",
    "    text = re.sub(r' ','',text) #detokenize\n",
    "    \n",
    "    #strip dialogue\n",
    "    no_dialogue = re.sub(r'「[^」]*」', '', text)\n",
    "\n",
    "    lemma_count = 0\n",
    "    thought_count = 0\n",
    "\n",
    "    #tokenize and check lemma forms\n",
    "    node = mecab.parseToNode(text)\n",
    "    node = node.next\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if len(re.split(r',', node.feature)) > 6:  #some words don't have a lemma form\n",
    "            lemma = re.split(r',', node.feature)[7]\n",
    "            if lemma in terms:\n",
    "                thought_count += 1\n",
    "            lemma_count += 1\n",
    "            node = node.next\n",
    "        else:   #if not, just count the plain token without checking content\n",
    "            lemma_count += 1\n",
    "            node = node.next\n",
    "\n",
    "    return(thought_count/lemma_count)\n",
    "\n",
    "#this function calculates proportion of non-dialogue sentences that begin with one of the conjunctions or connecting words\n",
    "#from Kisaka; pass to it a text with spaces removed\n",
    "def conjuncts(text):\n",
    "    \"\"\"\n",
    "    Finds ratio of coordinating conjunctions （接続詞語彙） in prose portions based on list by Kisaka;\n",
    "    \"\"\"\n",
    "    #items are ordered such that longer overlapping sequences are searched for first\n",
    "    terms = ['さうかと云って','さうしてまた','さうして','さうしたら','さうなると','さればとて','されば','さて','しかし','併し',\n",
    "             '然らば','然しながら','然し','しかも尚','しかも','然るに','すると','するうち','それでなければ','それにもかかわらず',\n",
    "             'それにも拘らず','それにしろ','それにまた','それにしても','それに','それから又','それだから','それから','それゆゑ',\n",
    "             'それ故','それとても','それとも','それでは','それでも','それで','それだのに','それは','それが又','それが','それより',\n",
    "             'そうかと云って','そうしたら','そうなると','そうして','そうすると','其の代り','そして又','そして','而して','そこで',\n",
    "             '従って','それなら','であるのに','だから','ですから','即ち','つまりは','つまり','要するに','けれども','けれど','だが',\n",
    "             'ところが','所で','所が','唯','但し','尤も','もっとも','又','然も','その上','且つ','更には','或は','何故と云へば',\n",
    "             'これから','かうして','斯うして','こうして','猶且','では','ただ','また','と又','又一方'] #'と','で','が']\n",
    "\n",
    "    no_dia = re.sub(r'「[^」]*」', '', text)\n",
    "    sents = re.findall(r'([^！？。(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)', no_dia)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for sentence in sents:\n",
    "        for word in terms:\n",
    "            if re.match(word, sentence[0]):\n",
    "                counter += 1\n",
    "            continue  #since we've found a match, skip to next sentence            \n",
    "    \n",
    "    return(counter/len(sents))\n",
    "    \n",
    "#function to calculate Yule's K, Yule's I, and Guiraud's C; text should be tokenized\n",
    "def get_metrics(s):\n",
    "    \"\"\" \n",
    "    Returns a tuple with Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "    tokens = s.split(' ')\n",
    "    for token in reversed(tokens):  #remove blank spaces\n",
    "        if token == '':\n",
    "            tokens.remove(token)\n",
    "    token_counter = collections.Counter(tokens)\n",
    "    \n",
    "    #calculate Yule's metrics\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    \n",
    "    #calculate Guiraud's lexical concentration metric w/ only words tagged as noun, adjective, verb\n",
    "    noun_tokens = []\n",
    "    node = mecab.parseToNode(tokenized)\n",
    "    node = node.next\n",
    "\n",
    "    while node:\n",
    "        head_tag = re.split(r',', node.feature)[0]\n",
    "        if head_tag == \"名詞\":\n",
    "            noun_tokens.append(node.surface)\n",
    "        \n",
    "        #go to next item\n",
    "        node = node.next\n",
    "\n",
    "    token_counter = collections.Counter(noun_tokens)\n",
    "    sorted_counts = sorted(token_counter.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #use this code if you want to set at 50 for every text\n",
    "    sum_top_50 = sum([item[1] for item in sorted_counts[0:50]])  #sum freqs of top 50 words\n",
    "    C = sum_top_50 / (2 * sum(token_counter.values()))  #divide by 2 * total words\n",
    "    \n",
    "    #use this code if you want the MAX to be a relative percent of vocabulary (e.g. 50%)\n",
    "    #MAX = 10   #start at a reasonably low value\n",
    "    \n",
    "    #while the count of types from 1 to MAX is less than x% of the vocabulary...keep adding to MAX\n",
    "    #while sum([item[1] for item in sorted_counts[0:MAX]]) < (sum(token_counter.values()) * .5):\n",
    "    #    MAX += 1\n",
    "    #C = sum([item[1] for item in sorted_counts[0:MAX]]) / (2 * sum(token_counter.values()))\n",
    "    \n",
    "    return (k, i, C)\n",
    "\n",
    "##############################################\n",
    "# Functions for Most Distinctive Word Analysis\n",
    "##############################################\n",
    "\n",
    "def count_differences(one_tokens, two_tokens):\n",
    "    #calculate total number of tokens\n",
    "    one_N=len(one_tokens)\n",
    "    two_N=len(two_tokens)\n",
    "    \n",
    "    #create holders for word counts\n",
    "    one_counts=Counter()\n",
    "    two_counts=Counter()\n",
    "    \n",
    "    #create empty dictionary for vocab items and count all types\n",
    "    vocab={}\n",
    "    for token in one_tokens:\n",
    "        one_counts[token]+=1\n",
    "        vocab[token]=1\n",
    "        \n",
    "    for token in two_tokens:\n",
    "        two_counts[token]+=1    \n",
    "        vocab[token]=1\n",
    "        \n",
    "    #calculate differences in usage for every vocab item\n",
    "    differences={}\n",
    "    for word in vocab:\n",
    "        freq1=one_counts[word]/one_N\n",
    "        freq2=two_counts[word]/two_N\n",
    "        \n",
    "        diff=freq1-freq2\n",
    "        differences[word]=diff\n",
    "        \n",
    "    return differences\n",
    "\n",
    "def difference_of_proportions(one_tokens, two_tokens):\n",
    "\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    #sort and print most distinctive words\n",
    "    sorted_differences = sorted(differences.items(), key=operator.itemgetter(1))\n",
    "    print (\"More Corpus A:\")\n",
    "    for k,v in reversed(sorted_differences[-30:]):\n",
    "        print (\"%s\\t%s\" % (k,v))\n",
    "    print(\"\\nMore Corpus B:\")\n",
    "    for k,v in sorted_differences[:30]:\n",
    "\n",
    "        print (\"%s\\t%s\" % (k,v))\n",
    "\n",
    "# convert a sequence of tokens into counts for each chunkLength-word window\n",
    "def get_chunk_counts(tokens, chunkLength):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(tokens), chunkLength):\n",
    "            counts=Counter()\n",
    "            for j in range(chunkLength):\n",
    "                if i+j < len(tokens):\n",
    "                    counts[tokens[i+j]]+=1\n",
    "            chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "# calculate mann-whitney test for each word in vocabulary\n",
    "def mann_whitney(one_tokens, two_tokens):\n",
    "\n",
    "    chunkLength=500\n",
    "    one_chunks=get_chunk_counts(one_tokens, chunkLength)\n",
    "    two_chunks=get_chunk_counts(two_tokens, chunkLength)\n",
    "    \n",
    "    # vocab is the union of terms in both sets\n",
    "    vocab={}\n",
    "    \n",
    "    for chunk in one_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    for chunk in two_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    \n",
    "    pvals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        a=[]\n",
    "        b=[]\n",
    "        \n",
    "        # Note a and b can be different lengths (i.e., different sample sizes)\n",
    "        # \n",
    "        # See Mann and Whitney (1947), \"On a Test of Whether one of Two Random \n",
    "        # Variables is Stochastically Larger than the Other\"\n",
    "        # https://projecteuclid.org/download/pdf_1/euclid.aoms/1177730491\n",
    "        \n",
    "        # (This is part of their innovation over the case of equal sample sizes in Wilcoxon 1945)\n",
    "        \n",
    "        for chunk in one_chunks:\n",
    "            a.append(chunk[word])\n",
    "        for chunk in two_chunks:\n",
    "            b.append(chunk[word])\n",
    "\n",
    "        statistic,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "        \n",
    "        # We'll use the p-value as our quantity of interest.  [Note in the normal appproximation\n",
    "        # that Mann-Whitney uses to assess significance for large sample sizes, the significance \n",
    "        # of the raw statistic depends on the number of ties in the data, so the statistic itself\n",
    "        # isn't exactly comparable across different words]\n",
    "        pvals[word]=pval\n",
    "\n",
    "    return pvals\n",
    "    \n",
    "# calculate mann-whitneyfor each word in vocabulary and present the top 10 terms for each group\n",
    "def mann_whitney_analysis(one_tokens, two_tokens):\n",
    "    \n",
    "    pvals=mann_whitney(one_tokens, two_tokens)\n",
    "    \n",
    "    # Mann-Whitney tells us the significance of a term's difference in two groups, but we also \n",
    "    # need the directionality of that difference (whether it's used more by group A or group B. \n",
    "    \n",
    "    # Let's use our difference-in-proportions function above to check the directionality.  \n",
    "    # [Note we could also measure directionality by checking whether the Mann-Whitney statistic\n",
    "    # is greater or less than the mean=len(one_chunks)*len(two_chunks)*0.5.]\n",
    "\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    one_terms={k : pvals[k] for k in pvals if differences[k] <= 0}\n",
    "    two_terms={k : pvals[k] for k in pvals if differences[k] > 0}\n",
    "    \n",
    "    sorted_pvals = sorted(two_terms.items(), key=operator.itemgetter(1))\n",
    "    print(\"More Corpus A:\\n\")\n",
    "    for k,v in sorted_pvals[:30]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n",
    "    print(\"\\nMore Corpus B:\\n\")\n",
    "    sorted_pvals = sorted(one_terms.items(), key=operator.itemgetter(1))\n",
    "    for k,v in sorted_pvals[:30]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n",
    "def get_counts(tokens):\n",
    "    counts=Counter()\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "    return counts\n",
    "\n",
    "def chi_square(one_counts, two_counts, shared_only=False):\n",
    "\n",
    "    one_sum=0.\n",
    "    two_sum=0.\n",
    "    vocab={}\n",
    "    for word in one_counts:\n",
    "        one_sum+=one_counts[word]\n",
    "        vocab[word]=1\n",
    "    for word in two_counts:\n",
    "        vocab[word]=1\n",
    "        two_sum+=two_counts[word]\n",
    "\n",
    "    N=one_sum+two_sum\n",
    "    vals={}\n",
    "    \n",
    "    if shared_only == True:\n",
    "        #only analyze words held in common\n",
    "        for word in vocab:\n",
    "            if word in one_counts and word in two_counts:\n",
    "                O11=one_counts[word]\n",
    "                O12=two_counts[word]\n",
    "                O21=one_sum-one_counts[word]\n",
    "                O22=two_sum-two_counts[word]\n",
    "        \n",
    "                # We'll use the simpler form given in Manning and Schuetze (1999) \n",
    "                # for 2x2 contingency tables: \n",
    "                # https://nlp.stanford.edu/fsnlp/promo/colloc.pdf, equation 5.7\n",
    "        \n",
    "                vals[word]=(N*(O11*O22 - O12*O21)**2)/((O11+O12)*(O11+O21)*(O12+O22)*(O21+O22))\n",
    "    else:\n",
    "        for word in vocab:\n",
    "            O11=one_counts[word]\n",
    "            O12=two_counts[word]\n",
    "            O21=one_sum-one_counts[word]\n",
    "            O22=two_sum-two_counts[word]\n",
    "        \n",
    "            # We'll use the simpler form given in Manning and Schuetze (1999) \n",
    "            # for 2x2 contingency tables: \n",
    "            # https://nlp.stanford.edu/fsnlp/promo/colloc.pdf, equation 5.7\n",
    "        \n",
    "            vals[word]=(N*(O11*O22 - O12*O21)**2)/((O11+O12)*(O11+O21)*(O12+O22)*(O21+O22))\n",
    "        \n",
    "    sorted_chi = sorted(vals.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    one=[]\n",
    "    two=[]\n",
    "    for k,v in sorted_chi:\n",
    "        if one_counts[k]/one_sum > two_counts[k]/two_sum:\n",
    "            one.append(k)\n",
    "        else:\n",
    "            two.append(k)\n",
    "    \n",
    "    print (\"Corpus A:\\n\")\n",
    "    for k in one[:50]:\n",
    "        print(\"%s\\t%s\" % (k,vals[k]))\n",
    "\n",
    "    print (\"\\n\\nCorpus B:\\n\")\n",
    "    for k in two[:50]:\n",
    "        print(\"%s\\t%s\" % (k,vals[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in corpus title information and store as DataFrame; Filter unnecessary columns\n",
    "df = pd.read_excel(r'Data\\Ch3CorpusMetadata.xlsx', sheet_name='Sheet1')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Most Distinctive Word Analysis\n",
    "\n",
    "<p>This section uses code adapted from David Bamman's \"Course repo for Applied Natural Language Processing\": </p>\n",
    "\n",
    "- https://github.com/dbamman/anlp19/blob/master/2.distinctive_terms/CompareCorpora.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subset the main dataframe into each sub-corpus\n",
    "inovel_df = df[df['GENRE'] == \"SHISHOSETSU\"]\n",
    "popular_df = df[df['GENRE'] == \"POPULAR\"]\n",
    "junbun_df = df[df['GENRE'] == \"JUNBUNGAKU\"]\n",
    "\n",
    "CORPUS_PATH = r'Texts\\\\'\n",
    "\n",
    "# create lists of all tokens for each sub-corpus\n",
    "inovel_tokens = get_tokens(inovel_df, CORPUS_PATH)\n",
    "popular_tokens = get_tokens(popular_df, CORPUS_PATH)\n",
    "junbun_tokens = get_tokens(junbun_df, CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Corpus A:\n",
      "た\t0.008464677355388743\n",
      "て\t0.006348733243627211\n",
      "に\t0.00539732011647534\n",
      "。\t0.003565800953577933\n",
      "も\t0.003134316006738769\n",
      "は\t0.002908110455997044\n",
      "私\t0.00250026484079847\n",
      "し\t0.002369025702906877\n",
      "自分\t0.0021595991966951887\n",
      "来\t0.001963810225293574\n",
      "い\t0.0017961961325462916\n",
      "彼\t0.0016827352988106692\n",
      "葉子\t0.0014905658319116402\n",
      "こと\t0.0014085543137520875\n",
      "さん\t0.0013171134646960867\n",
      "から\t0.0011927711984728003\n",
      "言っ\t0.0011483140031405847\n",
      "やう\t0.0011224396509807028\n",
      "な\t0.0010997662683195682\n",
      "岸本\t0.0010863011813830958\n",
      "彼女\t0.0010756988275525102\n",
      "家\t0.0010287723885635328\n",
      "を\t0.0009676742981517515\n",
      "時\t0.000894351460903489\n",
      "見\t0.0008481075088779251\n",
      "や\t0.0008308829712061142\n",
      "なかっ\t0.0007923053309747867\n",
      "さう\t0.0007573782395027515\n",
      "行っ\t0.000731121440607054\n",
      "たり\t0.0007079315249316752\n",
      "\n",
      "More Corpus B:\n",
      "、\t-0.025359910287495617\n",
      "―\t-0.003444489353164884\n",
      "」\t-0.0028542691337580823\n",
      "「\t-0.002777770822428416\n",
      "…\t-0.0023036701867795344\n",
      "！\t-0.0015075652594994339\n",
      "が\t-0.0014444628133618402\n",
      "ます\t-0.001185553164912914\n",
      "と\t-0.0011157303634891091\n",
      "この\t-0.0011040045505926826\n",
      "だ\t-0.0010644783566037454\n",
      "まし\t-0.0009445915132850929\n",
      "へ\t-0.0009129534912771476\n",
      "ぬ\t-0.000748205224107778\n",
      "様\t-0.0007243596397919614\n",
      "です\t-0.000707368178988442\n",
      "れ\t-0.0006709613633993959\n",
      "ござい\t-0.0006205857572159702\n",
      "（\t-0.0006084622337394933\n",
      "）\t-0.0006070740179776146\n",
      "いう\t-0.0006033982790458474\n",
      "左膳\t-0.0005287249147046981\n",
      "これ\t-0.000519733776154545\n",
      "眼\t-0.0005027889617828158\n",
      "あろう\t-0.0004849267969363546\n",
      "者\t-0.00043617410939901974\n",
      "ませ\t-0.00043410730815137723\n",
      "だっ\t-0.0004270596060295354\n",
      "刀\t-0.0004026703400503248\n",
      "き\t-0.0004003116249191752\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#calculate a simple difference of proportions on two corpora\n",
    "##############################################################\n",
    "\n",
    "difference_of_proportions(inovel_tokens, popular_tokens)\n",
    "\n",
    "###########################################################################\n",
    "#perform a Mann-Whitney rank-sum test to account for burstiness of language\n",
    "###########################################################################\n",
    "\n",
    "#mann_whitney_analysis(inovel_tokens, popular_tokens)\n",
    "\n",
    "###############################################################################\n",
    "#perform a chi-square test analysis to get statistically significant difference\n",
    "################################################################################\n",
    "\n",
    "#inovel_counts = get_counts(inovel_tokens)\n",
    "#popular_counts = get_counts(popular_tokens)\n",
    "#junbun_counts = get_counts(junbun_tokens)\n",
    "\n",
    "#chi_square(inovel_counts, popular_counts, shared_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus A:\n",
      "\n",
      "葉子\t3741.239886097401\n",
      "自分\t3456.3674316629695\n",
      "やう\t3163.2627825891313\n",
      "私\t3026.194237013213\n",
      "岸本\t2901.6842687501066\n",
      "た\t2755.4908197530517\n",
      "さう\t2143.1128902143737\n",
      "言っ\t2141.6441496370862\n",
      "来\t1980.0987123349432\n",
      "彼\t1938.3137604336773\n",
      "いふ\t1861.5259694500432\n",
      "彼女\t1817.2414585661218\n",
      "三吉\t1525.780062322704\n",
      "て\t1520.363199281394\n",
      "う\t1384.6787104347711\n",
      "あつ\t1365.7634129543894\n",
      "伸子\t1331.6143705216043\n",
      "家\t1190.650884222771\n",
      "に\t1110.5784590973012\n",
      "さん\t1107.820461253785\n",
      "姉\t1101.4741088148708\n",
      "なつ\t1056.9478080343765\n",
      "子供\t1012.0081548945462\n",
      "かつ\t1007.7519688871726\n",
      "母\t995.9967646566339\n",
      "好い\t969.8454879150677\n",
      "も\t946.9141647519618\n",
      "つた\t936.7628155286263\n",
      "母親\t931.7803450410457\n",
      "たり\t927.5449187514165\n",
      "其\t912.9679399219257\n",
      "成っ\t912.3537874524854\n",
      "細君\t908.4810266148512\n",
      "い\t817.3760798138065\n",
      "心\t806.7149115206053\n",
      "叔父\t781.0488942691899\n",
      "居\t738.8683554466102\n",
      "島\t689.0937401022709\n",
      "し\t670.5312312548826\n",
      "や\t660.3146400878035\n",
      "行っ\t648.9603887750385\n",
      "日\t638.5831944355741\n",
      "父\t630.3019017852066\n",
      "行つ\t625.8642350422926\n",
      "見\t623.848189880811\n",
      "なぞ\t607.9131084244331\n",
      "時\t607.1290384633511\n",
      "こと\t600.6242114920598\n",
      "一緒\t588.3488270896662\n",
      "つて\t569.780603399977\n",
      "\n",
      "\n",
      "Corpus B:\n",
      "\n",
      "、\t14945.4556411443\n",
      "―\t3045.0243300156717\n",
      "！\t2091.5897930224996\n",
      "…\t1351.6673595263185\n",
      "（\t1164.060686357475\n",
      "）\t1161.6762881477882\n",
      "ます\t1057.8920169947185\n",
      "ござい\t1043.810099819205\n",
      "刀\t1041.6662567167475\n",
      "」\t978.0411598301422\n",
      "「\t923.1691978159741\n",
      "殿\t823.8395229350092\n",
      "様\t786.9898647535434\n",
      "わし\t760.9457599213879\n",
      "あろう\t748.465061578302\n",
      "まし\t741.5361966135378\n",
      "この\t735.2984596268262\n",
      "ぬ\t666.2912573018643\n",
      "事件\t663.1916143152366\n",
      "江戸\t633.8811725311928\n",
      "刑事\t607.7152451967092\n",
      "城\t593.1082457171893\n",
      "ござり\t590.0203389023595\n",
      "深雪\t574.9823621831423\n",
      "犯人\t549.5255848500365\n",
      "守\t534.7561975753773\n",
      "いま\t531.6096900846568\n",
      "壺\t515.9332108533029\n",
      "古\t513.6758212136532\n",
      "き\t513.1009915562692\n",
      "牧\t504.84148208156836\n",
      "丸\t481.39744591621195\n",
      "おる\t461.814746039187\n",
      "艶\t451.1202590925834\n",
      "侍\t451.1179368735129\n",
      "なん\t449.51057252315195\n",
      "ここ\t433.7033613101076\n",
      "武士\t424.9659198791097\n",
      "武蔵\t407.58314284966025\n",
      "吉\t403.75621049429384\n",
      "眼\t397.42677682236314\n",
      "申し\t396.1967074321393\n",
      "丹波\t392.36922986720566\n",
      "黒\t384.87278431360255\n",
      "太郎\t384.8122185511329\n",
      "者\t369.9845838888273\n",
      "申す\t365.2001422065823\n",
      "よい\t360.1945748802768\n",
      "乾\t359.1591103276913\n",
      "軒\t358.65394914442646\n"
     ]
    }
   ],
   "source": [
    "inovel_counts = get_counts(inovel_tokens)\n",
    "popular_counts = get_counts(popular_tokens)\n",
    "junbun_counts = get_counts(junbun_tokens)\n",
    "\n",
    "chi_square(inovel_counts, popular_counts, shared_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features\n",
    "<p style='text-align: justify;'>The following cell calculates a portion of the total features contained in the overall model. The other features, particularly those related to lexical diversity and entropy, are calculated in a separate R file (GetFeatures.R). After running both, they must be joined manually into a single spreadsheet for analysis in R.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add columns for each feature to be extracted\n",
    "df = df.dropna(subset=['WORK_ID'])   #drop any empty rows at end of csv\n",
    "df['pronouns'] = Series('',index=df.index)\n",
    "df['thought'] = Series('',index=df.index)\n",
    "df['textlength'] = Series('',index=df.index)  #will capture text lenght feature in this cell\n",
    "df['dialogue'] = Series('',index=df.index)\n",
    "df['YulesK'] = Series('',index=df.index)\n",
    "df['YulesI'] = Series('',index=df.index)\n",
    "df['GuiraudC'] = Series('',index=df.index)\n",
    "df['conjuncts'] = Series('',index=df.index)\n",
    "\n",
    "#Point to folder where tokenized texts are stored\n",
    "CORPUS_PATH = r'Texts\\\\'\n",
    "\n",
    "#Iterate through all texts and extract features\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    tokenized = raw_text.read()            \n",
    "    \n",
    "    #do some preprocessing\n",
    "    tokenized = normalize_quotation(tokenized)  #make dialogue markers consistent\n",
    "    \n",
    "    #create untokenized version for certain functions\n",
    "    untokenized = re.sub(r'\\s', '', tokenized)\n",
    "        \n",
    "    #calculate text length\n",
    "    df.at[k, 'textlength'] = len(untokenized)\n",
    "    \n",
    "    #calculate amount of dialogue\n",
    "    df.at[k, 'dialogue'] = percent_dialogue(untokenized)\n",
    "    \n",
    "    #calculate use of first/third person singular pronouns per sentence\n",
    "    df.at[k, 'pronouns'] = pronouns(tokenized)\n",
    "    \n",
    "    #calculate thought/feeling words\n",
    "    df.at[k, 'thought'] = thought(untokenized)\n",
    "    \n",
    "    #calculate amount of conjunctions\n",
    "    df.at[k, 'conjuncts'] = conjuncts(untokenized)\n",
    "    \n",
    "    #strip punctuation for the next metrics\n",
    "    tokenized = remove_punc(tokenized)\n",
    "        \n",
    "    #calculate alternative metrics\n",
    "    df.at[k, 'YulesK'], df.at[k, 'YulesI'], df.at[k, 'GuiraudC'] = get_metrics(tokenized)\n",
    "    \n",
    "print(\"Processed \" + str(k) + \" files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results to Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'Results\\python_extracted_features_temp.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Distinctive Words in Low versus High Entropy I-Novel Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#identify files where chunks are stored\n",
    "lowent_file = r\"Results\\LowEntJP.txt\"\n",
    "highent_file = r\"Results\\HighEntJP.txt\"\n",
    "\n",
    "#clean and segment texts\n",
    "raw = open(lowent_file, encoding=\"utf-8\")\n",
    "raw = raw.read()\n",
    "raw = remove_punc(raw)\n",
    "raw = re.sub(r'\\n\\n', ' ', raw)\n",
    "lowent_tokens = re.split(r' ', raw)\n",
    "\n",
    "raw = open(highent_file, encoding=\"utf-8\")\n",
    "raw = raw.read()\n",
    "raw = remove_punc(raw)\n",
    "raw = re.sub(r'\\n\\n', ' ', raw)\n",
    "highent_tokens = re.split(r' ', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus A:\n",
      "\n",
      "自分\t706.1165592733796\n",
      "葉子\t536.1075912981897\n",
      "節子\t413.0830488376388\n",
      "岸本\t386.2202708914681\n",
      "こと\t372.54725007840517\n",
      "私\t326.6784203403483\n",
      "た\t321.28330285422146\n",
      "い\t311.08704350919453\n",
      "僕\t298.1244569781743\n",
      "て\t245.09041418746446\n",
      "いる\t243.95155539634825\n",
      "なかっ\t225.0414831298667\n",
      "倉地\t223.72963627947811\n",
      "それ\t217.91662742723412\n",
      "鶴\t204.61934483093697\n",
      "その\t203.57436429299827\n",
      "思っ\t194.48633581315667\n",
      "いっ\t180.08960389765693\n",
      "事\t176.61025827786648\n",
      "だっ\t168.01504067535515\n",
      "は\t160.0586214236515\n",
      "いう\t142.82615325959543\n",
      "柳沢\t141.60228156517195\n",
      "彼\t141.11430961978004\n",
      "庸三\t123.14606248291788\n",
      "宮\t117.92719671265124\n",
      "しかし\t115.86096915291952\n",
      "ない\t114.85017469710183\n",
      "に\t114.16456581793706\n",
      "よう\t114.14766401850966\n",
      "父\t112.96121312004487\n",
      "叔父\t111.53520621139434\n",
      "須山\t110.29175255834028\n",
      "あっ\t109.63838289547652\n",
      "然し\t101.52968594351951\n",
      "義雄\t101.25784248947815\n",
      "書い\t95.61663819220952\n",
      "児島\t94.60439673365246\n",
      "よこし\t93.06872888415718\n",
      "の\t90.93068191525252\n",
      "行っ\t88.80795800119355\n",
      "あなた\t86.97916687552245\n",
      "しまっ\t84.8838988929875\n",
      "ある\t83.10674038362012\n",
      "繁\t79.17402996488215\n",
      "岡\t73.1111524448354\n",
      "心\t70.50033960639851\n",
      "医者\t68.63436827762449\n",
      "ため\t68.44045548318137\n",
      "赤児\t67.68887935381075\n",
      "\n",
      "\n",
      "Corpus B:\n",
      "\n",
      "伸子\t577.3539402099291\n",
      "―\t389.96008004041727\n",
      "よ\t295.6445251672643\n",
      "ん\t197.37852649656716\n",
      "勤\t191.743922020409\n",
      "ね\t187.91847577449636\n",
      "候\t177.20029113961812\n",
      "つ\t164.7182903301927\n",
      "言つ\t154.18763264066467\n",
      "かつ\t148.85197425029358\n",
      "ねえ\t135.18559181540985\n",
      "光\t131.20218802346267\n",
      "佃\t130.8602712497043\n",
      "でし\t129.2247738882271\n",
      "つた\t124.62078835163797\n",
      "青木\t120.1003476092231\n",
      "ナオミ\t114.42266730476379\n",
      "わ\t109.87882921386314\n",
      "御\t104.33473178244179\n",
      "平三\t101.81524397225515\n",
      "もん\t94.95846144886316\n",
      "やら\t94.60890233249002\n",
      "來\t88.97423618375949\n",
      "生\t86.68487336099395\n",
      "屋\t85.93720531174799\n",
      "銑之助\t85.57938044655702\n",
      "です\t84.67459966317679\n",
      "さま\t83.05029694390987\n",
      "氣\t80.97431205237847\n",
      "ア\t76.91929055885774\n",
      "てる\t76.85125112640426\n",
      "懸け\t76.18819338283404\n",
      "秀雄\t72.7733993754617\n",
      "輪\t71.27076420388148\n",
      "田\t67.53306821691689\n",
      "比\t67.0599722771818\n",
      "平七\t66.70244366129576\n",
      "居る\t65.33183994335168\n",
      "佐\t63.964037520338884\n",
      "様\t63.44933400885065\n",
      "つて\t62.7729551617837\n",
      "ッて\t62.47375781953074\n",
      "じゃ\t61.03876699625434\n",
      "入つ\t59.47215124199588\n",
      "市川\t58.98266109779601\n",
      "笑\t56.84761798239602\n",
      "など\t55.64109730511902\n",
      "譲治\t54.57943924938933\n",
      "たる\t53.58308394094906\n",
      "三\t53.26028763627755\n"
     ]
    }
   ],
   "source": [
    "lowent_counts = get_counts(lowent_tokens)\n",
    "highent_counts = get_counts(highent_tokens)\n",
    "\n",
    "chi_square(lowent_counts, highent_counts, shared_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482.41131498 3.70602828746\n"
     ]
    }
   ],
   "source": [
    "#calculate average length of passage\n",
    "import numpy as np\n",
    "\n",
    "lowent_file = r\"Results/LowEntJP.txt\"\n",
    "char_length = []\n",
    "\n",
    "#clean and segment texts\n",
    "raw = open(lowent_file, encoding=\"utf-8\")\n",
    "raw = raw.read()\n",
    "chunks = re.split(r'\\n\\n', raw)\n",
    "for chunk in chunks:\n",
    "    no_space = re.sub(r' ', '', chunk)\n",
    "    chars = list(no_space)\n",
    "    char_length.append(len(chars))\n",
    "    \n",
    "print(np.mean(char_length), np.mean(char_length)/400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
