{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Square Test\n",
    "### Determine which words are significantly associated with low vs. high entropy chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk, re, pprint\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import operator\n",
    "from pandas import Series, DataFrame\n",
    "from scipy import stats\n",
    "from numpy.random import permutation, shuffle\n",
    "import string\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from nltk.corpus.reader import *\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.text import Text\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to clean punctuation\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','-','─','<',\n",
    "         '＃','△','※','＊', '!', '\\\"','#', '\\'', '^', '\\(', '\\)', '.', ',', '+']\n",
    "\n",
    "def cleaner(text):\n",
    "    for punc in puncs:\n",
    "        text = text.replace(punc, '')\n",
    "    text = re.sub(r'   ', ' ', text)          #get rid of double spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DataFrame for Lowest and Highest Entropy Chunks for Japanese Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41084, 1307)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store word_freq data and class labels here\n",
    "word_dict = {}\n",
    "meta_rows = []\n",
    "\n",
    "#identify files where chunks are stored\n",
    "low_ent_file = r\"Results/LowEntJP.txt\"\n",
    "high_ent_file = r\"Results/HighEntJP.txt\"\n",
    "\n",
    "#clean and segment texts\n",
    "raw = open(low_ent_file, encoding=\"utf-8\")\n",
    "low_ent_raw = raw.read()\n",
    "low_ent_raw = cleaner(low_ent_raw)  #clean punctuation\n",
    "low_ent_chunks = re.split(r'\\n\\n', low_ent_raw)[0:-1]\n",
    "\n",
    "raw = open(high_ent_file, encoding=\"utf-8\")\n",
    "high_ent_raw = raw.read()\n",
    "high_ent_raw = cleaner(high_ent_raw)  #clean punctuation\n",
    "high_ent_chunks = re.split(r'\\n\\n', high_ent_raw)[0:-1]\n",
    "\n",
    "#now turn chunk lists into a single DTM\n",
    "j = 0\n",
    "for chunk in low_ent_chunks:\n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = chunk.split(' ')\n",
    "    while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    while r'\\u3000' in tokens: tokens.remove(r'\\u3000')  #remove blank spaces\n",
    "\n",
    "    #produce the frequency list\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    freq_pairs = fdist.items()\n",
    "    \n",
    "    words = {}\n",
    "    class_label = \"low_ent\"\n",
    "    \n",
    "    for pair in freq_pairs:\n",
    "        words[pair[0]] = pair[1]\n",
    "    \n",
    "    word_dict[j] = words\n",
    "    meta_data = pd.DataFrame({'class_label': class_label}, index=[0])\n",
    "    meta_rows.append(meta_data)\n",
    "    \n",
    "    j+=1\n",
    "\n",
    "for chunk in high_ent_chunks:\n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = chunk.split(' ')\n",
    "    while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    while r'\\u3000' in tokens: tokens.remove(r'\\u3000')  #remove blank spaces\n",
    "\n",
    "    #produce the frequency list\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    freq_pairs = fdist.items()\n",
    "    \n",
    "    words = {}\n",
    "    class_label = \"high_ent\"\n",
    "    \n",
    "    for pair in freq_pairs:\n",
    "        words[pair[0]] = pair[1]\n",
    "    \n",
    "    word_dict[j] = words\n",
    "    meta_data = pd.DataFrame({'class_label': class_label}, index=[0])\n",
    "    meta_rows.append(meta_data)\n",
    "    \n",
    "    j+=1    \n",
    "\n",
    "#now combine everything from low and high entropy chunks\n",
    "all_words = pd.DataFrame(word_dict, index=word_dict.keys(), dtype='float32') \n",
    "#all_words = pd.DataFrame.from_dict(word_dict)\n",
    "all_words = all_words.fillna(0)\n",
    "all_words = all_words.astype(int)\n",
    "\n",
    "all_meta = pd.concat(meta_rows, join='outer')\n",
    "all_meta.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dtm = pd.concat([all_meta, all_words], axis= 1, join=\"outer\")\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_meta = pd.concat(meta_rows, join='outer')\n",
    "all_meta.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-b629b9dac664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"class_label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_meta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Hoyt\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \"\"\"\n\u001b[1;32m   2508\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2510\u001b[0m         self._data.insert(loc, column, value,\n\u001b[1;32m   2511\u001b[0m                           allow_duplicates=allow_duplicates)\n",
      "\u001b[0;32mC:\\Users\\Hoyt\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Hoyt\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Length of values does not match length of '\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "all_words = pd.DataFrame.from_dict(word_dict)\n",
    "all_words = all_words.fillna(0)\n",
    "all_words = all_words.astype(int)\n",
    "all_words.insert(0, \"class_label\", all_meta.class_label.tolist())\n",
    "all_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low_ent', 'low_ent', 'low_ent', 'low_ent', 'low_ent']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = all_meta.class_label.tolist()\n",
    "temp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-bf5e3f85e1a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#dtm = pd.concat([all_words, all_meta], axis=1, join=\"outer\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#dtm = pd.concat([all_words, all_meta], axis=1, join=\"outer\")\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1296</th>\n",
       "      <th>1297</th>\n",
       "      <th>1298</th>\n",
       "      <th>1299</th>\n",
       "      <th>1300</th>\n",
       "      <th>1301</th>\n",
       "      <th>1302</th>\n",
       "      <th>1303</th>\n",
       "      <th>1304</th>\n",
       "      <th>1305</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_label  0  1  2  3  4  5  6  7  8  ...   1296  1297  1298  1299  \\\n",
       "(           NaN  1  0  0  0  0  0  1  0  0  ...      0     0     0     0   \n",
       ")           NaN  1  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "0           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "000         NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "1           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "2           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "3           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "5           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "8           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       ":           NaN  0  0  0  0  0  0  0  0  0  ...      0     0     0     0   \n",
       "\n",
       "     1300  1301  1302  1303  1304  1305  \n",
       "(       0     0     0     0     0     0  \n",
       ")       0     0     0     0     0     0  \n",
       "0       0     0     0     0     0     0  \n",
       "000     0     0     0     0     0     0  \n",
       "1       0     0     0     0     0     0  \n",
       "2       0     0     0     0     0     0  \n",
       "3       0     0     0     0     0     0  \n",
       "5       0     0     0     0     0     0  \n",
       "8       0     0     0     0     0     0  \n",
       ":       0     0     0     0     0     0  \n",
       "\n",
       "[10 rows x 1307 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "labels = list(set(dtm.class_label.values))\n",
    "\n",
    "class_counts = {}\n",
    "#store class labels and counts to use below\n",
    "for i in range(len(dtm.class_label.value_counts().keys())):\n",
    "    class_counts[dtm.class_label.value_counts().keys()[i]] = dtm.class_label.value_counts()[i]\n",
    "\n",
    "for feature in dtm.columns[1:]:\n",
    "    feat_counts = {}\n",
    "    for label in labels:\n",
    "        feat_counts[label] = dtm[(dtm['class_label'] == label) & (dtm[feature] > 0)].shape[0]\n",
    "    \n",
    "    no_docs = sum(feat_counts.values())\n",
    "    \n",
    "    feat_inv = {}\n",
    "    for key in feat_counts:\n",
    "        feat_inv[key] = class_counts[key] - feat_counts[key]\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    for key in feat_counts:\n",
    "        a.append(feat_inv[key])\n",
    "        b.append(feat_counts[key])\n",
    "    obs = np.array([a, b])\n",
    "    \n",
    "    #only include features that have a count of 4 or more for each position in table\n",
    "    x = [el for el in obs[0]]\n",
    "    y = [el for el in obs[1]]\n",
    "    for el in y:\n",
    "        x.append(el)\n",
    "    \n",
    "    if len([item for item in x if item >= 4]) == 4:  #make sure all elements greater than 4\n",
    "        chi_sq_stat = chi2_contingency(obs)[0]\n",
    "        #append the data\n",
    "        stats.append((feature, chi_sq_stat, no_docs, feat_counts[labels[0]], feat_counts[labels[1]]))\n",
    "\n",
    "#turn into a dataframe and sort by chi_sq score\n",
    "df = pd.DataFrame(stats, columns=['feature', 'chi_sq', 'no_docs', 'high_ent_freq', 'low_ent_freq'])\n",
    "sorted_df = df.sort_values(by='chi_sq', ascending=False)\n",
    "\n",
    "#view the top 10 rows\n",
    "sorted_df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Same Procedure for Chinese Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now do this for the Chinese texts\n",
    "#a function to strip punctuation\n",
    "puncs = ['】','【','○','■','▉','●','，','。','“','”','：',',','？',':','！','、','?','；','!','’','‘',';','.','）','（','》',\n",
    "         '《','……','^','I','-','*','——','—','......','l','(',')','①','!......','>','」','「','•','A','B','C','D','E','F',\n",
    "         'G','H','J','K','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','1','2','3','4','5','6','7','8','9',\n",
    "         '0','一','二','三','四','五','六','七','八','九','十','百','〇','\\\"','第','回','a','b','c','d','e','f','g','h','i',\n",
    "        'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "def cleaner(text):\n",
    "    for punc in puncs:\n",
    "        text = text.replace(punc, '')\n",
    "    text = re.sub(r'   ', ' ', text)    #turn double spaces to single spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a DTM for the low and high entropy chunks\n",
    "\n",
    "#store word_freq data and class labels here\n",
    "word_dict = {}\n",
    "meta_rows = []\n",
    "\n",
    "low_ent_file = r\"LowEntCH.txt\"\n",
    "high_ent_file = r\"HighEntCH.txt\"\n",
    "\n",
    "#clean and segment texts\n",
    "raw = open(low_ent_file)\n",
    "low_ent_raw = raw.read()\n",
    "low_ent_raw = cleaner(low_ent_raw)  #clean punctuation\n",
    "low_ent_chunks = re.split(r'\\r\\n\\r\\n', low_ent_raw)[0:-1]\n",
    "\n",
    "raw = open(high_ent_file)\n",
    "high_ent_raw = raw.read()\n",
    "high_ent_raw = cleaner(high_ent_raw)  #clean punctuation\n",
    "high_ent_chunks = re.split(r'\\r\\n\\r\\n', high_ent_raw)[0:-1]\n",
    "\n",
    "#now turn chunk lists into a single DTM\n",
    "j = 0\n",
    "for chunk in low_ent_chunks[0:100]:\n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = chunk.split(' ')\n",
    "    while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    while ur'\\u3000' in tokens: tokens.remove(ur'\\u3000')  #remove blank spaces\n",
    "\n",
    "    #produce the frequency list\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    freq_pairs = fdist.items()\n",
    "    \n",
    "    words = {}\n",
    "    class_label = \"low_ent\"\n",
    "    \n",
    "    for pair in freq_pairs:\n",
    "        words[pair[0]] = pair[1]\n",
    "    \n",
    "    word_dict[j] = words\n",
    "    meta_data = pd.DataFrame({'class_label': class_label}, index=[0])\n",
    "    meta_rows.append(meta_data)\n",
    "    \n",
    "    j+=1\n",
    "\n",
    "for chunk in high_ent_chunks[0:100]:\n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = chunk.split(' ')\n",
    "    while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    while ur'\\u3000' in tokens: tokens.remove(ur'\\u3000')  #remove blank spaces\n",
    "\n",
    "    #produce the frequency list\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    freq_pairs = fdist.items()\n",
    "    \n",
    "    words = {}\n",
    "    class_label = \"high_ent\"\n",
    "    \n",
    "    for pair in freq_pairs:\n",
    "        words[pair[0]] = pair[1]\n",
    "    \n",
    "    word_dict[j] = words\n",
    "    meta_data = pd.DataFrame({'class_label': class_label}, index=[0])\n",
    "    meta_rows.append(meta_data)\n",
    "    \n",
    "    j+=1    \n",
    "\n",
    "#now combine everything\n",
    "all_words = pd.DataFrame(word_dict.values(), index=word_dict.keys(), dtype='float32') \n",
    "all_words = all_words.fillna(0)\n",
    "all_words = all_words.astype(int)\n",
    "\n",
    "all_meta = pd.concat(meta_rows, join='outer')\n",
    "all_meta.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dtm = pd.concat([all_meta, all_words], axis= 1, join=\"outer\")\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "labels = list(set(dtm.class_label.values))\n",
    "\n",
    "class_counts = {}\n",
    "#store class labels and counts to use below\n",
    "for i in range(len(dtm.class_label.value_counts().keys())):\n",
    "    class_counts[dtm.class_label.value_counts().keys()[i]] = dtm.class_label.value_counts()[i]\n",
    "\n",
    "for feature in dtm.columns[1:]:\n",
    "    feat_counts = {}\n",
    "    for label in labels:\n",
    "        feat_counts[label] = dtm[(dtm['class_label'] == label) & (dtm[feature] > 0)].shape[0]\n",
    "    \n",
    "    no_docs = sum(feat_counts.values())\n",
    "    \n",
    "    feat_inv = {}\n",
    "    for key in feat_counts:\n",
    "        feat_inv[key] = class_counts[key] - feat_counts[key]\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    for key in feat_counts:\n",
    "        a.append(feat_inv[key])\n",
    "        b.append(feat_counts[key])\n",
    "    obs = np.array([a, b])\n",
    "    \n",
    "    #only include features that have a count of 4 or more for each position in table\n",
    "    x = [el for el in obs[0]]\n",
    "    y = [el for el in obs[1]]\n",
    "    for el in y:\n",
    "        x.append(el)\n",
    "    \n",
    "    if len([item for item in x if item >= 4]) == 4:  #make sure all elements greater than 4\n",
    "        chi_sq_stat = chi2_contingency(obs)[0]\n",
    "        #append the data\n",
    "        stats.append((feature, chi_sq_stat, no_docs, feat_counts[labels[0]], feat_counts[labels[1]]))\n",
    "\n",
    "df2 = pd.DataFrame(stats, columns=['feature', 'chi_sq', 'no_docs', 'high_ent_freq', 'low_ent_freq'])\n",
    "sorted_df2 = df2.sort_values(by='chi_sq', ascending=False)\n",
    "sorted_df2[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export the dfs to excel files\n",
    "from pandas import ExcelWriter\n",
    "writer = ExcelWriter('./DistinctEntTerms.xlsx')\n",
    "sorted_df.to_excel(writer,'Sheet1')\n",
    "sorted_df2.to_excel(writer,'Sheet2')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
