{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Annotation Code for Chapter 5\n",
    "\n",
    "<p style='text-align: justify;'> This notebook contains some simple scripts that I used to identify character names and replace each with a unique identifier for subsequent analysis.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### imports\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import MeCab  #CHECK \"MECABRC\" FILE TO SEE WHICH DICTIONARY YOU ARE USING\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic\n",
    "import collections\n",
    "import operator\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fiction Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 38)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD in metadata for the fiction corpus\n",
    "folder_path = os.path.abspath(\"CharacterAnnotationCode\" + \"/../../\")\n",
    "fic_meta = folder_path + \"\\Data\\Fiction_Meta.xlsx\"\n",
    "df = pd.read_excel(fic_meta, sheet_name='Sheet1')\n",
    "\n",
    "#select works where you want to annotate characters\n",
    "df = df[df['FILTER'] == 'char_annot']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Character Names in Texts using Frequency Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43622, 45269, 1743, 10000224, 46576, 10000230, 1101, 45061, 159, 33197, 1875, 1320, 4512, 46636, 2246, 10000231, 46488, 10000202, 10000227, 50899, 2100, 10000213, 2012, 52236, 10000232, 1502, 10000201, 10000200, 156, 10000203, 45485, 3585, 10000214, 1418, 10000225, 2117, 10000212, 10000205, 10000204, 3370, 3527, 10000215, 1398, 10000216, 10000217, 2929, 10000218, 10000206, 10000226, 10000207, 10000228, 3369, 10000220, 10000209, 10000211, 10000229, 10000208, 10000222, 10000221, 10000219, 10000223, 10000210, 2277]\n"
     ]
    }
   ],
   "source": [
    "#get list of file_ids and select a work to inspect in next cell\n",
    "file_ids = df.WORK_ID.tolist()\n",
    "print(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set path where tokenized files are stored\n",
    "CORPUS_PATH = folder_path + \"Corpora\\AozoraFictionTokenized\\\\\"\n",
    "\n",
    "#grab a single text from list in previous cell\n",
    "text_id = file_ids[0]  #analyzing the first text in list\n",
    "source = CORPUS_PATH + str(text_id) + '.txt'\n",
    "\n",
    "#create a global list to hold the tokens\n",
    "tokens = []\n",
    "\n",
    "#read in text and do some pre-processing\n",
    "raw = open(source, encoding=\"utf-8\", errors=\"ignore\")       #grab text\n",
    "text = raw.read()\n",
    "\n",
    "#split the text into a list of individual tokens\n",
    "tokens = text.split(' ')\n",
    "while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    \n",
    "total_tokens = len(tokens)  #keep track of total tokens to calculate rel freq later\n",
    "\n",
    "#produce the frequency list\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "freq_pairs = fdist.items()\n",
    "sort_freq_pairs = sorted(freq_pairs, key=lambda x:x[1], reverse=True)  #sort by decreasing frequency\n",
    "\n",
    "#create a dictionary to store word-frequency pairs\n",
    "word_freqs = {}\n",
    "\n",
    "#fill dictionary with pairs\n",
    "for item in sort_freq_pairs:\n",
    "    word_freqs[item[0]] = item[1]\n",
    "    \n",
    "freqs_df = DataFrame.from_dict(word_freqs, orient='index')  #convert dict to dataframe\n",
    "freqs_df = freqs_df.rename(columns={0:'frequency'})    #rename column\n",
    "freqs_df = freqs_df.sort_values(by='frequency', ascending=False)   #sort by frequency\n",
    "\n",
    "#build stop-words list\n",
    "stoplist_dir = folder_path + '\\WordLists\\\\'\n",
    "text = open(stoplist_dir + \"stopwords.txt\", encoding='utf-8')\n",
    "raw = text.read()\n",
    "#need to turn stopwords into a list\n",
    "raw = raw.split('\\n')  #need to split on return carriage and newline\n",
    "jp_stopwords = []\n",
    "for word in raw:\n",
    "    jp_stopwords.append(word)\n",
    "\n",
    "#add punctuation\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','<','〔','〕',\n",
    "         '＃','△','※','＊','—','(',')','.','．']\n",
    "jp_stopwords = jp_stopwords + puncs\n",
    "\n",
    "#eliminate stopwords from top of list\n",
    "drop_list = []   \n",
    "for i in range(500):\n",
    "    if freqs_df.iloc[i].name in jp_stopwords:\n",
    "        drop_list.append(i)   #store index number in a list\n",
    "freqs_df = freqs_df.drop(freqs_df.index[drop_list])  #drop all the stopwords using list of index numbers\n",
    "\n",
    "#compute relative frequencies\n",
    "freqs_df['rel_freq'] = freqs_df.frequency / total_tokens\n",
    "freqs_df = freqs_df.reset_index()\n",
    "\n",
    "#inspect top 50 hits to extract most common character names in text\n",
    "freqs_df[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Character Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary that identifies major and minor characters in each text to be annotated and assigns unique ID\n",
    "# these characters were hand-identified using previous cell\n",
    "# character names are as they appear in UNIDIC tokenized text\n",
    "\n",
    "char_names = {'10000224':['コーレ','ピラ'], \n",
    "              '10000219':['李 艶','奎 栄','鳳 琴','潤 芝','董 翠 花','チュルガン','金 毓桂','慶 亭'],\n",
    "              '10000215':['申 重 欽'],\n",
    "              '10000210':['寿 善','北原'],\n",
    "              '10000202':['栄 策'],\n",
    "              '10000230':['伊東','柏年'],\n",
    "              '10000203':[],\n",
    "              '10000232':['碧雲'],\n",
    "              '10000200':['お久 さん'],\n",
    "              '10000214':['張'],\n",
    "              '10000211':['八 吉'],\n",
    "              '10000220':['清吉','健','シ ノブ','しづ子','黒井'],\n",
    "              '10000231':['秀 梅','遠矢','細君'],\n",
    "              '10000205':['金 、 太郎','母親','父親','乾 爺 さん'],\n",
    "              '10000212':['姉','露助','ジナイーダ','ズナ'],\n",
    "              '10000229':['周長 乾 老人','周長 坤','弟','父親','叔父','兄'],\n",
    "              '10000221':['どん げん'],\n",
    "              '10000227':['順','雪子','加代','老母'],\n",
    "              '10000207':['朴','朴 泰 民'],\n",
    "              '10000228':['劉 石 虎','楊 名声'],\n",
    "              '10000225':['田中','母','父'],\n",
    "              '10000213':['フユ','父親','母親','フデ'],\n",
    "              '10000206':['先生','坊主','本多'],\n",
    "              '10000222':['マリ ヤン','H 氏'],\n",
    "              '10000223':['ノー カナ','コック 長','ボーイ'],\n",
    "              '10000201':['操','文吉'],\n",
    "              '10000204':['童 伊','許 生 員','蓬 坪'],\n",
    "              '10000216':['張','李','リベカ'],\n",
    "              '10000217':['王','丸 焼'],\n",
    "              '10000218':['祝','真吉','吉村','祝 廉 天'],\n",
    "              '10000226':['采 雲','楊','母'],\n",
    "              '10000208':['先生','秀 東'],\n",
    "              '10000209':['エップニ','姉'],\n",
    "              '45269':['ラシイヌ','レザール 氏','ダン チョン 画家','張 教仁','紅玉'],\n",
    "              '43622':['ジョン','紋 太夫','ホーキン 氏'],\n",
    "              '3370':['怪 塔 王','兵曹 長','小浜','塩田 大尉','青江 三 空曹','大利根 博士'],\n",
    "              '3527':['川上','杉田','リット 提督'],\n",
    "              '46576':['清','フー ラー','武田 博士'],\n",
    "              '2117':['呉羽','轟 氏'],\n",
    "              '2100':['バード ・ ストーン'],\n",
    "              '33197':[],\n",
    "              '45485':['叔父','源兵衛'],\n",
    "              '3369':['竹見','ハルク','ノルマン','ポー ニン','モロ'],\n",
    "              '1320':['マヌエラ','カーク','ヤン'], #think these are gorillas, actually\n",
    "              '50899':['参 木','甲谷','お 杉','宮子','山口','オルガ','お 柳'],\n",
    "              '1502':['丑松','銀之助','志保','瀬川','蓮太郎'],\n",
    "              '2246':['矢代','千鶴子','久慈','真紀子','東野'],\n",
    "              '1418':['山崎','幹太郎','中津','高取','小山'],\n",
    "              '2012':['伸子','素子','蜂谷'],\n",
    "              '1743':['マターファ','スティヴンスン','ファニイ'],\n",
    "              '3585':['光一','手塚','チビ 公','千三','阪井','巌','文子'],\n",
    "              '2929':['玄竜','田中','大村'],\n",
    "              '1398':['春雄','山田','李'],\n",
    "              '2277':['周 さん','津田','藤野'],\n",
    "              '52236':['ゆき子','富岡','加野'],\n",
    "              '46488':['富士 男','ドノバン','ゴルドン','次郎','イバン ス','サービス','バクスター','モ コウ'],\n",
    "              '156':['穂積 中佐','将軍','田口 一 等 卒'],\n",
    "              '1101':['俊寛','成経','有王','康頼'],\n",
    "              '45061':['ガルーダ を ぢ さん','首領']}\n",
    "              '159':['俊寛',' 女 '], \n",
    "              '1875':['阿賀 妻'],\n",
    "              '4512':['駒井','米 友','神尾','お松','岩倉 三 位'],\n",
    "              '46636':['キューネ','ハチロウ','ナエーア']}\n",
    "\n",
    "#assign unique ids to all annotated character names\n",
    "all_char_ids = {}\n",
    "for key in char_names.keys():\n",
    "    i = 1\n",
    "    char_ids = []\n",
    "    for char_name in char_names[key]:\n",
    "        char_id = '0' + key + '0000' + str(i)\n",
    "        char_ids.append((char_name, char_id))\n",
    "        i += 1\n",
    "    all_char_ids[key] = char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Versions of Texts with Character Names replaced by Unique IDs\n",
    "\n",
    "<p style='text-align: justify;'>The resulting texts will be in lemmatized form to allow searching of significant semantic clusters using words generated from Cluster Detection analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1. Import unidic text\n",
    "#2. Replace character names with the unique ID for that character\n",
    "#3. Strip spaces and then re-tokenize into lemma form\n",
    "#4. Replace character IDs with name-title combo and output to new directory\n",
    "\n",
    "#work on unidic tokenized corpus\n",
    "CORPUS_PATH = folder_path + \"\\Corpora\\AozoraFictionTokenized\\\\\"\n",
    "OUTPUT_PATH = folder_path + \"\\Corpora\\CharAnnot\\\\\"\n",
    "\n",
    "#iterate through all character annotated texts and substitute character names with ids\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(int(df.WORK_ID[k])) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    #use work_id to access the char_ids dictionary\n",
    "    char_ids = all_char_ids[str(df.WORK_ID[k])]\n",
    "    \n",
    "    #replace character names with unique character id\n",
    "    for pair in char_ids:\n",
    "        raw = re.sub(pair[0], pair[1], raw)\n",
    "        \n",
    "    #strip spaces and re-tokenize into lemma form\n",
    "    raw = re.sub(r'\\s', '', raw)\n",
    "    \n",
    "    lemma_tokens = []\n",
    "    node = mecab.parseToNode(raw)\n",
    "    node = node.next\n",
    "\n",
    "    while node:\n",
    "        if len(re.split(r',', node.feature)) > 6:  #some words don't have a lemma form\n",
    "            lemma_tokens.append(re.split(r',', node.feature)[7])\n",
    "            node = node.next\n",
    "        else:   #if not, just add the plain token\n",
    "            lemma_tokens.append(node.surface)\n",
    "            node = node.next\n",
    "\n",
    "    #merge lemma tokens\n",
    "    new_text = ' '.join(lemma_tokens)\n",
    "    \n",
    "    #replace character ids with an easier to interpret tag\n",
    "    for pair in char_ids:\n",
    "        new_id = pair[0].replace(\" \", \"\") + '_' + df.WORK_TITLE[k].replace(\" \", \"\")\n",
    "        new_text = re.sub(pair[1], new_id, new_text)\n",
    "        \n",
    "    #now print the revised text back out to a file\n",
    "    with open(OUTPUT_PATH + str(df.WORK_ID[k]) + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(new_text)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
