{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre and Post Processing Code for Chapter 5\n",
    "\n",
    "<p style='text-align: justify;'> This notebook contains code for various pre and post-processing steps related to the corpora used in Chapter 5. This includes steps such as lemmatization, obtaining word counts, unification of racial identifiers, making a concordance, using word-embedding models to identify semantically similar words, and the extraction of semantic clusters with specific racial identifiers. Owing to copyright restrictions, the base texts on which all these processing steps rely cannot be made public. Thus, this code is for reference only. I have also made it generic for both the fiction and kindai magazine collections. To apply to either corpus, simply change the directory names to point to appropriate file folders.</p>\n",
    "\n",
    "<p style='text-align: justify;'>The bulk of the computational analysis for this chapter is done with python files described in more detail in the <b>Read Me</b> file. I have provided a post-processed copy of the fiction corpus so that the word-embedding and cluster analysis can be reproduced.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from shutil import copyfile\n",
    "import MeCab  #CHECK \"MECABRC\" FILE TO SEE WHICH DICTIONARY YOU ARE USING\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic\n",
    "import collections\n",
    "import operator\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "import xlsxwriter\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Functions for Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some cleaning and pre-processing functions for Japanese corpus\n",
    "\n",
    "#this first one should only be used for non-tokenized texts; basically cleans them for tokenization step\n",
    "def strip_chap_titles(raw):\n",
    "    #get rid of chapter titles that use Chinese numbers with or without surronding parantheses\n",
    "    raw = re.sub(r'（*([一二三四五六七八九十])+(）)*\\n', '', raw)\n",
    "    #get rid of chapter titles that use utf-8 alpha-numeric numbers\n",
    "    raw = re.sub(r'[１-９]+\\n', '', raw)\n",
    "    raw = re.sub(r'[第弐拾章参壱一二三四五六七八九十]+\\n', '', raw)\n",
    "    raw = re.sub(r'『', r'「', raw)   #replace all 『 with 「\n",
    "    raw = re.sub(r'』', r'」', raw)   #replace all 』 with 」\n",
    "    raw = re.sub(r'\\n', '', raw)  #strips all newlines\n",
    "    raw = re.sub(r'\\r', '', raw)  #strips all returns\n",
    "    #raw = re.sub(r'\\s', '', raw)\n",
    "    return raw\n",
    "\n",
    "#might need to run this separately for already tokenized files\n",
    "def bracket_cleaner(raw):\n",
    "    raw = re.sub(r'［[^］]+］', '', raw)   #replace annotations in brackets ([#...])\n",
    "    raw = re.sub(r'\\s+', ' ', raw)                         #get rid of double spaces\n",
    "    return raw\n",
    "\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','<','〔','〕',\n",
    "         '＃','△','※','＊']\n",
    "\n",
    "def cleaner(text):\n",
    "    for punc in puncs:\n",
    "        text = re.sub(punc, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)                         #get rid of double spaces\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    new_list = [token for token in tokens if token not in stopwords]\n",
    "    return new_list\n",
    "\n",
    "#this function computes percentage of text that is dialogue\n",
    "def percent_dialogue(text):\n",
    "    no_quotes = re.sub(r'「[^」]*」', '', text)   #eliminate all dialogue passages for single bracket quotes\n",
    "    per_dialogue = (len(text)-len(no_quotes))/len(text)\n",
    "    return per_dialogue\n",
    "\n",
    "def punct(text):\n",
    "    punctuation = ['、','。','…','！','？']\n",
    "    count = 0\n",
    "    \n",
    "    #strip dialogue\n",
    "    no_dialogue = re.sub(r'「[^」]*」', '', text)\n",
    "    \n",
    "    #search and tabulate first and third person usage\n",
    "    for word in punctuation:\n",
    "        instances = re.findall(word, no_dialogue)\n",
    "        count += len(instances)\n",
    "\n",
    "    return count/len(no_dialogue)\n",
    "    \n",
    "def geo_words(text):\n",
    "    path = r'WordLists\\Empire_Place_Names.txt'\n",
    "    f = open(path, encoding='utf-8')\n",
    "    words = f.read()\n",
    "    geo_words = re.split(r'\\n', words)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    #grab instances of \"colonies\" first\n",
    "    instances = re.findall(r'植民　地', text)\n",
    "    count += len(instances)\n",
    "    \n",
    "    #tokenize text and search for geographic names\n",
    "    tokens = re.split(r'\\s', text)\n",
    "    \n",
    "    for word in geo_words:\n",
    "        if word in tokens:\n",
    "            count += tokens.count(word)\n",
    "    \n",
    "    return count\n",
    "    \n",
    "    \n",
    "def race_words(text, race_terms):\n",
    "    count = 0\n",
    "    \n",
    "    #search and tabulate\n",
    "    for word in race_terms:\n",
    "        instances = re.findall(word, text)\n",
    "        count += len(instances)\n",
    "    \n",
    "    return count\n",
    "\n",
    "def get_stopwords(path):\n",
    "    f = open(path, encoding='utf-8')\n",
    "    words = f.read()\n",
    "    return re.split(r'\\n', words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata Corpora (choose either Fiction or Kindai Magazine Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1905, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD in metadata for the fiction corpus\n",
    "df = pd.read_excel(r\"Data\\Fiction_Meta.xlsx\", sheet_name='Sheet1')\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape\n",
    "\n",
    "#LOAD in metadata for kindai magazine corpus\n",
    "#df = pd.read_excel(r'Data\\Kindai_Meta.xlsx', sheet_name='Sheet1')\n",
    "#df = df[df['FILTER'] != 'YES']\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the source path where pre-tokenized or non-tokenized texts are stored\n",
    "CORPUS_PATH = r\"Corpora\\AozoraFictionTokenized\\\\\"\n",
    "#Set the output path where lemmatized texts will be stored\n",
    "OUTPUT = r\"Corpora\\UnidicLemma\\\\\"\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    #eliminate white space\n",
    "    raw = re.sub(r'\\s', '', raw)\n",
    "    \n",
    "    # create a list of all tokens\n",
    "    node = mecab.parseToNode(raw)\n",
    "    node = node.next\n",
    "    tokens = []\n",
    "    while node:\n",
    "        #extract the lemma form based on unidic parsing\n",
    "        if len(re.split(r',', node.feature)) > 6:  #some words don't have a lemma form\n",
    "            tokens.append(re.split(r',', node.feature)[7])\n",
    "            node = node.next\n",
    "        else:   #if not, just add the plain token\n",
    "            tokens.append(node.surface)\n",
    "            node = node.next\n",
    "    \n",
    "    #now print back out\n",
    "    with open(OUTPUT + str(df.WORK_ID[k]) + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in tokens:\n",
    "            f.write(token + ' ')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Number of Tokens and Word Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create column in data frame to store word token counts\n",
    "df['TOKENS'] = Series('', index=df.index)\n",
    "\n",
    "#set path where lemmatized texts are stored\n",
    "CORPUS_PATH = 'Corpora\\UnidicLemma\\\\'\n",
    "all_tokens = []\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(int(df.WORK_ID[k])) + '.txt'\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = re.split(r' ', text)\n",
    "    \n",
    "    #keep track of tokens per document\n",
    "    df.at[k, 'TOKENS'] = len(tokens)\n",
    "    \n",
    "    #add to global list\n",
    "    all_tokens += tokens \n",
    "\n",
    "print(len(all_tokens))\n",
    "types = set(all_tokens)\n",
    "print(len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Total and Document Frequency of Specific Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set path where corpus is stored\n",
    "CORPUS_PATH = r\"Corpora\\UnidicLemma\\\\\"\n",
    "\n",
    "keywords_dict = {'朝鮮人':[0,0], '中国人':[0,0], '西洋人':[0,0], '日本人':[0,0], '黒人':[0,0], '部落民':[0,0],\n",
    "                 '土人':[0,0], '東洋人':[0,0], '外国人':[0,0]}\n",
    "                 \n",
    "#iterate through all texts and count keywords; also keep track of number of texts in which keyword appears\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(int(df.WORK_ID[k])) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    for key in keywords_dict:\n",
    "        count = re.findall(key, raw)\n",
    "        keywords_dict[key][0] += len(count)  #first element is total count\n",
    "        #keep track of number of docs\n",
    "        if len(count) > 0:\n",
    "            keywords_dict[key][1] += 1      #second element is doc count\n",
    "        \n",
    "print(keywords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Categories of Words (e.g., racial terms; geographical terms) and add to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set path where corpus to be analyzed is stored\n",
    "CORPUS_PATH = r'Corpora\\UnidicLemma\\\\'\n",
    "\n",
    "#add columns to your data frame to store words counts\n",
    "df['RACEWORDS'] = Series('',index=df.index)\n",
    "df['GEO_WORDS'] = Series('',index=df.index)\n",
    "\n",
    "#specify list of racial identifiers to count; geography terms come from a pre-specified list \n",
    "race_terms = ['朝鮮人', '中国人', '西洋人','部落民','土人']\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    try:\n",
    "        source_text = CORPUS_PATH + str(int(df.WORK_ID[k])) + \".txt\"\n",
    "        raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "        raw = raw_text.read()\n",
    "        df.GEO_WORDS.loc[k] = geo_words(raw)\n",
    "        df.at[k, 'RACEWORDS'] = race_words(raw, race_terms)\n",
    "    except:\n",
    "        df.GEO_WORDS.loc[k] = 0\n",
    "        df.at[k, 'RACEWORDS'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'Results\\Fiction_Meta_Temp.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate Spaces for Key Race Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Owing to the ways that Unidic tokenizes, key race terms need to be condensed to single word units\n",
    "# This insures that they are treated as a single word in the ensuing analysis\n",
    "\n",
    "# Point to folder where lemmatized corpus is stored\n",
    "CORPUS_PATH = r\"Corpora\\UnidicLemma\\\\\"\n",
    "\n",
    "# order of search is important in some cases (e.g., tuples 3 and 4) to avoid writing over one term with another\n",
    "race_terms = [('朝鮮 人', '朝鮮人'), ('朝鮮 民族', '朝鮮民族'), ('朝鮮 の 人 々','朝鮮の人々'), ('朝鮮 の 人','朝鮮の人'),\n",
    "             ('朝鮮 の 人達','朝鮮の人達'),('半島 の 人達','半島の人達'),('鮮人 達','鮮人達'),('セン 女','鮮女'),('半島 人','半島人'),\n",
    "             ('中国 人', '中国人'), ('中国 民族','中国民族'),('中国 民衆', '中国民衆'),('中国 の 民衆','中国の民衆'),\n",
    "             ('中国 の 人 々','中国の人々'),('中国 の 人','中国の人'),('支那 人','支那人'),('支那 の 人','支那の人'),\n",
    "             ('支那 の 民族','支那の民族'), ('支那 の 民衆','支那の民衆'), ('西洋 人','西洋人'), ('西洋 の 人','西洋の人'),\n",
    "             ('西洋 の 人　々','西洋の人々'), ('オウシュウ-外国 人','欧州人'), ('外国 人','外国人'), ('日本 人','日本人'), \n",
    "             ('日本 の 人 々','日本の人々'), ('日本 の 人','日本の人'), ('ヤマト 民族','ヤマト民族'), ('日本 民族','日本民族'),\n",
    "             ('日本 の 民族','日本の民族'), ('日本 民衆','日本民衆'), ('日本 の 民衆','日本の民衆'),('日本 国民','日本国民'),\n",
    "             ('日本 の 国民','日本の国民'), ('ジャップ-Jap', 'ジャップ'), ('内地 人','内地人'),('新 平民','新平民'),('部落 民','部落民'),\n",
    "             ('アフリカ-Africa 人', 'アフリカ-Africa人'), ('東洋 人','東洋人'),('東洋 の 人','東洋の人'),('黄 人','黄人'),\n",
    "             ('黄 人種','黄人種'),('有色 人種','有色人種'), ('アジア-Asia 人','アジア-Asia人'),('野蛮 人','野蛮人'),\n",
    "             ('本島 人','本島人'),('タカサゴ 族','タカサゴ族'),('リュウキュウ 人','リュウキュウ人'), ('オキナワ 人','オキナワ人')]\n",
    "\n",
    "altered_docs = 0\n",
    "change = 0\n",
    "\n",
    "for k in df.index:\n",
    "    #get the text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    for pair in race_terms:\n",
    "        if re.search(pair[0], raw):\n",
    "            change = 1\n",
    "            raw = re.sub(pair[0], pair[1], raw)  #replace all instances of the race term with condensed version\n",
    "    \n",
    "    if change == 1:\n",
    "        altered_docs += 1\n",
    "    \n",
    "    #now print back out\n",
    "    with open(CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        \n",
    "    change = 0\n",
    "\n",
    "print(altered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Merge Racial Identifiers into Unified Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the race terms into single terms in the corpus; Must run previous cell first\n",
    "\n",
    "CORPUS_PATH = r\"Corpora\\UnidicLemma\\\\\"\n",
    "OUTPUT_PATH = r\"Corpora\\UnidicLemmaMerged\\\\\"\n",
    "\n",
    "#list of unified terms\n",
    "unified_terms = ['朝鮮人','中国人','西洋人','外国人','日本人','部落民','黒人','東洋人','土人']\n",
    "#set group number based on element in above list; 0 indicates first item\n",
    "race_group = 0\n",
    "\n",
    "#sub-lists of terms that will be replaced by unified term; order of sub-lists and unified terms must correspond \n",
    "all_merge_terms = [['朝鮮民族','朝鮮の人々','朝鮮の人',' 鮮人','半島人','朝鮮の人達','半島の人達','鮮人達','鮮女','在日','ヨボ',\n",
    "                    '韓人'],\n",
    "                   ['中国民族','中国民衆','中国の民衆','中国の人','中国の人々','シナ 人','支那人','支那の人','支那の民族',\n",
    "                    '支那の民衆','華僑'],\n",
    "                   ['西洋の人','白人','毛唐','欧州人','西洋の人々'],\n",
    "                   ['外人','異人'],\n",
    "                   ['日本の人々','日本の人','大和民族','日本民族','日本の民族','日本民衆','日本の民衆','日本国民',\n",
    "                    '日本の国民','ジャップ','和人','内地人','邦人'],\n",
    "                   ['穢多','非人','新平民'],\n",
    "                   ['黒ん坊','アフリカ-Africa人','ニグロ'],\n",
    "                   ['東洋の人','黄人','黄人種','有色人種','アジア-Asia人'],\n",
    "                   ['蕃人','蛮族','蕃族','生蕃','熟蕃','タカサゴ族','本島人',' 蛮人','野蛮人']]\n",
    "\n",
    "#select the set of merge terms based on race group\n",
    "merge_terms = all_merge_terms[race_group]\n",
    "\n",
    "#initialize some counters\n",
    "altered_docs = 0\n",
    "change = 0\n",
    "\n",
    "for k in df.index:\n",
    "    #get the text\n",
    "    source_text = CORPUS_PATH + df.FILE_ID[k]\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    #insert exceptions for 鮮人 and 蛮人 since they require an aditional space in front\n",
    "    for word in merge_terms:\n",
    "        if re.search(word, raw):\n",
    "            if word == ' 鮮人':\n",
    "                change = 1\n",
    "                raw = re.sub(word, ' ' + unified_terms[race_group], raw)\n",
    "            elif word == ' 蛮人':\n",
    "                change = 1\n",
    "                raw = re.sub(word, ' ' + unified_terms[race_group], raw)\n",
    "            else:\n",
    "                change = 1\n",
    "                raw = re.sub(word, unified_terms[race_group], raw)\n",
    "    \n",
    "    if change == 1:\n",
    "        altered_docs += 1\n",
    "    \n",
    "    #now print back out\n",
    "    with open(OUTPUT_PATH + df.FILE_ID[k], \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        \n",
    "    change = 0\n",
    "\n",
    "print(altered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Frequency Table for Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set path where corpus is stored\n",
    "CORPUS_PATH = r\"Corpora\\UnidicLemma\\\\\"\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "#set this flag before you run\n",
    "rmv_stopwords = True\n",
    "\n",
    "#specify stopword list to use (e.g., \"stopwords.txt\" for fiction and \"stopwordsNonFic.txt\" for Kindai magazine corpus)\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'WordLists\\stopwords.txt')\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets (need to do this first)\n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = re.split(r' ', text)\n",
    "    #while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    \n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "\n",
    "    #add to global list\n",
    "    all_tokens += tokens \n",
    "\n",
    "#produce the frequency list\n",
    "fdist = nltk.FreqDist(all_tokens)\n",
    "freq_pairs = fdist.items()\n",
    "sort_freq_pairs = sorted(freq_pairs, key=lambda x:x[1], reverse=True)  #sort by decreasing frequency\n",
    "\n",
    "token_count = len(all_tokens)\n",
    "\n",
    "#create a dictionary to store word-frequency pairs\n",
    "word_freqs = {}\n",
    "\n",
    "#fill dictionary with pairs\n",
    "for item in sort_freq_pairs:\n",
    "    word_freqs[item[0]] = item[1]\n",
    "    \n",
    "freqs_df = DataFrame.from_dict(word_freqs, orient='index')  #convert dict to dataframe\n",
    "freqs_df = freqs_df.rename(columns={0:'frequency'})    #rename column\n",
    "freqs_df = freqs_df.sort_values(by='frequency', ascending=False)   #sort by frequency\n",
    "\n",
    "#compute relative frequencies\n",
    "freqs_df['rel_freq'] = freqs_df.frequency / token_count\n",
    "freqs_df = freqs_df.reset_index()\n",
    "\n",
    "# OPTIONAL: print out the results to an excel file\n",
    "#import xlsxwriter\n",
    "#import openpyxl\n",
    "#writer = pd.ExcelWriter(r'Results\\LemmaFreqTable.xlsx', engine='xlsxwriter')\n",
    "#freqs_df.to_excel(writer, sheet_name='Sheet1')\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Concordance for Close Analysis of Word Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set location where lemmatized and term-unified corpus is stored\n",
    "CORPUS_PATH = r\"Corpora\\UnidicLemmaMerged\\\\\"\n",
    "\n",
    "#list of terms to include in concordance\n",
    "seed_terms = ['朝鮮人','中国人','西洋人','日本人','黒人','部落民','土人','東洋人','外国人']\n",
    "\n",
    "#set window size (# of word tokens) on each side of seed term\n",
    "window_size = 15\n",
    "\n",
    "#set this optional flag\n",
    "rmv_stopwords = True\n",
    "#specify stopword list to use (e.g., \"stopwords.txt\" for fiction and \"stopwordsNonFic.txt\" for Kindai magazine corpus)\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'WordLists\\stopwords.txt')\n",
    "\n",
    "#OPTIONAL: prepare file to print out results\n",
    "#f = open(r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\LemmaConcordance.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "#set up data structure to hold info\n",
    "metadata = {'text_id': [], 'seed_term': [], 'index_pos': [], 'left_tokens': [], 'right_tokens': []}\n",
    "\n",
    "hits = 0\n",
    "\n",
    "#iterate through all texts and count keywords; also keep track of number of texts in which keyword appears\n",
    "for k in df.index:\n",
    "    current_position = 0\n",
    "    \n",
    "    #get the text\n",
    "    text_id = str(df.WORK_ID[k]) + \".txt\"\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #tokenize the text\n",
    "    tokens = re.split(r' ', text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "    \n",
    "    #iterate through tokens in search of seed_terms\n",
    "    for token in tokens:\n",
    "        #check for seed term\n",
    "        if token in seed_terms:\n",
    "            hits += 1\n",
    "            #f.write(text_id + ' ' + token + ' ' + str(current_position) + '\\t')\n",
    "            metadata['text_id'].append(text_id)\n",
    "            metadata['seed_term'].append(token)\n",
    "            metadata['index_pos'].append(current_position)\n",
    "            #check to make sure we are not at start of text\n",
    "            if current_position - window_size > 0:\n",
    "                #f.write(' '.join(tokens[current_position-window_size:current_position]) + ' ')\n",
    "                metadata['left_tokens'].append(' '.join(tokens[current_position-window_size:current_position]))\n",
    "            else:\n",
    "                #f.write(' '.join(tokens[0:current_position]) + ' ')\n",
    "                metadata['left_tokens'].append(' '.join(tokens[0:current_position]))\n",
    "            \n",
    "            #f.write('SEED_TERM' + ' ')        \n",
    "            #check to make sure we are not at end of text\n",
    "            if current_position + window_size < len(tokens):\n",
    "                #f.write(' '.join(tokens[current_position+1:current_position+window_size]) + '\\n')\n",
    "                metadata['right_tokens'].append(' '.join(tokens[current_position+1:current_position+window_size]))\n",
    "            else:\n",
    "                #f.write(' '.join(tokens[current_position+1:]) + '\\n')\n",
    "                metadata['right_tokens'].append(' '.join(tokens[current_position+1:]))\n",
    "            \n",
    "        current_position += 1\n",
    "\n",
    "#f.close()\n",
    "\n",
    "#turn results into a Data Frame and sort\n",
    "concord_df = DataFrame(metadata, columns=['text_id', 'seed_term', 'index_pos', 'left_tokens', 'right_tokens'])\n",
    "concord_df = concord_df.sort_values(by = [\"seed_term\", \"text_id\", \"index_pos\"])\n",
    "concord_df = concord_df.reset_index(drop=True)\n",
    "#concord_df[0:5]  #inspect first 5 rows\n",
    "\n",
    "# OPTIONAL: print out the results to an excel file\n",
    "#import xlsxwriter\n",
    "#import openpyxl\n",
    "#writer = pd.ExcelWriter(r'Results\\LemmaConcordance.xlsx', engine='xlsxwriter')\n",
    "#concord_df.to_excel(writer, sheet_name='Sheet1')\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing Code\n",
    "\n",
    "<p style='text-align: justify;'>The following cells contain code for working with word-embedding models and for further analysis once significant semantic clusters have been identified.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Single Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim #For word2vec, etc\n",
    "\n",
    "# load a pre-saved word embedding model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(r'./results_fic_bootstrap/word2vec.txt') #for fiction\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(r'./results_kindai_bootstrap/word2vec.txt') #for non-fiction\n",
    "\n",
    "#get 20 words closest to specified word\n",
    "model.most_similar('日本人', topn=20) \n",
    "#get 10 words most similar to \"Japan\" vector minus combined vector of other words\n",
    "model.most_similar(positive=['日本'], negative=['下等','頭数','病人','ごろつき','坑夫','醜い'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Passages where Semantic Cluster Terms appear in Context Window of Racial Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set path where lemmatized corpus is stored (and where race terms have been unified)\n",
    "CORPUS_PATH = r\"Corpora\\UnidicLemmaMerged\\\\\"\n",
    "\n",
    "# specify racial identifier of interest\n",
    "race_term = '土人'\n",
    "\n",
    "#specify list of terms in semantic cluster of interest\n",
    "atom_terms = ['音','聞こえる','叫び声','わっ','声','悲鳴','喚く','わあ','響く','鳴る','どっと','叫び','泣き声','響き']\n",
    "\n",
    "#set window size on each side of race term\n",
    "window_size = 20\n",
    "\n",
    "rmv_stopwords = False\n",
    "#specify stopword list to use (e.g., \"stopwords.txt\" for fiction and \"stopwordsNonFic.txt\" for Kindai magazine corpus)\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'WordLists\\stopwords.txt')\n",
    "\n",
    "#prepare file to print out results; Name should indicate racial identifier and semantic cluster label\n",
    "f = open(r\"ClusterContextsFiction\\NativeVoice.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "#set up data structure to hold info\n",
    "metadata = {'text_id': [], 'race_term': [], 'left_tokens': [], 'right_tokens': []}\n",
    "\n",
    "hits = 0\n",
    "\n",
    "#iterate through all texts and find instances where any atom_terms appears within 20 characters of race term\n",
    "for k in df.index:\n",
    "    \n",
    "    #get the text\n",
    "    text_id = str(df.WORK_ID[k]) + \".txt\"\n",
    "    title = df.WORK_TITLE[k]\n",
    "    year = df.PUBL_START[k]\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "    text = cleaner(text)          #remove punctuation\n",
    "    tokens = re.split(r' ', text) #tokenize the text\n",
    "    \n",
    "    #remove stopwords\n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "    \n",
    "    #get index positions of all race terms in text\n",
    "    index_positions = [i for i, x in enumerate(tokens) if x == race_term] \n",
    "    \n",
    "    #check to see if race term is in the text; skip otherwise\n",
    "    if index_positions:\n",
    "        for cur_pos in index_positions:    #iterate through all race terms\n",
    "            \n",
    "            #get tokens to the left of race term\n",
    "            if cur_pos - window_size > 0:\n",
    "                left_tokens = ' '.join(tokens[cur_pos-window_size:cur_pos])\n",
    "            else:\n",
    "                left_tokens = ' '.join(tokens[0:cur_pos])\n",
    "        \n",
    "            #get tokens to the right of race term\n",
    "            if cur_pos + window_size < len(tokens):\n",
    "                right_tokens = ' '.join(tokens[cur_pos+1:cur_pos+window_size])\n",
    "            else:\n",
    "                right_tokens = ' '.join(tokens[cur_pos+1:])\n",
    "                \n",
    "            #check to see if any atom words are in left and right tokens\n",
    "            for word in atom_terms:\n",
    "                if word in left_tokens or word in right_tokens:\n",
    "                    metadata['text_id'].append(text_id)\n",
    "                    metadata['race_term'].append(race_term)\n",
    "                    metadata['left_tokens'].append(left_tokens)\n",
    "                    metadata['right_tokens'].append(right_tokens)\n",
    "                    \n",
    "                    hits+=1  #record as a hit\n",
    "                    \n",
    "                    #print the results to a file\n",
    "                    f.write(text_id + ' ' + str(title) + ' ' + str(year) + ' ' + word + '\\n')\n",
    "                    f.write(left_tokens + ' ' + race_term + ' ' + right_tokens + '\\n\\n')\n",
    "                    \n",
    "                    #try to grab the corresponding passage in original base version of text\n",
    "                    CORPUS_DIR = r\"Corpora\\AozoraFictionTokenized\\\\\"\n",
    "                    #merge target passage into a single string\n",
    "                    target_passage = left_tokens + ' ' + race_term + ' ' + right_tokens\n",
    "                    target_passage = re.split(r' ', target_passage)\n",
    "                    \n",
    "                    #open the corresponding tokenized text\n",
    "                    source = CORPUS_DIR + str(df.WORK_ID[k]) + \".txt\"\n",
    "                    raw = open(source, encoding=\"utf-8\")       #grab text\n",
    "                    new_text = raw.read()\n",
    "    \n",
    "                    sim_scores = {}\n",
    "                    \n",
    "                    new_text = bracket_cleaner(new_text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "                    #remove punctuation\n",
    "                    #new_text = cleaner(new_text)\n",
    "    \n",
    "                    #tokenize the text\n",
    "                    new_tokens = re.split(r' ', new_text)\n",
    "                   \n",
    "                    sliding_window_size = 2 * window_size\n",
    "                   \n",
    "                    #calcuate similarity across all passages of a given window size\n",
    "                    for i in range(len(new_tokens) - (sliding_window_size - 1)):\n",
    "                        source_passage = new_tokens[i:i+sliding_window_size]\n",
    "                        \n",
    "                        #make sure passages are equal length for similarity comparison\n",
    "                        diff = abs(len(target_passage)-len(source_passage))\n",
    "                        if len(target_passage) > len(source_passage):\n",
    "                            source_passage.extend(diff*['*'])\n",
    "                        elif len(source_passage) > len(target_passage):\n",
    "                            target_passage.extend(diff*['*'])\n",
    "                        \n",
    "                        j_score = jaccard_similarity_score(target_passage, source_passage)\n",
    "                        sim_scores[i] = j_score\n",
    "    \n",
    "                    #now rank by similarity score\n",
    "                    sim_df = DataFrame.from_dict(sim_scores, orient='index')  #convert dict to dataframe\n",
    "                    sim_df = sim_df.rename(columns={0:'j_score'})    #rename column\n",
    "                    sim_df = sim_df.sort_values(by='j_score', ascending=False)   #sort by frequency\n",
    "                    #get top most similar passage\n",
    "                    top_sim_index = sim_df.index[0:1].tolist()[0]           \n",
    "                    #grab the corresponding window (plus 20 words before and after)\n",
    "                    matching_passage = new_tokens[top_sim_index - 20:top_sim_index + 60]\n",
    "                    f.write('---' + ''.join(matching_passage) + '\\n\\n')\n",
    "                   \n",
    "                    break   #exit loop; no need to check the other atom_terms\n",
    "            \n",
    "f.close()\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Passages where Semantic Cluster Terms appear in Context Window of Character Name\n",
    "\n",
    "<p style='text-align: justify;'>This cell draws on texts that have been previously annotated for character names. This process is handled in a separate ipython notebook, as described in the <b>Read Me</b> file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify folder where character annotated texts are stored\n",
    "CORPUS_PATH = r\"Corpora\\CharAnnot\\\\\"\n",
    "\n",
    "#reduce data frame (only for Fiction) to texts with character annotation and where occurrences of \"native\" >= 5\n",
    "df = df[df['FILTER'] == 'char_annot']\n",
    "df = df[df['NATIVE'] >= 10]\n",
    "\n",
    "#set list of semantic cluster terms to search for\n",
    "atom_terms = ['音','聞こえる','叫び声','わっ','声','悲鳴','喚く','わあ','響く','鳴る','どっと']\n",
    "\n",
    "#specify window size on each side of character reference\n",
    "window_size = 15\n",
    "\n",
    "#optional flag for removing stop words from results\n",
    "rmv_stopwords = False\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'WordLists\\stopwords.txt')\n",
    "\n",
    "#prepare file to print out results\n",
    "f = open(r\"ClusterContextsFiction\\CharNativeVoice.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "#set up data structure to hold info\n",
    "metadata = {'text_id': [], 'char_name': [], 'left_tokens': [], 'right_tokens': []}\n",
    "\n",
    "total_hits = 0\n",
    "\n",
    "#iterate through all texts and find instances where any atom_terms appears within 20 characters of identified character name\n",
    "for k in df.index:\n",
    "    \n",
    "    #get the text\n",
    "    text_id = str(df.WORK_ID[k]) + \".txt\"\n",
    "    title = df.WORK_TITLE[k]\n",
    "    year = df.PUBL_START[k]\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    work_hits = 0\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "    #text = cleaner(text)          #remove punctuation\n",
    "    tokens = re.split(r' ', text) #tokenize the text\n",
    "    \n",
    "    #remove stopwords\n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "    \n",
    "    #get index positions of all character names in text\n",
    "    all_positions = [i for i, x in enumerate(tokens) if re.search(r'[^\\s]*_[^\\s]*', x)]\n",
    "    \n",
    "    #remove index positions within 15 positions from one another to avoid double counting\n",
    "    index_positions = [x for i, x in reversed(list(enumerate(all_positions))) if abs(x - all_positions[i-1]) > 15]\n",
    "    index_positions.reverse()  #reverse the order back to increasing\n",
    "    \n",
    "    #check to see if character names are in the text; skip otherwise\n",
    "    if index_positions:\n",
    "        \n",
    "        for cur_pos in index_positions:    #iterate through all character names\n",
    "            \n",
    "            #get tokens to the left of character name\n",
    "            if cur_pos - window_size > 0:\n",
    "                left_tokens = ' '.join(tokens[cur_pos-window_size:cur_pos])\n",
    "            else:\n",
    "                left_tokens = ' '.join(tokens[0:cur_pos])\n",
    "        \n",
    "            #get tokens to the right of character name\n",
    "            if cur_pos + window_size < len(tokens):\n",
    "                right_tokens = ' '.join(tokens[cur_pos+1:cur_pos+window_size])\n",
    "            else:\n",
    "                right_tokens = ' '.join(tokens[cur_pos+1:])\n",
    "            \n",
    "            #check to see if any atom words are in left and right tokens\n",
    "            for word in atom_terms:\n",
    "                if word in left_tokens or word in right_tokens:\n",
    "                    metadata['text_id'].append(text_id)\n",
    "                    metadata['char_name'].append(tokens[cur_pos])\n",
    "                    metadata['left_tokens'].append(left_tokens)\n",
    "                    metadata['right_tokens'].append(right_tokens)\n",
    "                    \n",
    "                    total_hits += 1  #record as a hit\n",
    "                    work_hits += 1\n",
    "                    \n",
    "                    #print the results to a file\n",
    "                    f.write(text_id + ' ' + str(title) + ' ' + str(year) + ' ' + str(cur_pos) + ' ' + word + '\\n')\n",
    "                    f.write(left_tokens + ' ' + re.split('_',tokens[cur_pos])[0] + ' ' + right_tokens + '\\n\\n')\n",
    "            \n",
    "                    #try to grab the corresponding passage in non-lemmatized version of text\n",
    "                    CORPUS_DIR = r\"Corpora\\AozoraFictionTokenized\\\\\"\n",
    "                \n",
    "                    #open the corresponding tokenized text\n",
    "                    source = CORPUS_DIR + str(df.WORK_ID[k]) + \".txt\"\n",
    "                    raw = open(source, encoding=\"utf-8\")       #grab text\n",
    "                    new_text = raw.read()\n",
    "                    new_text = bracket_cleaner(new_text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "                    new_text = strip_chap_titles(new_text)\n",
    "                    #remove punctuation\n",
    "                    #new_text = cleaner(new_text)\n",
    "    \n",
    "                    #tokenize the text\n",
    "                    new_tokens = re.split(r' ', new_text)\n",
    "            \n",
    "                    #merge target passage into a single string\n",
    "                    target_passage = left_tokens + ' ' + re.split('_',tokens[cur_pos])[0] + ' ' + right_tokens\n",
    "                    target_passage = re.split(r' ', target_passage)  #tokenize\n",
    "                    \n",
    "                    #set window of text to scan based on cur_pos in lemmatized text\n",
    "                    if cur_pos < 2000:  #we're at start of text\n",
    "                        search_area = new_tokens[0:cur_pos+2000]\n",
    "                    elif (len(new_tokens) - cur_pos) < 2000:  #we're at end of text \n",
    "                        search_area = new_tokens[-2000:]\n",
    "                    else:\n",
    "                        search_area = new_tokens[cur_pos - 2000:cur_pos + 2000]\n",
    "                    \n",
    "                    sim_scores = {}\n",
    "                    sliding_window_size = 2 * window_size\n",
    "                    \n",
    "                    #calcuate similarity across all passages of a given window size\n",
    "                    for i in range(len(search_area) - (sliding_window_size - 1)):\n",
    "                        source_passage = search_area[i:i+sliding_window_size]\n",
    "                        j_score = jaccard_similarity_score(target_passage, source_passage)\n",
    "                        sim_scores[i] = j_score\n",
    "    \n",
    "                    #now rank by similarity score\n",
    "                    sim_df = DataFrame.from_dict(sim_scores, orient='index')  #convert dict to dataframe\n",
    "                    sim_df = sim_df.rename(columns={0:'j_score'})    #rename column\n",
    "                    sim_df = sim_df.sort_values(by='j_score', ascending=False)   #sort by frequency\n",
    "                    #get top most similar passage\n",
    "                    top_sim_index = sim_df.index[0:1].tolist()[0]           \n",
    "                    #grab the corresponding window (plus 30 words before and after)\n",
    "                    matching_passage = search_area[top_sim_index - 30:top_sim_index + 70]\n",
    "                    f.write('---' + ''.join(matching_passage) + '\\n\\n')\n",
    "                \n",
    "                    break   #exit loop; no need to check the other atom_terms\n",
    "    \n",
    "    #output results\n",
    "    if index_positions:\n",
    "        print(title, len(index_positions), work_hits, work_hits/len(index_positions))\n",
    "    else:\n",
    "        print(title, \"No Characters Annotated\")\n",
    "            \n",
    "f.close()\n",
    "print(total_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Cluster Detection Output File for Analysis in Excel\n",
    "\n",
    "<p style='text-align: justify;'>This cell reads the output from the \"Cluster_Detection_Fic.py\" or \"Cluster_Detection_Kindai.py\" files and converts it into a more readable spreadsheet format.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify the output file to process\n",
    "path = r\"results_fic_bootstrap\\kfree_thres0.69_numtest5000_window20_alpha0.05_overlap_percent0.50.txt\"\n",
    "\n",
    "#create dictionary to store data\n",
    "results_dict = {}\n",
    "\n",
    "f = open(path, encoding='utf-8')\n",
    "raw = f.read()\n",
    "results = re.split(r'\\n\\n', raw)[0:-1]  #exclude the last entry, which is just a newline\n",
    "\n",
    "#process each line of output, excluding instances where semantic cluster term does not appear around race term\n",
    "for result in results:\n",
    "    temp = re.split(r':\\n', result)  #split title from atom results\n",
    "    pair = re.findall(r'[^\\s]*\\sthan\\s[^\\s]*', temp[0]) #extract the comparison terms\n",
    "    pair_string = '_'.join(re.split(r'\\s', pair[0])) #create identifier string for dictionary\n",
    "    results_dict[pair_string] = []  #create an empty list for this identifier\n",
    "    \n",
    "    #make sure there are atoms for this pair\n",
    "    if len(temp) > 1:\n",
    "        atoms = re.split(r'-\\s', temp[1])[1:] #retrieve all atoms and split\n",
    "    \n",
    "        for atom in atoms:\n",
    "            atom_freq = 0  #variable to keep track of strength of atom terms\n",
    "        \n",
    "            terms = re.split(r'[^,]\\s', atom)[0:-1]\n",
    "            keywords = []\n",
    "            for term in terms:\n",
    "                temp2 = re.split(r'[\\((,\\s)]', term)\n",
    "                if float(temp2[1]) > 0:  # or float(temp2[3]) > 0:  #get only words that appear around race term\n",
    "                    keywords.append(temp2[0])  #add the keyword to list\n",
    "                    atom_freq += float(temp2[1])  #add first freq to total\n",
    "        \n",
    "            #add keywords of this atom to list in dictionary\n",
    "            results_dict[pair_string].append((keywords, atom_freq))\n",
    "            \n",
    "#turn dictionary into a dataframe\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "writer = pd.ExcelWriter(r'results_fic_bootstrap\\Fiction_Clusters.xlsx', engine='xlsxwriter')\n",
    "results_df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
