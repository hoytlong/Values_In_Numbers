{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### imports\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import codecs\n",
    "import sys\n",
    "import re\n",
    "from shutil import copyfile\n",
    "from __future__ import division\n",
    "#from __future__ import print_function\n",
    "#sys.stdout = codecs.getwriter('utf_8')(sys.stdout)\n",
    "#sys.stdin = codecs.getreader('utf_8')(sys.stdin)\n",
    "import MeCab  #CHECK \"MECABRC\" FILE TO SEE WHICH DICTIONARY YOU ARE USING\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic\n",
    "#mecab = MeCab.Tagger(\"-Ochasen\")  #using MeCab's ipadic\n",
    "import collections\n",
    "import operator\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some cleaning and pre-processing functions for Japanese corpus\n",
    "\n",
    "#this first one should only be used for non-tokenized texts; basically cleans them for tokenization step\n",
    "def strip_chap_titles(raw):\n",
    "    #get rid of chapter titles that use Chinese numbers with or without surronding parantheses\n",
    "    raw = re.sub(r'（*([一二三四五六七八九十])+(）)*\\n', '', raw)\n",
    "    #get rid of chapter titles that use utf-8 alpha-numeric numbers\n",
    "    raw = re.sub(r'[１-９]+\\n', '', raw)\n",
    "    raw = re.sub(r'[第弐拾章参壱一二三四五六七八九十]+\\n', '', raw)\n",
    "    raw = re.sub(r'『', r'「', raw)   #replace all 『 with 「\n",
    "    raw = re.sub(r'』', r'」', raw)   #replace all 』 with 」\n",
    "    raw = re.sub(r'\\n', '', raw)  #strips all newlines\n",
    "    raw = re.sub(r'\\r', '', raw)  #strips all returns\n",
    "    #raw = re.sub(r'\\s', '', raw)\n",
    "    return raw\n",
    "\n",
    "#might need to run this separately for already tokenized files\n",
    "def bracket_cleaner(raw):\n",
    "    raw = re.sub(r'［[^］]+］', '', raw)   #replace annotations in brackets ([#...])\n",
    "    raw = re.sub(r'\\s+', ' ', raw)                         #get rid of double spaces\n",
    "    return raw\n",
    "\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','<','〔','〕',\n",
    "         '＃','△','※','＊']\n",
    "\n",
    "def cleaner(text):\n",
    "    for punc in puncs:\n",
    "        text = re.sub(punc, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)                         #get rid of double spaces\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    new_list = [token for token in tokens if token not in stopwords]\n",
    "    return new_list\n",
    "\n",
    "#this function computes percentage of text that is dialogue\n",
    "def percent_dialogue(text):\n",
    "    no_quotes = re.sub(r'「[^」]*」', '', text)   #eliminate all dialogue passages for single bracket quotes\n",
    "    per_dialogue = (len(text)-len(no_quotes))/len(text)\n",
    "    return per_dialogue\n",
    "\n",
    "def punct(text):\n",
    "    punctuation = ['、','。','…','！','？']\n",
    "    count = 0\n",
    "    \n",
    "    #strip dialogue\n",
    "    no_dialogue = re.sub(r'「[^」]*」', '', text)\n",
    "    \n",
    "    #search and tabulate first and third person usage\n",
    "    for word in punctuation:\n",
    "        instances = re.findall(word, no_dialogue)\n",
    "        count += len(instances)\n",
    "\n",
    "    return count/len(no_dialogue)\n",
    "\n",
    "def geo_words(text):\n",
    "    path = r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\Empire_Place_Names.txt'\n",
    "    f = open(path, encoding='utf-8')\n",
    "    words = f.read()\n",
    "    geo_words = re.split(r'\\n', words)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    #grab instances of \"colonies\" first\n",
    "    instances = re.findall(r'植民　地', text)\n",
    "    count += len(instances)\n",
    "    \n",
    "    #tokenize text and search for geographic names\n",
    "    tokens = re.split(r'\\s', text)\n",
    "    \n",
    "    for word in geo_words:\n",
    "        if word in tokens:\n",
    "            count += tokens.count(word)\n",
    "    \n",
    "    return count    \n",
    "    \n",
    "def race_words(text, race_terms):\n",
    "    #race_words = ['西洋人','外国人']    #'朝鮮人','中国人','西洋人','黒人','部落民','土人','東洋人','外国人','アイヌ-Ainu']\n",
    "    count = 0\n",
    "    \n",
    "    #search and tabulate\n",
    "    for word in race_terms:\n",
    "        instances = re.findall(word, text)\n",
    "        count += len(instances)\n",
    "    \n",
    "    #tokens = re.split(r'\\s', text)\n",
    "\n",
    "    return count  #/len(tokens)\n",
    "\n",
    "def get_stopwords(path):\n",
    "    f = open(path, encoding='utf-8')\n",
    "    words = f.read()\n",
    "    return re.split(r'\\n', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8867, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD in metadata for the kindai zasshi corpus\n",
    "\n",
    "df = pd.read_excel(r'Kindai_Meta.xlsx', sheetname='Sheet1')\n",
    "#df = df[df['YEAR'] < 1960]\n",
    "#df = df[df['YEAR'] > 1875]\n",
    "df = df[df['FILTER'] != 'YES']\n",
    "\n",
    "#add features that you are measuring\n",
    "#df['GEO_WORDS'] = Series('',index=df.index)\n",
    "df['RACEWORDS'] = Series('',index=df.index)\n",
    "df['JAPANESE'] = Series('',index=df.index)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Token and Type Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoyt\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21964304\n",
      "122951\n"
     ]
    }
   ],
   "source": [
    "df['TOKENS'] = Series('', index=df.index)\n",
    "\n",
    "CORPUS_PATH = './KindaiLemmaMerge/'\n",
    "all_tokens = []\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(df.FILE_ID[k])\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = re.split(r' ', text)\n",
    "    \n",
    "    #keep track of tokens per document\n",
    "    df.at[k, 'TOKENS'] = len(tokens)\n",
    "    \n",
    "    #add to global list\n",
    "    all_tokens += tokens \n",
    "\n",
    "print(len(all_tokens))\n",
    "types = set(all_tokens)\n",
    "print(len(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\KindaiMetaTemp.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count selected keywords in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'漢人': [64, 32], '夷狄': [57, 25], '韓民': [18, 9], '清国 人': [75, 47], '清国 民': [12, 9], '夷': [658, 284], 'チュウゴク 人': [43, 26]}\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "#code to COUNT selected keywords in corpus\n",
    "############################################\n",
    "\n",
    "CORPUS_PATH = r\"KindaiLemmaMerge\\\\\"\n",
    "\n",
    "#keywords_dict = {'中国人':[0,0], '中国 人':[0,0], '中国 民族':[0,0],'中国 民衆':[0,0],'中国 の 民衆':[0,0],'中国 の 人':[0,0],\n",
    "#                 '中国 の 人々':[0,0],'シナ 人':[0,0],'支那 人':[0,0],'支那 の 人':[0,0],'支那 の 民族':[0,0],\n",
    "#                 '支那 の 民衆':[0,0], '華僑':[0,0]}\n",
    "\n",
    "#keywords_dict = {'朝朝鮮人':[0,0], '朝鮮 人':[0,0],'朝鮮 民族':[0,0],'朝鮮 の 人':[0,0],'朝鮮 の 人々':[0,0],'朝鮮 民族':[0,0],\n",
    "#                 '鮮人':[0,0],'在日':[0,0], '韓国 人':[0,0], '韓人':[0,0]}\n",
    "\n",
    "#keywords_dict = {'西洋 人':[0,0],'西洋 の 人':[0,0],'白人':[0,0],'ハイカラ':[0,0],'毛唐':[0,0],'外国 人':[0,0],'外人':[0,0],\n",
    "#                 '欧州 人':[0,0],'オウシュウ-外国 人':[0,0],'外 人':[0,0]}\n",
    "\n",
    "#keywords_dict = {'日本 人':[0,0],'日本 の 人々':[0,0],'日本 の 人':[0,0],'大和 民族':[0,0],'大和 （ やまと ） 民族':[0,0],\n",
    "#                 '日本 民族':[0,0],'日本 の 民族':[0,0],'日本 民衆':[0,0],'日本 の 民衆':[0,0],'日本 国民':[0,0],\n",
    "#                 '日本 の 国民':[0,0],'ジャップ':[0,0],'和人':[0,0],'内地 人':[0,0],'日系':[0,0], '邦人':[0,0]}\n",
    "\n",
    "#keywords_dict = {'穢 多':[0,0],'非人':[0,0],'新 平民':[0,0],'部落 民':[0,0],'平民':[0,0],'アイヌ':[0,0],'偉人':[0,0],\n",
    "#                 '蝦夷':[0,0],'気違い':[0,0],'リュウキュウ 人':[0,0],'オキナワ 人':[0,0],'オキナワ の 人':[0,0],'四足':[0,0]}\n",
    "\n",
    "#keywords_dict = {'黒人':[0,0], '黒ん坊':[0,0], 'ニグロ':[0,0], 'アフリカ 人':[0,0], '土人':[0,0],'混血':[0,0]} \n",
    "\n",
    "#keywords_dict = {'アジア 人':[0,0],'東洋 人':[0,0],'東洋 の 人':[0,0],'黄 人':[0,0],'黄 人種':[0,0],'有色 人種':[0,0],\n",
    "#                 '異 人種':[0,0]}\n",
    "\n",
    "#keywords_dict = {'蕃人':[0,0], '蛮族':[0,0], '生蕃':[0,0], '熟蕃':[0,0], 'タカサゴ 族':[0,0], '本島 人':[0,0],\n",
    "#                 '蛮人':[0,0], '野蛮 人':[0,0]}\n",
    "\n",
    "#keywords_dict = {'朝鮮人':[0,0], '中国人':[0,0], '西洋人':[0,0], '日本人':[0,0], '黒人':[0,0], '部落民':[0,0],\n",
    "#                 '土人':[0,0], '東洋人':[0,0], '外国人':[0,0]}\n",
    "\n",
    "keywords_dict = {'漢人':[0,0], '夷狄':[0,0], '韓民':[0,0], '清国 人':[0,0],'清国 民':[0,0], '夷':[0,0], 'チュウゴク 人':[0,0]}\n",
    "                                 \n",
    "#iterate through all texts and count keywords; also keep track of number of texts in which keyword appears\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + df.FILE_ID[k]\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    for key in keywords_dict:\n",
    "        count = re.findall(key, raw)\n",
    "        keywords_dict[key][0] += len(count)  #first element is total count\n",
    "        #keep track of number of docs\n",
    "        if len(count) > 0:\n",
    "            keywords_dict[key][1] += 1      #second element is doc count\n",
    "            #print(df.FILE_ID[k])\n",
    "        \n",
    "print(keywords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove spaces between key race terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2890\n"
     ]
    }
   ],
   "source": [
    "# condense key race terms into single word units\n",
    "\n",
    "CORPUS_PATH = r\"kindai_lemma_raw\\\\\"\n",
    "OUTPUT_PATH = r\"kindai_lemma_merge_race_terms\\\\\"\n",
    "\n",
    "#keep in mind that order is important in some of these cases (e.g., tuples 3 and 4)\n",
    "\n",
    "race_terms = [('朝鮮 人', '朝鮮人'), ('朝鮮 民族', '朝鮮民族'), ('朝鮮 の 人 々','朝鮮の人々'), ('朝鮮 の 人','朝鮮の人'),\n",
    "             ('朝鮮 の 人達','朝鮮の人達'),('半島 の 人達','半島の人達'),('鮮人 達','鮮人達'),('セン 女','鮮女'),('半島 人','半島人'),\n",
    "             ('中国 人', '中国人'), ('中国 民族','中国民族'),('中国 民衆', '中国民衆'),('中国 の 民衆','中国の民衆'),\n",
    "             ('中国 の 人 々','中国の人々'),('中国 の 人','中国の人'),('支那 人','支那人'),('支那 の 人','支那の人'),\n",
    "             ('支那 の 民族','支那の民族'), ('支那 の 民衆','支那の民衆'), ('西洋 人','西洋人'), ('西洋 の 人','西洋の人'),\n",
    "             ('西洋 の 人　々','西洋の人々'), ('オウシュウ-外国 人','欧州人'), ('外国 人','外国人'), ('日本 人','日本人'), \n",
    "             ('日本 の 人 々','日本の人々'), ('日本 の 人','日本の人'), ('ヤマト 民族','ヤマト民族'), ('日本 民族','日本民族'),\n",
    "             ('日本 の 民族','日本の民族'), ('日本 民衆','日本民衆'), ('日本 の 民衆','日本の民衆'),('日本 国民','日本国民'),\n",
    "             ('日本 の 国民','日本の国民'), ('ジャップ-Jap', 'ジャップ'), ('内地 人','内地人'),('新 平民','新平民'),('部落 民','部落民'),\n",
    "             ('アフリカ-Africa 人', 'アフリカ-Africa人'), ('東洋 人','東洋人'),('東洋 の 人','東洋の人'),('黄 人','黄人'),\n",
    "             ('黄 人種','黄人種'),('有色 人種','有色人種'), ('アジア-Asia 人','アジア-Asia人'),('野蛮 人','野蛮人'),\n",
    "             ('本島 人','本島人'),('タカサゴ 族','タカサゴ族'),('リュウキュウ 人','リュウキュウ人'), ('オキナワ 人','オキナワ人')]\n",
    "\n",
    "altered_docs = 0\n",
    "change = 0\n",
    "\n",
    "for k in df.index:\n",
    "    #get the text\n",
    "    source_text = CORPUS_PATH + df.FILE_ID[k]\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    for pair in race_terms:\n",
    "        if re.search(pair[0], raw):\n",
    "            change = 1\n",
    "            raw = re.sub(pair[0], pair[1], raw)  #replace all instances of the race term with condensed version\n",
    "    \n",
    "    if change == 1:\n",
    "        altered_docs += 1\n",
    "    \n",
    "    #now print back out\n",
    "    with open(OUTPUT_PATH + df.FILE_ID[k], \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        \n",
    "    change = 0\n",
    "\n",
    "print(altered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Race Terms into Single Race Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# Merge the race terms into single terms in the corpus\n",
    "\n",
    "CORPUS_PATH = r\"kindai_lemma_merge_race_terms\\\\\"\n",
    "\n",
    "unified_terms = ['朝鮮人','中国人','西洋人','外国人','日本人','部落民','黒人','東洋人','土人']\n",
    "#set group number based on element in above list\n",
    "race_group = 8\n",
    "\n",
    "#note that I have already eliminated spaces for these words in the above cell; \n",
    "#otherwise you would need to include spaces according to how unidic tokenizes these words \n",
    "\n",
    "all_merge_terms = [['朝鮮民族','朝鮮の人々','朝鮮の人',' 鮮人','半島人','朝鮮の人達','半島の人達','鮮人達','鮮女','在日','ヨボ',\n",
    "                    '韓人'],\n",
    "                   ['中国民族','中国民衆','中国の民衆','中国の人','中国の人々','シナ 人','支那人','支那の人','支那の民族',\n",
    "                    '支那の民衆','華僑'],\n",
    "                   ['西洋の人','白人','毛唐','欧州人','西洋の人々'],\n",
    "                   ['外人','異人'],\n",
    "                   ['日本の人々','日本の人','大和民族','日本民族','日本の民族','日本民衆','日本の民衆','日本国民',\n",
    "                    '日本の国民','ジャップ','和人','内地人','邦人'],\n",
    "                   ['穢多','非人','新平民'],\n",
    "                   ['黒ん坊','アフリカ-Africa人','ニグロ'],\n",
    "                   ['東洋の人','黄人','黄人種','有色人種','アジア-Asia人'],\n",
    "                   ['蕃人','蛮族','蕃族','生蕃','熟蕃','タカサゴ族','本島人',' 蛮人','野蛮人']]\n",
    "\n",
    "#select the set of merge terms based on race group\n",
    "merge_terms = all_merge_terms[race_group]\n",
    "\n",
    "#initialize some counters\n",
    "altered_docs = 0\n",
    "change = 0\n",
    "\n",
    "for k in df.index:\n",
    "    #get the text\n",
    "    source_text = CORPUS_PATH + df.FILE_ID[k]\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    #insert exceptions for 鮮人 and 蛮人 since they require an aditional space in front\n",
    "    for word in merge_terms:\n",
    "        if re.search(word, raw):\n",
    "            if word == ' 鮮人':\n",
    "                change = 1\n",
    "                raw = re.sub(word, ' ' + unified_terms[race_group], raw)\n",
    "            elif word == ' 蛮人':\n",
    "                change = 1\n",
    "                raw = re.sub(word, ' ' + unified_terms[race_group], raw)\n",
    "            else:\n",
    "                change = 1\n",
    "                raw = re.sub(word, unified_terms[race_group], raw)\n",
    "    \n",
    "    if change == 1:\n",
    "        altered_docs += 1\n",
    "    \n",
    "    #now print back out\n",
    "    with open(CORPUS_PATH + df.FILE_ID[k], \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        \n",
    "    change = 0\n",
    "\n",
    "print(altered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoyt\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#if you're working on unidic tokenized corpus\n",
    "CORPUS_PATH = r\"KindaiLemmaMerge\\\\\"\n",
    "\n",
    "race_terms = ['朝鮮人', '中国人', '西洋人','部落民','土人']\n",
    "japanese = ['日本人']\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    try:\n",
    "        source_text = CORPUS_PATH + df.FILE_ID[k]\n",
    "        raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "        raw = raw_text.read()\n",
    "        #df.GEO_WORDS.loc[k] = geo_words(raw)\n",
    "        df.RACEWORDS.loc[k] = race_words(raw, race_terms)\n",
    "        df.JAPANESE.loc[k] = race_words(raw, japanese)\n",
    "    except:\n",
    "        #df.GEO_WORDS.loc[k] = 0\n",
    "        df.RACEWORDS.loc[k] = 0\n",
    "        df.JAPANESE.loc[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>MAGAZINE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>TOKENS</th>\n",
       "      <th>RACEWORDS</th>\n",
       "      <th>JAPANESE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19276</th>\n",
       "      <td>19276.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-018_聴診器の響_口語_b</td>\n",
       "      <td>1096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19277</th>\n",
       "      <td>19277.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-019_戦時の伯剌西爾と日本_口語_b</td>\n",
       "      <td>4804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4494.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>19278.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-020_新刊紹介_口語_b</td>\n",
       "      <td>369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>19279.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-021_強国と成る可き根本大策（工業教育の振興）_口語_b</td>\n",
       "      <td>10518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9998.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>19280.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-022_新刊紹介_口語_b</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>19281.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-023_羅馬法皇の講和提議_口語_b</td>\n",
       "      <td>1370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>19282.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-024_日英の経済的関係改善論_口語_b</td>\n",
       "      <td>4413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4066.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19283</th>\n",
       "      <td>19283.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-025_噂の立ちぎき_口語_b</td>\n",
       "      <td>678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>639.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19284</th>\n",
       "      <td>19284.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-026_新帝国技芸員評判記（下）_口語_b</td>\n",
       "      <td>1384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19285</th>\n",
       "      <td>19285.txt</td>\n",
       "      <td>太陽</td>\n",
       "      <td>1917</td>\n",
       "      <td>10-027_新刊紹介_口語_b</td>\n",
       "      <td>1270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FILE_ID MAGAZINE  YEAR                             TITLE  LENGTH  \\\n",
       "19276  19276.txt       太陽  1917                 10-018_聴診器の響_口語_b    1096   \n",
       "19277  19277.txt       太陽  1917            10-019_戦時の伯剌西爾と日本_口語_b    4804   \n",
       "19278  19278.txt       太陽  1917                  10-020_新刊紹介_口語_b     369   \n",
       "19279  19279.txt       太陽  1917  10-021_強国と成る可き根本大策（工業教育の振興）_口語_b   10518   \n",
       "19280  19280.txt       太陽  1917                  10-022_新刊紹介_口語_b     187   \n",
       "19281  19281.txt       太陽  1917             10-023_羅馬法皇の講和提議_口語_b    1370   \n",
       "19282  19282.txt       太陽  1917           10-024_日英の経済的関係改善論_口語_b    4413   \n",
       "19283  19283.txt       太陽  1917                10-025_噂の立ちぎき_口語_b     678   \n",
       "19284  19284.txt       太陽  1917          10-026_新帝国技芸員評判記（下）_口語_b    1384   \n",
       "19285  19285.txt       太陽  1917                  10-027_新刊紹介_口語_b    1270   \n",
       "\n",
       "      FILTER  TOKENS RACEWORDS JAPANESE  \n",
       "19276    NaN  1024.0         0        0  \n",
       "19277    NaN  4494.0         0        2  \n",
       "19278    NaN   337.0         0        0  \n",
       "19279    NaN  9998.0         0        3  \n",
       "19280    NaN   161.0         0        0  \n",
       "19281    NaN  1268.0         0        0  \n",
       "19282    NaN  4066.0         0        2  \n",
       "19283    NaN   639.0         0        0  \n",
       "19284    NaN  1251.0         0        0  \n",
       "19285    NaN  1156.0         0        2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\Temp.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_terms = keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'朝鮮 人': [147, 40], '朝鮮 民族': [4, 1], '朝鮮 の 人': [9, 7], '朝鮮 の 人々': [0, 0], '鮮人': [33, 10], '在日': [1, 1], '中国人': [221, 39], '中国 民族': [2, 2], '中国 民衆': [0, 0], '中国 の 民衆': [2, 2], '中国 の 人': [17, 7], '中国 の 人々': [5, 2], 'シナ 人': [4, 4], '支那 人': [612, 109], '支那 の 人': [8, 4], '支那 の 民族': [2, 2], '支那 の 民衆': [7, 3], '西洋 人': [342, 139], '西洋 の 人': [7, 6], '白人': [408, 53], 'ハイカラ': [288, 134], '毛唐': [257, 61], '外国 人': [394, 137], '外人': [283, 73], '欧州 人': [2, 2], '日本人': [1843, 310], '日本 の 人々': [5, 4], '日本 の 人': [50, 34], '大和 民族': [22, 14], '大和 （ やまと ） 民族': [0, 0], '日本 民族': [56, 21], '日本 の 民族': [2, 2], '日本 民衆': [1, 1], '日本 の 民衆': [2, 2], '日本 国民': [39, 26], '日本 の 国民': [5, 5], 'ジャップ': [14, 7], '和人': [11, 2], '内地 人': [31, 8], '日系': [98, 7], '穢 多': [152, 7], '非人': [261, 108], '新 平民': [52, 4], '部落 民': [13, 5], '平民': [289, 74], 'アイヌ': [162, 24], '偉人': [84, 50], '蝦夷': [222, 43], '気違い': [292, 119], '琉球 人': [8, 5], '沖縄 人': [2, 1], '沖縄 の 人': [0, 0], '四足': [52, 38], '黒人': [424, 56], '黒ん坊': [107, 31], 'アフリカ 人': [2, 1], '土人': [573, 78], '混血': [119, 46], 'アジア 人': [1, 1], '東洋 人': [151, 51], '東洋 の 人': [4, 4], '黄 人': [6, 2], '黄 人種': [0, 0], '有色 人種': [12, 4], '異 人種': [12, 12]}\n"
     ]
    }
   ],
   "source": [
    "all_terms = {**korea, **china, **west, **japan, **other, **black, **asia}  #merge all dictionaries\n",
    "print(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = DataFrame.from_dict(all_terms, orient='index')\n",
    "terms.columns = ['total_count', 'num_of_docs']\n",
    "\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\Race_Terms_Unified.xlsx', engine='xlsxwriter')\n",
    "terms.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\UnidicCorpus\\\\\"\n",
    "source_text = CORPUS_PATH + str(df.WORK_ID[0]) + \".txt\"\n",
    "raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "raw = raw_text.read()\n",
    "\n",
    "if re.search(r'怪談', raw):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Race Terms into Unified Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Merge the race terms into single terms in the corpus\n",
    "\n",
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\UnidicCorpusLemma\\\\\"\n",
    "\n",
    "unified_term = '朝鮮人'\n",
    "\n",
    "merge_terms = ['半島 人']\n",
    "\n",
    "#merge_terms = ['朝鮮 人', '朝鮮 民族', '朝鮮 の 人', '朝鮮 の 人 々', '鮮人']\n",
    "\n",
    "#merge_terms = ['中国 人','中国 民族','中国 民衆','中国 の 民衆','中国 の 人','中国 の 人々','シナ 人','支那 人','支那 の 人',\n",
    "#               '支那 の 民族','支那 の 民衆']\n",
    "\n",
    "#merge_terms = ['中国 人','中国 民族','中国 民衆','中国 の 民衆','中国 の 人','中国 の 人 々','支那 人','支那 の 人',\n",
    "#               '支那 の 民族','支那 の 民衆']\n",
    "\n",
    "#merge_terms = ['西洋 人','西洋 の 人','白人','毛唐','オウシュウ-外国 人']    #'欧州 人']\n",
    "#merge_terms = ['外国 人','外人','異人']\n",
    "\n",
    "#merge_terms = ['日本 人','日本 の 人々','日本 の 人','大和 民族','大和 （ やまと ） 民族','日本 民族','日本 の 民族','日本 民衆',\n",
    "#               '日本 の 民衆','日本 国民','日本 の 国民','ジャップ','和人','内地 人']\n",
    "\n",
    "#merge_terms = ['日本 人','日本 の 人 々','日本 の 人','ヤマト 民族','ヤマト （ ヤマト ） 民族','日本 民族','日本 の 民族','日本 民衆',\n",
    "#               '日本 の 民衆','日本 国民','日本 の 国民','ジャップ-Jap','和人','内 地人']\n",
    "\n",
    "#merge_terms = ['穢多','非人','新 平民','部落 民']\n",
    "#merge_terms = ['黒人','黒ん坊','アフリカ-Africa 人']       #'アフリカ 人']     \n",
    "#merge_terms = ['東洋 人','東洋 の 人','黄 人','黄 人種','有色 人種', 'アジア-Asia 人'] #'アジア 人']\n",
    "#merge_terms = ['蕃人','蛮族','蕃族','生蕃','熟蕃','タカサゴ 族','高砂 族','本島 人','蛮人','野蛮 人']  #,'土人']\n",
    "\n",
    "altered_docs = 0\n",
    "change = 0\n",
    "\n",
    "for k in df.index:\n",
    "    #get the text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    for word in merge_terms:\n",
    "        if re.search(word, raw):\n",
    "            change = 1\n",
    "            raw = re.sub(word, unified_term, raw)\n",
    "    \n",
    "    if change == 1:\n",
    "        altered_docs += 1\n",
    "    \n",
    "    #now print back out\n",
    "    with open(CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        \n",
    "    change = 0\n",
    "\n",
    "print(altered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condense key race terms into single word units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n"
     ]
    }
   ],
   "source": [
    "# condense key race terms into single word units\n",
    "\n",
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\UnidicCorpusLemmaNoMerge\\\\\"\n",
    "\n",
    "race_terms = [('朝鮮 人', '朝鮮人'), ('朝鮮 民族', '朝鮮民族'), ('朝鮮 の 人 々','朝鮮の人々'), ('朝鮮 の 人','朝鮮の人'),\n",
    "             ('中国 人', '中国人'), ('中国 民族','中国民族'),('中国 民衆', '中国民衆'),('中国 の 民衆','中国の民衆'),\n",
    "             ('中国 の 人 々','中国の人々'),('中国 の 人','中国の人'),('支那 人','支那人'),('支那 の 人','支那の人'),\n",
    "             ('支那 の 民族','支那の民族'), ('支那 の 民衆','支那の民衆'), ('西洋 人','西洋人'), ('西洋 の 人','西洋の人'),\n",
    "             ('オウシュウ-外国 人','欧州人'), ('外国 人','外国人'), ('日本 人','日本人'), ('日本 の 人 々','日本の人々'),\n",
    "             ('日本 の 人','日本の人'), ('ヤマト 民族','ヤマト民族'), ('日本 民族','日本民族'),('日本 の 民族','日本の民族'),\n",
    "             ('日本 民衆','日本民衆'), ('日本 の 民衆','日本の民衆'),('日本 国民','日本国民'),('日本 の 国民','日本の国民'),\n",
    "             ('内 地人','内地人'),('新 平民','新平民'),('部落 民','部落民'),('アフリカ-Africa 人', 'アフリカ-Africa人'),\n",
    "             ('東洋 人','東洋人'),('東洋 の 人','東洋の人'),('黄 人','黄人'),('黄 人種','黄人種'),('有色 人種','有色人種'),\n",
    "             ('アジア-Asia 人','アジア-Asia人'),('野蛮 人','野蛮人')]\n",
    "\n",
    "altered_docs = 0\n",
    "change = 0\n",
    "\n",
    "for k in df.index:\n",
    "    #get the text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    for pair in race_terms:\n",
    "        if re.search(pair[0], raw):\n",
    "            change = 1\n",
    "            raw = re.sub(pair[0], pair[1], raw)  #replace all instances of the race term with condensed version\n",
    "    \n",
    "    if change == 1:\n",
    "        altered_docs += 1\n",
    "    \n",
    "    #now print back out\n",
    "    with open(CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        \n",
    "    change = 0\n",
    "\n",
    "print(altered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ルイコウ',\n",
       " 'ツキジ',\n",
       " 'ハラ',\n",
       " 'オ',\n",
       " 'オコン',\n",
       " 'スイ',\n",
       " 'ルイコウ',\n",
       " 'ルイコウ',\n",
       " 'ルイコウ',\n",
       " 'カムロ',\n",
       " 'ツキジ',\n",
       " 'ハラ',\n",
       " 'ツキジ',\n",
       " 'ボウ',\n",
       " 'ツキジ',\n",
       " 'ボウ',\n",
       " 'ダイミョウ',\n",
       " 'ハカタ',\n",
       " 'トコロ',\n",
       " 'ヌ',\n",
       " 'ジョ',\n",
       " 'ジョ',\n",
       " 'オソレ',\n",
       " 'ヒトシ',\n",
       " 'ハカタ',\n",
       " 'トウキョウ',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'ヤ',\n",
       " 'コク',\n",
       " 'ア',\n",
       " 'オオトモ',\n",
       " 'コク',\n",
       " 'ケン',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'モク',\n",
       " 'オオトモ',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'コム',\n",
       " 'ミノル',\n",
       " 'オオトモ',\n",
       " 'ヒツ',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ヤ',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ヒビノ',\n",
       " 'ハラ',\n",
       " 'ヒビヤ',\n",
       " 'ハラ',\n",
       " 'ハラ',\n",
       " 'ハラ',\n",
       " 'ツキジ',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ネムリ',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ドバ',\n",
       " 'ドバ',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ヌ',\n",
       " 'オオトモ',\n",
       " 'ハイポセシス',\n",
       " 'コク',\n",
       " 'ヒ',\n",
       " 'ユウ',\n",
       " 'コク',\n",
       " 'ツキジ',\n",
       " 'ケン',\n",
       " 'ハカタ',\n",
       " 'コク',\n",
       " 'オオトモ',\n",
       " 'ブン',\n",
       " 'タクミ',\n",
       " 'ツキジ',\n",
       " 'リ',\n",
       " 'コク',\n",
       " 'エ',\n",
       " 'コク',\n",
       " 'オオトモ',\n",
       " 'エ',\n",
       " 'コク',\n",
       " 'オオトモ',\n",
       " 'スイ',\n",
       " 'コク',\n",
       " 'エー',\n",
       " 'エ',\n",
       " 'セン',\n",
       " 'チョン',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ー',\n",
       " 'コク',\n",
       " 'コク',\n",
       " 'ツキジ',\n",
       " 'ツキジ',\n",
       " 'チョウ',\n",
       " 'コク',\n",
       " 'オコン',\n",
       " 'オコン',\n",
       " 'オコン',\n",
       " 'ツキジ',\n",
       " 'エ',\n",
       " 'ケン',\n",
       " 'クレ',\n",
       " 'ケン',\n",
       " 'コク',\n",
       " 'キャ',\n",
       " 'コク',\n",
       " 'イヤサ',\n",
       " 'コク',\n",
       " 'リョウ',\n",
       " 'ボ',\n",
       " 'オギ',\n",
       " 'ヌ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'オモウ',\n",
       " 'オギ',\n",
       " 'ヤ',\n",
       " 'ア',\n",
       " 'ツイ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'イ',\n",
       " 'ペラ',\n",
       " 'ソリャ',\n",
       " 'エジェンシー',\n",
       " 'ツキジ',\n",
       " 'オオトモ',\n",
       " 'ケン',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'タカ',\n",
       " 'ショウ',\n",
       " 'ブックリ',\n",
       " 'コウ',\n",
       " 'コク',\n",
       " 'タカ',\n",
       " 'ショウ',\n",
       " 'イ',\n",
       " 'エ',\n",
       " 'エ',\n",
       " 'ケン',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'オコン',\n",
       " 'ツキジ',\n",
       " 'オコン',\n",
       " 'ツキジ',\n",
       " 'ツキジ',\n",
       " 'アサクサ',\n",
       " 'ウシゴメ',\n",
       " 'ウシゴメ',\n",
       " 'アサクサ',\n",
       " 'ウシゴメ',\n",
       " 'アサクサ',\n",
       " 'ア',\n",
       " 'アサクサ',\n",
       " 'ウシゴメ',\n",
       " 'ハセ',\n",
       " 'ハセ',\n",
       " 'オギ',\n",
       " 'オコン',\n",
       " 'コク',\n",
       " 'ユキエ',\n",
       " 'エ',\n",
       " 'ダイミョウ',\n",
       " 'オギ',\n",
       " 'アサクサ',\n",
       " 'ツキジ',\n",
       " 'コク',\n",
       " 'ツキジ',\n",
       " 'オギ',\n",
       " 'オコン',\n",
       " 'コク',\n",
       " 'アサクサ',\n",
       " 'ツキジ',\n",
       " 'タカ',\n",
       " 'オコン',\n",
       " 'オギ',\n",
       " 'イヨ',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'キコエ',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'オコン',\n",
       " 'ボ',\n",
       " 'オギ',\n",
       " 'モク',\n",
       " 'メイ',\n",
       " 'オオイ',\n",
       " 'オオトモ',\n",
       " 'ア',\n",
       " 'オオトモ',\n",
       " 'オコン',\n",
       " 'オオトモ',\n",
       " 'ミスジ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'ブン',\n",
       " 'オギ',\n",
       " 'コウ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'オオトモ',\n",
       " 'ト',\n",
       " 'トコロ',\n",
       " 'オギ',\n",
       " 'ク',\n",
       " 'オギ',\n",
       " 'イエ',\n",
       " 'オギ',\n",
       " 'オコン',\n",
       " 'オギ',\n",
       " 'セイ',\n",
       " 'オギ',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'イン',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'ミスジ',\n",
       " 'ムカイ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'カ',\n",
       " 'ギ',\n",
       " 'オオクマ',\n",
       " 'ショウ',\n",
       " 'エ',\n",
       " 'エ',\n",
       " 'サワ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'ユウ',\n",
       " 'アリ',\n",
       " 'クネ',\n",
       " 'エ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'ギョウ',\n",
       " 'ナオ',\n",
       " 'トコロ',\n",
       " 'ボ',\n",
       " 'オギ',\n",
       " 'ブン',\n",
       " 'リョウ',\n",
       " 'ム',\n",
       " 'ボウ',\n",
       " 'オウ',\n",
       " 'ボウ',\n",
       " 'オウ',\n",
       " 'トコロ',\n",
       " 'アオグ',\n",
       " 'スラ',\n",
       " 'ドシン',\n",
       " 'ヅシン',\n",
       " 'オウ',\n",
       " 'ボウ',\n",
       " 'オウ',\n",
       " 'オギ',\n",
       " 'ドウジ',\n",
       " 'オウ',\n",
       " 'オギ',\n",
       " 'サイ',\n",
       " 'オウ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'ハカタ',\n",
       " 'オギ',\n",
       " 'ケン',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'ユウ',\n",
       " 'オギ',\n",
       " 'イヤサ',\n",
       " 'オギ',\n",
       " 'ハセ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'ミステリイ',\n",
       " 'ミステリイ',\n",
       " 'ミステリイ',\n",
       " 'オギ',\n",
       " 'ブン',\n",
       " 'オギ',\n",
       " 'ミステリイ',\n",
       " 'オオトモ',\n",
       " 'トコロ',\n",
       " 'ユウ',\n",
       " 'モン',\n",
       " 'チ',\n",
       " 'モン',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'トウキョウ',\n",
       " 'ヨコハマ',\n",
       " 'トウキョウ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'オギ',\n",
       " 'スイ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'カケヤ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'イマ',\n",
       " 'ネイ',\n",
       " 'タカ',\n",
       " 'オギ',\n",
       " 'ネイ',\n",
       " 'コク',\n",
       " 'ウオ',\n",
       " 'オコン',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'オギ',\n",
       " 'オオトモ',\n",
       " 'コク',\n",
       " 'エ',\n",
       " 'オコン',\n",
       " 'オギ',\n",
       " 'オコン',\n",
       " 'ナガサキ',\n",
       " 'シャンハイ',\n",
       " 'シャンハイ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'ナガサキ',\n",
       " 'ナガサキ',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'ナガサキ',\n",
       " 'ヤスシ',\n",
       " 'ネイ',\n",
       " 'キン',\n",
       " 'ネイ',\n",
       " 'キン',\n",
       " 'ネイ',\n",
       " 'キン',\n",
       " 'ネイ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'キン',\n",
       " 'キン',\n",
       " 'キン',\n",
       " 'キン',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'コウベ',\n",
       " 'ヤスシ',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'コウベ',\n",
       " 'キン',\n",
       " 'トウキョウ',\n",
       " 'コウベ',\n",
       " 'トウキョウ',\n",
       " 'ツキジ',\n",
       " 'カネコ',\n",
       " 'キン',\n",
       " 'ツキジ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'ナガサキ',\n",
       " 'トウキョウ',\n",
       " 'ツキジ',\n",
       " 'ホンゴウ',\n",
       " 'ヨコハマ',\n",
       " 'キン',\n",
       " 'ヨコハマ',\n",
       " 'アサクサ',\n",
       " 'オコン',\n",
       " 'ネイ',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ネイ',\n",
       " 'ア',\n",
       " 'オコン',\n",
       " 'トウキョウ',\n",
       " 'トウキョウ',\n",
       " 'チイ',\n",
       " 'ア',\n",
       " 'ク',\n",
       " 'ア',\n",
       " 'ホンゴウ',\n",
       " 'ア',\n",
       " 'キン',\n",
       " 'ア',\n",
       " 'ヨコハマ',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ア',\n",
       " 'ヨコハマ',\n",
       " 'イエ',\n",
       " 'ハセ',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'ヨコハマ',\n",
       " 'エ',\n",
       " 'キン',\n",
       " 'ツキジ',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'ヨコハマ',\n",
       " 'ネイ',\n",
       " 'キ',\n",
       " 'キン',\n",
       " 'ヨコハマ',\n",
       " 'ヨコハマ',\n",
       " 'キン',\n",
       " 'ススム',\n",
       " 'ツキジ',\n",
       " 'ネイ',\n",
       " 'オコン',\n",
       " 'キン',\n",
       " 'チン',\n",
       " 'ヤスシ',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'キン',\n",
       " 'ヨコハマ',\n",
       " 'キン',\n",
       " 'ネイ',\n",
       " 'キン',\n",
       " 'アザブ',\n",
       " 'オギ',\n",
       " 'キン',\n",
       " 'オギ',\n",
       " 'キン',\n",
       " 'キン',\n",
       " 'キン',\n",
       " 'ナガサキ',\n",
       " 'ダイミョウ',\n",
       " 'ハカタ',\n",
       " 'オギ',\n",
       " 'オギ',\n",
       " 'ア',\n",
       " 'チン',\n",
       " 'ネイ',\n",
       " 'ネイ',\n",
       " 'ヨコハマ',\n",
       " 'シャンハイ',\n",
       " 'オオトモ',\n",
       " 'オコン',\n",
       " 'オオトモ',\n",
       " 'オギ',\n",
       " 'レ',\n",
       " 'ガモン',\n",
       " 'ウンケイ',\n",
       " 'ヒダ',\n",
       " 'サガ',\n",
       " 'アラシヤマ',\n",
       " 'サンジョウ',\n",
       " 'シミズ',\n",
       " 'キ',\n",
       " 'シマバラ',\n",
       " 'ト',\n",
       " 'ケツ',\n",
       " 'ソナエ',\n",
       " 'サエ',\n",
       " 'キ',\n",
       " 'シュウン',\n",
       " 'ナラ',\n",
       " 'ギ',\n",
       " 'ギ',\n",
       " 'チ',\n",
       " 'ナオ',\n",
       " 'シュウン',\n",
       " 'ヒン',\n",
       " 'カモガワ',\n",
       " 'イ',\n",
       " 'スイ',\n",
       " 'トウカイドウ',\n",
       " 'カマクラ',\n",
       " 'ナラ',\n",
       " 'ウスイ',\n",
       " 'アサマ',\n",
       " 'レツ',\n",
       " 'ワダ',\n",
       " 'キソ',\n",
       " 'スハラ',\n",
       " 'タイ',\n",
       " 'カン',\n",
       " 'スハラ',\n",
       " 'ドク',\n",
       " 'ヤマナシ',\n",
       " 'ヤマモト',\n",
       " 'シュウン',\n",
       " 'キソ',\n",
       " 'シナノ',\n",
       " 'テン',\n",
       " 'キ',\n",
       " 'ホウ',\n",
       " 'トウ',\n",
       " 'ラ',\n",
       " 'アラ',\n",
       " 'ヤサカ',\n",
       " 'オトワ',\n",
       " 'ウメオカ',\n",
       " 'トバ',\n",
       " 'スイ',\n",
       " 'ト',\n",
       " 'ギョク',\n",
       " 'フミイリ',\n",
       " 'アズマ',\n",
       " 'オウシュウ',\n",
       " 'ヨシツネ',\n",
       " 'オトコヤマ',\n",
       " 'ト',\n",
       " 'タ',\n",
       " 'チクシ',\n",
       " 'コロ',\n",
       " 'チャン',\n",
       " 'カケヤ',\n",
       " 'ヒロシゲ',\n",
       " 'カオル',\n",
       " 'イセ',\n",
       " 'シュウン',\n",
       " 'ウメヤマ',\n",
       " 'キソ',\n",
       " 'ウ',\n",
       " 'チ',\n",
       " 'スパリ',\n",
       " 'ムロ',\n",
       " 'キ',\n",
       " 'オタツ',\n",
       " 'サトコ',\n",
       " 'チ',\n",
       " 'オン',\n",
       " 'ヨリコ',\n",
       " 'キ',\n",
       " 'ミノ',\n",
       " 'シナノ',\n",
       " 'スハラ',\n",
       " 'オキチ',\n",
       " 'シ',\n",
       " 'ムラナカ',\n",
       " 'オタツ',\n",
       " 'カモ',\n",
       " 'ハルカ',\n",
       " 'シミズ',\n",
       " 'シチゾウ',\n",
       " 'シラキ',\n",
       " 'ムロ',\n",
       " 'キ',\n",
       " 'ギオン',\n",
       " 'シチゾウ',\n",
       " 'ボウ',\n",
       " 'カワバタ',\n",
       " 'ムロ',\n",
       " 'オキチ',\n",
       " 'キョウ',\n",
       " 'ミエ',\n",
       " 'トリベノ',\n",
       " 'オキチ',\n",
       " 'スハラ',\n",
       " 'クマゲ',\n",
       " 'マツモト',\n",
       " 'イイダ',\n",
       " 'ナオ',\n",
       " 'シュン',\n",
       " 'ラ',\n",
       " 'ナガクボ',\n",
       " 'スハラ',\n",
       " 'ト',\n",
       " 'シバキ',\n",
       " 'シチゾウ',\n",
       " 'ウエダ',\n",
       " 'シュウン',\n",
       " 'マゴメ',\n",
       " 'ナカツガワ',\n",
       " 'ケタカ',\n",
       " 'カンノン',\n",
       " 'サエ',\n",
       " 'テンシン',\n",
       " 'ソナエ',\n",
       " 'ヒトシ',\n",
       " 'シュウン',\n",
       " 'シチゾウ',\n",
       " 'ユキエ',\n",
       " 'スケノブ',\n",
       " 'キ',\n",
       " 'ト',\n",
       " 'ギ',\n",
       " 'リハク',\n",
       " 'トオル',\n",
       " 'ヘキ',\n",
       " 'シロトリ',\n",
       " 'アラキ',\n",
       " 'シュウン',\n",
       " 'タノモ',\n",
       " 'キ',\n",
       " 'シュウン',\n",
       " 'スハラ',\n",
       " 'キヨシ',\n",
       " 'マリ',\n",
       " 'シュウン',\n",
       " 'ドッコイ',\n",
       " 'サトシ',\n",
       " 'カメヤ',\n",
       " 'シュウ',\n",
       " 'シュウン',\n",
       " 'カメヤ',\n",
       " 'ライ',\n",
       " 'オタツ',\n",
       " 'アツシ',\n",
       " 'キ',\n",
       " 'ナカ',\n",
       " 'シュウン',\n",
       " 'ヒトシ',\n",
       " 'マゴメ',\n",
       " 'シュウン',\n",
       " 'リョウ',\n",
       " 'カメヤ',\n",
       " 'イマガワ',\n",
       " 'シュウン',\n",
       " 'ノウ',\n",
       " 'キチベエ',\n",
       " 'オタツ',\n",
       " 'コマ',\n",
       " 'シュウン',\n",
       " 'ト',\n",
       " 'オタツ',\n",
       " 'ハン',\n",
       " 'トウ',\n",
       " 'ウエダ',\n",
       " 'シュウン',\n",
       " 'ヒトシ',\n",
       " 'カメヤ',\n",
       " 'オキチ',\n",
       " 'ハ',\n",
       " 'シュウン',\n",
       " 'ギョウ',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'オタツ',\n",
       " 'オタツ',\n",
       " 'キチベエ',\n",
       " 'シチゾウ',\n",
       " 'シチゾウ',\n",
       " 'チュン',\n",
       " 'キチベエ',\n",
       " 'カメヤ',\n",
       " 'シュウン',\n",
       " 'ヤワタ',\n",
       " 'シュウン',\n",
       " 'カン',\n",
       " 'カメヤ',\n",
       " 'ト',\n",
       " 'シオカイ',\n",
       " 'シュウン',\n",
       " 'クレ',\n",
       " 'ジュウ',\n",
       " 'ナラ',\n",
       " 'サトシ',\n",
       " 'シチゾウ',\n",
       " 'フン',\n",
       " 'サトシ',\n",
       " 'オタツ',\n",
       " 'コレ',\n",
       " 'ナラ',\n",
       " 'ヨシナカ',\n",
       " 'コレ',\n",
       " 'ク',\n",
       " 'サトシ',\n",
       " 'ヨッカイチ',\n",
       " 'トウキョウ',\n",
       " 'キッショウ',\n",
       " 'シュウン',\n",
       " 'アリフク',\n",
       " 'セン',\n",
       " 'サイギョウ',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'ヤスシ',\n",
       " 'ドッコイ',\n",
       " 'ナラ',\n",
       " 'ユ',\n",
       " 'アンセイ',\n",
       " 'シュウン',\n",
       " 'キチベエ',\n",
       " 'ト',\n",
       " 'シュク',\n",
       " 'シュウン',\n",
       " 'キチベエ',\n",
       " 'ト',\n",
       " 'ホク',\n",
       " 'オタツ',\n",
       " 'キチベエ',\n",
       " 'シュウン',\n",
       " 'オタツ',\n",
       " 'アリ',\n",
       " 'カメヤ',\n",
       " 'イワヌマ',\n",
       " 'タハラ',\n",
       " 'シュウン',\n",
       " 'ムロ',\n",
       " 'コウ',\n",
       " 'カシマ',\n",
       " 'アサギリ',\n",
       " 'エド',\n",
       " 'オウシュウ',\n",
       " 'ト',\n",
       " 'ムロ',\n",
       " 'イワヌマ',\n",
       " 'ムロ',\n",
       " 'ト',\n",
       " 'ト',\n",
       " 'カン',\n",
       " 'ズ',\n",
       " 'キ',\n",
       " 'ナオ',\n",
       " 'キ',\n",
       " 'ハナゾノ',\n",
       " 'シナノ',\n",
       " 'シンシュウ',\n",
       " 'タハラ',\n",
       " 'シチゾウ',\n",
       " 'シュウン',\n",
       " 'カメヤ',\n",
       " 'タハラ',\n",
       " 'チ',\n",
       " 'カメヤ',\n",
       " 'タハラ',\n",
       " 'イワヌマ',\n",
       " 'トオル',\n",
       " 'エチゴヤ',\n",
       " 'シュウン',\n",
       " 'イワヌマ',\n",
       " 'キチベエ',\n",
       " 'カノウ',\n",
       " 'タハラ',\n",
       " 'ケン',\n",
       " 'シュウン',\n",
       " 'チ',\n",
       " 'ド',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'トウ',\n",
       " 'イワヌマ',\n",
       " 'カネコ',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'キョ',\n",
       " 'イワヌマ',\n",
       " 'セン',\n",
       " 'レン',\n",
       " 'イワヌマ',\n",
       " 'ジ',\n",
       " 'イワヌマ',\n",
       " 'スワ',\n",
       " 'シ',\n",
       " 'タハラ',\n",
       " 'オタツ',\n",
       " 'イワヌマ',\n",
       " 'オタツ',\n",
       " 'オタツ',\n",
       " 'トウキョウ',\n",
       " 'キ',\n",
       " 'タハラ',\n",
       " 'シュウン',\n",
       " 'コウコウ',\n",
       " 'ジョウチョウ',\n",
       " 'セン',\n",
       " 'ミチエルアンジロ',\n",
       " 'キソ',\n",
       " 'サカマキ',\n",
       " 'サトル',\n",
       " 'ヌ',\n",
       " 'フカミ',\n",
       " 'サトシ',\n",
       " 'シュウン',\n",
       " 'ナガイ',\n",
       " 'ナラ',\n",
       " 'ト',\n",
       " 'ミナミムカイ',\n",
       " 'キソ',\n",
       " 'カメヤ',\n",
       " 'テンポウ',\n",
       " 'テン',\n",
       " 'シュウン',\n",
       " 'サツマ',\n",
       " 'カン',\n",
       " 'ズ',\n",
       " 'ネイ',\n",
       " 'キチベエ',\n",
       " 'シュウン',\n",
       " 'スワ',\n",
       " 'カツヨリ',\n",
       " 'ワタル',\n",
       " 'オタツ',\n",
       " 'チュン',\n",
       " 'カメヤ',\n",
       " 'テン',\n",
       " 'キチベエ',\n",
       " 'ト',\n",
       " 'シュウン',\n",
       " 'ミトク',\n",
       " 'ユウ',\n",
       " 'キ',\n",
       " 'ハ',\n",
       " 'クロツチ',\n",
       " 'シュウン',\n",
       " 'クレ',\n",
       " 'ハセ',\n",
       " 'ア',\n",
       " 'カンノン',\n",
       " 'オウセ',\n",
       " 'シュウン',\n",
       " 'フカミ',\n",
       " 'レイ',\n",
       " 'キン',\n",
       " 'ク',\n",
       " 'ボク',\n",
       " 'ハルカ',\n",
       " 'ハナイ',\n",
       " 'シュウン',\n",
       " 'キュウ',\n",
       " 'ノボル',\n",
       " 'サエ',\n",
       " 'サエ',\n",
       " 'ラ',\n",
       " 'ハナイ',\n",
       " 'クウゲ',\n",
       " 'シン',\n",
       " 'シュウン',\n",
       " 'ジョ',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'キチベエ',\n",
       " 'シュウン',\n",
       " 'コウヤギ',\n",
       " 'タイ',\n",
       " 'ナラ',\n",
       " 'チラリト',\n",
       " 'ドッコイ',\n",
       " 'ナラ',\n",
       " 'サトシ',\n",
       " 'シュウン',\n",
       " 'テン',\n",
       " 'ケン',\n",
       " 'コウ',\n",
       " 'オタツ',\n",
       " 'バイカ',\n",
       " 'プン',\n",
       " 'サイギョウ',\n",
       " 'ギフ',\n",
       " 'リョウ',\n",
       " 'ショウ',\n",
       " 'オタツ',\n",
       " 'オタツ',\n",
       " 'オタツ',\n",
       " 'オタツ',\n",
       " 'キチベエ',\n",
       " 'イワヌマ',\n",
       " 'ナリヒラ',\n",
       " 'イワヌマ',\n",
       " 'ナリヒラ',\n",
       " 'ナリヒラ',\n",
       " 'イタル',\n",
       " 'クマノ',\n",
       " 'キ',\n",
       " 'ギ',\n",
       " 'シュウン',\n",
       " 'タハラ',\n",
       " 'キヨシ',\n",
       " 'ア',\n",
       " 'カメヤ',\n",
       " 'シュウン',\n",
       " 'ケン',\n",
       " 'ウメ',\n",
       " 'トオル',\n",
       " 'キ',\n",
       " 'モリ',\n",
       " 'テン',\n",
       " 'リョウ',\n",
       " 'アリ',\n",
       " 'キチベエ',\n",
       " 'シバ',\n",
       " 'キチベエ',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'モク',\n",
       " 'カメヤ',\n",
       " 'マゴメ',\n",
       " 'キチベエ',\n",
       " 'シュウン',\n",
       " 'ギ',\n",
       " 'ジョ',\n",
       " 'タニガワ',\n",
       " 'ナリヒラ',\n",
       " 'チ',\n",
       " 'アオグ',\n",
       " 'サカエ',\n",
       " 'ウチ',\n",
       " 'キチベエ',\n",
       " 'ユウ',\n",
       " 'ナリヒラ',\n",
       " 'セイ',\n",
       " 'キヨシ',\n",
       " 'セイ',\n",
       " 'タハラ',\n",
       " 'ウラン',\n",
       " 'シメギ',\n",
       " 'ライ',\n",
       " 'シミズ',\n",
       " 'チチャポン',\n",
       " 'シュウン',\n",
       " 'ハン',\n",
       " 'クレ',\n",
       " 'シュウン',\n",
       " 'シュウン',\n",
       " 'ボウ',\n",
       " 'サエ',\n",
       " 'ヒトシ',\n",
       " 'ルル',\n",
       " 'フカシ',\n",
       " 'シュウン',\n",
       " 'オモウ',\n",
       " 'ユウ',\n",
       " 'シュウン',\n",
       " 'モン',\n",
       " 'ゲン',\n",
       " 'ゲン',\n",
       " 'シュウン',\n",
       " 'キチベエ',\n",
       " 'イッソン',\n",
       " 'シチゾウ',\n",
       " 'タハラ',\n",
       " 'セン',\n",
       " 'アワ',\n",
       " 'ホッカイドウ',\n",
       " 'グ',\n",
       " 'ナリヒラ',\n",
       " 'シュウン',\n",
       " 'モリモト',\n",
       " 'シュウン',\n",
       " 'モルモン',\n",
       " 'ウ',\n",
       " 'シミズ',\n",
       " 'オユキ',\n",
       " 'オユキ',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\UnidicCorpusLemma\\\\\"\n",
    "\n",
    "all_kana = []\n",
    "\n",
    "for k in df.index[0:5]:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets (need to do this first)\n",
    "\n",
    "    kana = re.findall(r'\\s([ア-ンーァィゥェォ]+)\\s', text)\n",
    "    \n",
    "    all_kana += kana\n",
    "\n",
    "\n",
    "#produce the frequency list\n",
    "#fdist = nltk.FreqDist(all_kana)\n",
    "#freq_pairs = fdist.items()\n",
    "#sort_freq_pairs = sorted(freq_pairs, key=lambda x:x[1], reverse=True)  #sort by decreasing frequency\n",
    "\n",
    "#create a dictionary to store word-frequency pairs\n",
    "#word_freqs = {}\n",
    "\n",
    "#fill dictionary with pairs\n",
    "#for item in sort_freq_pairs:\n",
    "#    word_freqs[item[0]] = item[1]\n",
    "    \n",
    "#kana_df = DataFrame.from_dict(word_freqs, orient='index')  #convert dict to dataframe\n",
    "#kana_df = kana_df.rename(columns={0:'frequency'})    #rename column\n",
    "#kana_df = kana_df.sort_values(by='frequency', ascending=False)   #sort by frequency\n",
    "\n",
    "#import xlsxwriter\n",
    "#import openpyxl\n",
    "#writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\CharNames.xlsx', engine='xlsxwriter')\n",
    "#kana_df.to_excel(writer, sheet_name='Sheet1')\n",
    "#writer.save()\n",
    "\n",
    "all_kana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Frequency Table for All Words in Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if you're working on unidic tokenized corpus\n",
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\UnidicCorpusLemma\\\\\"\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "#set this flag before you run\n",
    "rmv_stopwords = True\n",
    "\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\stopwords.txt')\n",
    "\n",
    "for k in df.index:\n",
    "    #get the tokenized text\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets (need to do this first)\n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #split the text into a list of individual tokens\n",
    "    tokens = re.split(r' ', text)\n",
    "    #while '' in tokens: tokens.remove('')  #remove blank spaces\n",
    "    \n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "\n",
    "    #add to global list\n",
    "    all_tokens += tokens \n",
    "\n",
    "#produce the frequency list\n",
    "fdist = nltk.FreqDist(all_tokens)\n",
    "freq_pairs = fdist.items()\n",
    "sort_freq_pairs = sorted(freq_pairs, key=lambda x:x[1], reverse=True)  #sort by decreasing frequency\n",
    "\n",
    "# store relative frequencies of words\n",
    "# need to subtract the number of '' tokens from total, since we will eliminate these\n",
    "# IF YOU RUN CODE AGAIN, WON'T NEED TO DO THIS -- I FIXED THE CLEANER FUNCTION\n",
    "#token_count = len(all_tokens) - sort_freq_pairs[0][1]   #assuming that first element is '' token\n",
    "\n",
    "token_count = len(all_tokens)\n",
    "\n",
    "#create a dictionary to store word-frequency pairs\n",
    "word_freqs = {}\n",
    "\n",
    "#fill dictionary with pairs\n",
    "for item in sort_freq_pairs:\n",
    "    word_freqs[item[0]] = item[1]\n",
    "    \n",
    "freqs_df = DataFrame.from_dict(word_freqs, orient='index')  #convert dict to dataframe\n",
    "freqs_df = freqs_df.rename(columns={0:'frequency'})    #rename column\n",
    "freqs_df = freqs_df.sort_values(by='frequency', ascending=False)   #sort by frequency\n",
    "\n",
    "#now drop the first element in dictionary: the '' token\n",
    "#freqs_df = freqs_df.drop(freqs_df.index[0])\n",
    "\n",
    "#compute relative frequencies\n",
    "freqs_df['rel_freq'] = freqs_df.frequency / token_count\n",
    "freqs_df = freqs_df.reset_index()\n",
    "\n",
    "# print out the results to an excel file\n",
    "#import xlsxwriter\n",
    "#import openpyxl\n",
    "#writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\LemmaFreqTable.xlsx', engine='xlsxwriter')\n",
    "#freqs_df.to_excel(writer, sheet_name='Sheet1')\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19265996\n",
      "(114021, 3)\n"
     ]
    }
   ],
   "source": [
    "print(token_count)\n",
    "print(freqs_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce a concordance for all the Race Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6570\n"
     ]
    }
   ],
   "source": [
    "# Create concordances for our seed terms\n",
    "\n",
    "#if you're working on unidic tokenized corpus\n",
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\UnidicCorpusLemma\\\\\"\n",
    "\n",
    "seed_terms = ['朝鮮人','中国人','西洋人','日本人','黒人','部落民','土人','東洋人','外国人']\n",
    "\n",
    "window_size = 15\n",
    "rmv_stopwords = True\n",
    "\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\stopwords.txt')\n",
    "\n",
    "#prepare file to print out results\n",
    "#f = open(r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\LemmaConcordance.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "#set up data structure to hold info\n",
    "metadata = {'text_id': [], 'seed_term': [], 'index_pos': [], 'left_tokens': [], 'right_tokens': []}\n",
    "\n",
    "hits = 0\n",
    "\n",
    "#iterate through all texts and count keywords; also keep track of number of texts in which keyword appears\n",
    "for k in df.index:\n",
    "    current_position = 0\n",
    "    \n",
    "    #get the text\n",
    "    text_id = str(df.WORK_ID[k]) + \".txt\"\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    text = bracket_cleaner(text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #tokenize the text\n",
    "    tokens = re.split(r' ', text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "    \n",
    "    #iterate through tokens in search of seed_terms\n",
    "    for token in tokens:\n",
    "        #check for seed term\n",
    "        if token in seed_terms:\n",
    "            hits += 1\n",
    "            #f.write(text_id + ' ' + token + ' ' + str(current_position) + '\\t')\n",
    "            metadata['text_id'].append(text_id)\n",
    "            metadata['seed_term'].append(token)\n",
    "            metadata['index_pos'].append(current_position)\n",
    "            #check to make sure we are not at start of text\n",
    "            if current_position - window_size > 0:\n",
    "                #f.write(' '.join(tokens[current_position-window_size:current_position]) + ' ')\n",
    "                metadata['left_tokens'].append(' '.join(tokens[current_position-window_size:current_position]))\n",
    "            else:\n",
    "                #f.write(' '.join(tokens[0:current_position]) + ' ')\n",
    "                metadata['left_tokens'].append(' '.join(tokens[0:current_position]))\n",
    "            \n",
    "            #f.write('SEED_TERM' + ' ')        \n",
    "            #check to make sure we are not at end of text\n",
    "            if current_position + window_size < len(tokens):\n",
    "                #f.write(' '.join(tokens[current_position+1:current_position+window_size]) + '\\n')\n",
    "                metadata['right_tokens'].append(' '.join(tokens[current_position+1:current_position+window_size]))\n",
    "            else:\n",
    "                #f.write(' '.join(tokens[current_position+1:]) + '\\n')\n",
    "                metadata['right_tokens'].append(' '.join(tokens[current_position+1:]))\n",
    "            \n",
    "        current_position += 1\n",
    "\n",
    "#f.close()\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6570, 5)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concord_df = DataFrame(metadata, columns=['text_id', 'seed_term', 'index_pos', 'left_tokens', 'right_tokens'])\n",
    "concord_df.shape   #this should be about 6594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>seed_term</th>\n",
       "      <th>index_pos</th>\n",
       "      <th>left_tokens</th>\n",
       "      <th>right_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000001.txt</td>\n",
       "      <td>中国人</td>\n",
       "      <td>10655</td>\n",
       "      <td>思う 行く 見る ふふん 嘲笑 婦 私-代名詞 顔 見る 言う まあ 良い 種 奴 買う</td>\n",
       "      <td>出る 言う 私-代名詞 固より 良い 気持ち 理由 承知 案外 平気 ナガタ ふふん 橇 うー</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000001.txt</td>\n",
       "      <td>中国人</td>\n",
       "      <td>10725</td>\n",
       "      <td>気 うう 負け けち 味 言う 論 うー 言う 冷笑 私-代名詞 却って ナガタ 宥める 良い</td>\n",
       "      <td>癩病 違う 君-代名詞 清浄 素性 分かる まあ 構え 苦笑 間切る 見る 見る 振り 一寸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000015.txt</td>\n",
       "      <td>中国人</td>\n",
       "      <td>46</td>\n",
       "      <td>来る 車 屋 迷惑 そう-様態 言う セン ギン 青い 紙幣 広げる 私-代名詞 掌 戻す 門前</td>\n",
       "      <td>小売り 店 明日 差し入れる 為 白い 塵紙 帖 カ ウ 小さな 銀貨 枚 戾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000015.txt</td>\n",
       "      <td>中国人</td>\n",
       "      <td>3246</td>\n",
       "      <td>見える 担架 死亡 室 行く 広い 庭 回る 私-代名詞 寝台 粗い 格子 担架 担ぐ 行く</td>\n",
       "      <td>弁 髮 尻 辺り ぴんぴん 歩く 度 跳ね返す 見る 中国人 踏む 行く 庭 地面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000015.txt</td>\n",
       "      <td>中国人</td>\n",
       "      <td>3256</td>\n",
       "      <td>粗い 格子 担架 担ぐ 行く 中国人 弁 髮 尻 辺り ぴんぴん 歩く 度 跳ね返す 見る</td>\n",
       "      <td>踏む 行く 庭 地面 石 拉ぐ 蒲公英 金色 裂く 月 半ば 室 目 転ずる</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id seed_term  index_pos  \\\n",
       "0  10000001.txt       中国人      10655   \n",
       "1  10000001.txt       中国人      10725   \n",
       "2  10000015.txt       中国人         46   \n",
       "3  10000015.txt       中国人       3246   \n",
       "4  10000015.txt       中国人       3256   \n",
       "\n",
       "                                        left_tokens  \\\n",
       "0      思う 行く 見る ふふん 嘲笑 婦 私-代名詞 顔 見る 言う まあ 良い 種 奴 買う   \n",
       "1   気 うう 負け けち 味 言う 論 うー 言う 冷笑 私-代名詞 却って ナガタ 宥める 良い   \n",
       "2  来る 車 屋 迷惑 そう-様態 言う セン ギン 青い 紙幣 広げる 私-代名詞 掌 戻す 門前   \n",
       "3    見える 担架 死亡 室 行く 広い 庭 回る 私-代名詞 寝台 粗い 格子 担架 担ぐ 行く   \n",
       "4     粗い 格子 担架 担ぐ 行く 中国人 弁 髮 尻 辺り ぴんぴん 歩く 度 跳ね返す 見る   \n",
       "\n",
       "                                      right_tokens  \n",
       "0  出る 言う 私-代名詞 固より 良い 気持ち 理由 承知 案外 平気 ナガタ ふふん 橇 うー  \n",
       "1   癩病 違う 君-代名詞 清浄 素性 分かる まあ 構え 苦笑 間切る 見る 見る 振り 一寸  \n",
       "2          小売り 店 明日 差し入れる 為 白い 塵紙 帖 カ ウ 小さな 銀貨 枚 戾  \n",
       "3        弁 髮 尻 辺り ぴんぴん 歩く 度 跳ね返す 見る 中国人 踏む 行く 庭 地面  \n",
       "4           踏む 行く 庭 地面 石 拉ぐ 蒲公英 金色 裂く 月 半ば 室 目 転ずる  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concord_df = concord_df.sort_values(by = [\"seed_term\", \"text_id\", \"index_pos\"])\n",
    "concord_df = concord_df.reset_index(drop=True)\n",
    "concord_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out the results to an excel file\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\LemmaConcordance.xlsx', engine='xlsxwriter')\n",
    "concord_df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6979, 5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the concordance data\n",
    "concord_df = pd.read_excel(r'./LemmaConcordance.xlsx', sheet_name='Sheet1')\n",
    "concord_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_table = freqs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Collocate Frequencies for Each Seed Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frequency</th>\n",
       "      <th>colloc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>言う</td>\n",
       "      <td>380</td>\n",
       "      <td>6.574623e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>土人</td>\n",
       "      <td>349</td>\n",
       "      <td>6.038272e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>来る</td>\n",
       "      <td>236</td>\n",
       "      <td>4.083187e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>人</td>\n",
       "      <td>235</td>\n",
       "      <td>4.065885e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彼</td>\n",
       "      <td>218</td>\n",
       "      <td>3.771758e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>私-代名詞</td>\n",
       "      <td>184</td>\n",
       "      <td>3.183502e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>行く</td>\n",
       "      <td>171</td>\n",
       "      <td>2.958580e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>見る</td>\n",
       "      <td>156</td>\n",
       "      <td>2.699056e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>―</td>\n",
       "      <td>138</td>\n",
       "      <td>2.387626e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>等</td>\n",
       "      <td>137</td>\n",
       "      <td>2.370325e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  frequency   colloc_freq\n",
       "0     言う        380  6.574623e-07\n",
       "1     土人        349  6.038272e-07\n",
       "2     来る        236  4.083187e-07\n",
       "3      人        235  4.065885e-07\n",
       "4      彼        218  3.771758e-07\n",
       "5  私-代名詞        184  3.183502e-07\n",
       "6     行く        171  2.958580e-07\n",
       "7     見る        156  2.699056e-07\n",
       "8      ―        138  2.387626e-07\n",
       "9      等        137  2.370325e-07"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next step is to grab all left and right tokens in the data frame, separated out by seed_term\n",
    "# Then produce counts of these context words\n",
    "\n",
    "keyword = \"土人\"\n",
    "\n",
    "#filter by race keyword\n",
    "seed_df = concord_df[concord_df['seed_term'] == keyword]\n",
    "seed_df = seed_df.reset_index(drop=True)\n",
    "\n",
    "#filter by multiple race words\n",
    "#seed_df = concord_df[(concord_df['seed_term'] == '中国人') | (concord_df['seed_term'] == '朝鮮人')]\n",
    "#seed_df = seed_df.reset_index(drop=True)\n",
    "\n",
    "collocates = []\n",
    "\n",
    "for k in seed_df.index:\n",
    "    context_words = ''\n",
    "    if str(seed_df.left_tokens[k]) != 'nan':\n",
    "        context_words += seed_df.left_tokens[k]\n",
    "    context_words += seed_df.right_tokens[k]\n",
    "    context_words = re.sub(r'\\s+', ' ', context_words)\n",
    "    \n",
    "    collocates += re.split(r'\\s', context_words)\n",
    "    \n",
    "#produce the frequency list\n",
    "fdist = nltk.FreqDist(collocates)\n",
    "freq_pairs = fdist.items()\n",
    "sort_freq_pairs = sorted(freq_pairs, key=lambda x:x[1], reverse=True)  #sort by decreasing frequency\n",
    "\n",
    "#create a dictionary to store word-frequency pairs\n",
    "word_freqs = {}\n",
    "\n",
    "#fill dictionary with pairs\n",
    "for item in sort_freq_pairs:\n",
    "    word_freqs[item[0]] = item[1]\n",
    "    \n",
    "freqs_df = DataFrame.from_dict(word_freqs, orient='index')  #convert dict to dataframe\n",
    "freqs_df = freqs_df.rename(columns={0:'frequency'})    #rename column\n",
    "freqs_df = freqs_df.sort_values(by='frequency', ascending=False)   #sort by frequency\n",
    "\n",
    "#rough estimation of the total number of co-occurrences in co-occurency matrix given window size\n",
    "total_collocates = token_count * (2 * window_size)   #total words * total window size\n",
    "\n",
    "#compute relative frequencies\n",
    "freqs_df['colloc_freq'] = freqs_df.frequency / total_collocates\n",
    "freqs_df = freqs_df.reset_index()\n",
    "\n",
    "freqs_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load your frequency table\n",
    "freq_table = pd.read_excel(r'./LemmaFreqTable.xlsx', sheet_name='Sheet1')\n",
    "freq_table[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the PMI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a new df to store results\n",
    "new_df = freqs_df\n",
    "\n",
    "#calclate the PMI scores for the selected seed term\n",
    "def calc(row, keyword, alpha):\n",
    "    a = .75  #set the alpha value\n",
    "    term = row.values[0]\n",
    "    w_c_prob = row.colloc_freq   #grab the probability of word given context; already stored in data frame\n",
    "    if not freq_table[freq_table['index'] == term].empty:\n",
    "        v1 = freq_table[freq_table['index'] == term].rel_freq.values[0]\n",
    "    else:\n",
    "        return 0\n",
    "    if alpha == True:  #apply alpha parameter to give less value to rare words\n",
    "        #take the raw count to the power of alpha\n",
    "        temp1 = math.pow(freq_table[freq_table['index'] == keyword].frequency.values[0], a)\n",
    "        #take the total word count to the power of alpha\n",
    "        temp2 = token_count   #total number of tokens (calculated in advance)\n",
    "        #divide the values\n",
    "        v2 = temp1 / temp2\n",
    "    else:\n",
    "        v2 = freq_table[freq_table['index'] == keyword].rel_freq.values[0]\n",
    "    \n",
    "    return math.log2(w_c_prob / (v1 * v2)) #do the PMI calculation\n",
    "    \n",
    "new_df['score'] = new_df.apply(lambda x: calc(x, keyword, alpha=True), axis=1)\n",
    "\n",
    "#now sort by score\n",
    "new_df = new_df.sort_values(by = 'score', ascending=False)\n",
    "new_df = new_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frequency</th>\n",
       "      <th>colloc_freq</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>サヴァイイ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>皮籠</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>怒り罵る</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>ロブ-lob</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>伏波</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>アハアハアハ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>カトレット</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>博大</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>タイサク</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>戯謔</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>矢筒</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>眷顧</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>不合</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>博言</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>ウヰルソン</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>維摩</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>影向</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>闘剣</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>凱陣</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>羊腸</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>ムリヌウ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>コウセイ</td>\n",
       "      <td>7</td>\n",
       "      <td>1.211115e-08</td>\n",
       "      <td>9.548086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>速成</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>ジンヨウ-外国</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>日本人間</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>リセ-lycee</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>銀坑</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>悪習</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>空砲</td>\n",
       "      <td>3</td>\n",
       "      <td>5.190492e-09</td>\n",
       "      <td>9.486159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>日英</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.465097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>荼奴</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.465097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>リマ-Lima</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.465097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>他見</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>榴</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>ハジン</td>\n",
       "      <td>3</td>\n",
       "      <td>5.190492e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>北狄</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>生え伸びる</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>思入れ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>赤木</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>迴</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>ベレスフォード</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>バロメーター-barometer</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>州民</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>旋条</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>両刃</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>修補</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>断り状</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>馬楝</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>コメカミ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>三匝</td>\n",
       "      <td>1</td>\n",
       "      <td>1.730164e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index  frequency   colloc_freq     score\n",
       "200             サヴァイイ          2  3.460328e-09  9.666731\n",
       "201                皮籠          2  3.460328e-09  9.666731\n",
       "202              怒り罵る          1  1.730164e-09  9.666731\n",
       "203            ロブ-lob          1  1.730164e-09  9.666731\n",
       "204                伏波          1  1.730164e-09  9.666731\n",
       "205            アハアハアハ          1  1.730164e-09  9.666731\n",
       "206             カトレット          1  1.730164e-09  9.666731\n",
       "207                博大          1  1.730164e-09  9.666731\n",
       "208              タイサク          1  1.730164e-09  9.666731\n",
       "209                戯謔          1  1.730164e-09  9.666731\n",
       "210                矢筒          1  1.730164e-09  9.666731\n",
       "211                眷顧          1  1.730164e-09  9.666731\n",
       "212                不合          1  1.730164e-09  9.666731\n",
       "213                博言          1  1.730164e-09  9.666731\n",
       "214             ウヰルソン          1  1.730164e-09  9.666731\n",
       "215                維摩          1  1.730164e-09  9.666731\n",
       "216                影向          1  1.730164e-09  9.666731\n",
       "217                闘剣          2  3.460328e-09  9.666731\n",
       "218                凱陣          1  1.730164e-09  9.666731\n",
       "219                羊腸          2  3.460328e-09  9.666731\n",
       "220              ムリヌウ          2  3.460328e-09  9.666731\n",
       "221              コウセイ          7  1.211115e-08  9.548086\n",
       "222                速成          2  3.460328e-09  9.529227\n",
       "223           ジンヨウ-外国          2  3.460328e-09  9.529227\n",
       "224              日本人間          2  3.460328e-09  9.529227\n",
       "225          リセ-lycee          2  3.460328e-09  9.529227\n",
       "226                銀坑          2  3.460328e-09  9.529227\n",
       "227                悪習          2  3.460328e-09  9.529227\n",
       "228                空砲          3  5.190492e-09  9.486159\n",
       "229                日英          4  6.920656e-09  9.465097\n",
       "230                荼奴          4  6.920656e-09  9.465097\n",
       "231           リマ-Lima          4  6.920656e-09  9.465097\n",
       "232                他見          1  1.730164e-09  9.403696\n",
       "233                 榴          1  1.730164e-09  9.403696\n",
       "234               ハジン          3  5.190492e-09  9.403696\n",
       "235                北狄          1  1.730164e-09  9.403696\n",
       "236             生え伸びる          1  1.730164e-09  9.403696\n",
       "237               思入れ          1  1.730164e-09  9.403696\n",
       "238                赤木          1  1.730164e-09  9.403696\n",
       "239                 迴          1  1.730164e-09  9.403696\n",
       "240           ベレスフォード          1  1.730164e-09  9.403696\n",
       "241  バロメーター-barometer          1  1.730164e-09  9.403696\n",
       "242                州民          1  1.730164e-09  9.403696\n",
       "243                旋条          2  3.460328e-09  9.403696\n",
       "244                両刃          2  3.460328e-09  9.403696\n",
       "245                修補          1  1.730164e-09  9.403696\n",
       "246               断り状          1  1.730164e-09  9.403696\n",
       "247                馬楝          1  1.730164e-09  9.403696\n",
       "248              コメカミ          1  1.730164e-09  9.403696\n",
       "249                三匝          1  1.730164e-09  9.403696"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[200:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list = [china_colloc, korea_colloc, black_colloc, buraku_colloc, western_colloc, native_colloc]\n",
    "title_list = ['china','korea','black','buraku','western','native']\n",
    "\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\PMIResults\\\\' + title_list[i] + '.xlsx', \n",
    "                            engine='xlsxwriter')\n",
    "    df_list[i].to_excel(writer, sheet_name='Sheet1')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frequency</th>\n",
       "      <th>colloc_freq</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>カツサン</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>10.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>ショウドシマ</td>\n",
       "      <td>8</td>\n",
       "      <td>1.384131e-08</td>\n",
       "      <td>10.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>又の名</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>10.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>鼻音</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>10.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>ジョン-John</td>\n",
       "      <td>60</td>\n",
       "      <td>1.038098e-07</td>\n",
       "      <td>10.202062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ヴァイリマ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>10.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>キシガミ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>10.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>トシヒロ</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>10.081768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>流謫</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.988659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>シル-sill</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.988659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>藁靴</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.818734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>マーシャル</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.818734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>パータ-外国</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.818734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>ポインター-pointer</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.818734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>フカバヤシ</td>\n",
       "      <td>6</td>\n",
       "      <td>1.038098e-08</td>\n",
       "      <td>9.715640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>サヴァイイ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>皮籠</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>闘剣</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>羊腸</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>ムリヌウ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.666731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>コウセイ</td>\n",
       "      <td>7</td>\n",
       "      <td>1.211115e-08</td>\n",
       "      <td>9.548086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>速成</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>ジンヨウ-外国</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>日本人間</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>リセ-lycee</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>銀坑</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>悪習</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.529227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>空砲</td>\n",
       "      <td>3</td>\n",
       "      <td>5.190492e-09</td>\n",
       "      <td>9.486159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>日英</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.465097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>荼奴</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.465097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>リマ-Lima</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.465097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>ハジン</td>\n",
       "      <td>3</td>\n",
       "      <td>5.190492e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>旋条</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>両刃</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.403696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>オン-on</td>\n",
       "      <td>18</td>\n",
       "      <td>3.114295e-08</td>\n",
       "      <td>9.364168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>泥濘む</td>\n",
       "      <td>3</td>\n",
       "      <td>5.190492e-09</td>\n",
       "      <td>9.325694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>ラシイヌ</td>\n",
       "      <td>27</td>\n",
       "      <td>4.671443e-08</td>\n",
       "      <td>9.300603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>牝鹿</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.288219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>パタゴニア-Patagonia</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.288219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>アイノ-aino</td>\n",
       "      <td>6</td>\n",
       "      <td>1.038098e-08</td>\n",
       "      <td>9.288219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>涌</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.288219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>漁猟</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.288219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>鯛焼き</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.288219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>供述</td>\n",
       "      <td>4</td>\n",
       "      <td>6.920656e-09</td>\n",
       "      <td>9.233771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>ツシタラ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>捷径</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>クラーレ-curare</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>実写</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>チョン</td>\n",
       "      <td>17</td>\n",
       "      <td>2.941279e-08</td>\n",
       "      <td>9.181304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>ウサ</td>\n",
       "      <td>2</td>\n",
       "      <td>3.460328e-09</td>\n",
       "      <td>9.181304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  frequency   colloc_freq      score\n",
       "146             カツサン          2  3.460328e-09  10.403696\n",
       "153           ショウドシマ          8  1.384131e-08  10.403696\n",
       "154              又の名          2  3.460328e-09  10.403696\n",
       "164               鼻音          2  3.460328e-09  10.403696\n",
       "172         ジョン-John         60  1.038098e-07  10.202062\n",
       "173            ヴァイリマ          2  3.460328e-09  10.181304\n",
       "174             キシガミ          2  3.460328e-09  10.181304\n",
       "175             トシヒロ          4  6.920656e-09  10.081768\n",
       "179               流謫          2  3.460328e-09   9.988659\n",
       "180          シル-sill          2  3.460328e-09   9.988659\n",
       "195               藁靴          2  3.460328e-09   9.818734\n",
       "196            マーシャル          2  3.460328e-09   9.818734\n",
       "197           パータ-外国          2  3.460328e-09   9.818734\n",
       "198    ポインター-pointer          2  3.460328e-09   9.818734\n",
       "199            フカバヤシ          6  1.038098e-08   9.715640\n",
       "200            サヴァイイ          2  3.460328e-09   9.666731\n",
       "201               皮籠          2  3.460328e-09   9.666731\n",
       "217               闘剣          2  3.460328e-09   9.666731\n",
       "219               羊腸          2  3.460328e-09   9.666731\n",
       "220             ムリヌウ          2  3.460328e-09   9.666731\n",
       "221             コウセイ          7  1.211115e-08   9.548086\n",
       "222               速成          2  3.460328e-09   9.529227\n",
       "223          ジンヨウ-外国          2  3.460328e-09   9.529227\n",
       "224             日本人間          2  3.460328e-09   9.529227\n",
       "225         リセ-lycee          2  3.460328e-09   9.529227\n",
       "226               銀坑          2  3.460328e-09   9.529227\n",
       "227               悪習          2  3.460328e-09   9.529227\n",
       "228               空砲          3  5.190492e-09   9.486159\n",
       "229               日英          4  6.920656e-09   9.465097\n",
       "230               荼奴          4  6.920656e-09   9.465097\n",
       "231          リマ-Lima          4  6.920656e-09   9.465097\n",
       "234              ハジン          3  5.190492e-09   9.403696\n",
       "243               旋条          2  3.460328e-09   9.403696\n",
       "244               両刃          2  3.460328e-09   9.403696\n",
       "250            オン-on         18  3.114295e-08   9.364168\n",
       "251              泥濘む          3  5.190492e-09   9.325694\n",
       "252             ラシイヌ         27  4.671443e-08   9.300603\n",
       "253               牝鹿          2  3.460328e-09   9.288219\n",
       "254  パタゴニア-Patagonia          2  3.460328e-09   9.288219\n",
       "255         アイノ-aino          6  1.038098e-08   9.288219\n",
       "256                涌          2  3.460328e-09   9.288219\n",
       "257               漁猟          2  3.460328e-09   9.288219\n",
       "258              鯛焼き          4  6.920656e-09   9.288219\n",
       "259               供述          4  6.920656e-09   9.233771\n",
       "265             ツシタラ          2  3.460328e-09   9.181304\n",
       "273               捷径          2  3.460328e-09   9.181304\n",
       "274      クラーレ-curare          2  3.460328e-09   9.181304\n",
       "277               実写          2  3.460328e-09   9.181304\n",
       "278              チョン         17  2.941279e-08   9.181304\n",
       "279               ウサ          2  3.460328e-09   9.181304"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test = native_colloc[native_colloc['frequency'] > 1]\n",
    "test[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0578920130638\n",
      "5.25996260335e-06\n",
      "2.64634146341e-07\n",
      "-0.20248954363594757\n"
     ]
    }
   ],
   "source": [
    "#freq_table = pd.read_excel(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\FreqTable.xlsx', sheet_name='Sheet1')\n",
    "v1 = freq_table[freq_table['index'] == freqs_df.ix[0][0]].rel_freq.values[0]\n",
    "v2 = freq_table[freq_table['index'] == keyword].rel_freq.values[0]\n",
    "v3 = freqs_df.ix[0].colloc_freq\n",
    "\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v3)\n",
    "print(math.log2(v3 / (v1 * v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_text = CORPUS_PATH + str(df.WORK_ID[2]) + \".txt\"\n",
    "raw_text = codecs.open(source_text, encoding=\"utf-8\", errors=\"ignore\")       #grab text\n",
    "raw = raw_text.read()            \n",
    "    \n",
    "#do some preprocessing\n",
    "raw = strip_chap_titles(raw)  #get rid of chapter titles and newline breaks; make dialogue markers consistent\n",
    "raw = bracket_cleaner(raw)\n",
    "#raw = cleaner(raw)\n",
    "\n",
    "sents = re.findall(u'([^！？。(――)(——)\\(\\)]+(」を.*)*(」と[^。]*)*(」、と[^。]*)*(？」と[^。]*)*[！？。」(……)]*)', raw)    \n",
    "\n",
    "\n",
    "#tokens = raw.split(' ')\n",
    "#for token in reversed(tokens):  #remove blank spaces; faster way to do it\n",
    "#        if token == '':\n",
    "#            tokens.remove(token)\n",
    "#token_counter = collections.Counter(tokens)\n",
    "#sorted_counts = sorted(token_counter.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
