{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### imports\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import codecs\n",
    "import sys\n",
    "import re\n",
    "from shutil import copyfile\n",
    "from __future__ import division\n",
    "#from __future__ import print_function\n",
    "#sys.stdout = codecs.getwriter('utf_8')(sys.stdout)\n",
    "#sys.stdin = codecs.getreader('utf_8')(sys.stdin)\n",
    "import MeCab  #CHECK \"MECABRC\" FILE TO SEE WHICH DICTIONARY YOU ARE USING\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic\n",
    "#mecab = MeCab.Tagger(\"-Ochasen\")  #using MeCab's ipadic\n",
    "import collections\n",
    "import operator\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.metrics import jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some cleaning and pre-processing functions for Japanese corpus\n",
    "\n",
    "#this first one should only be used for non-tokenized texts; basically cleans them for tokenization step\n",
    "def strip_chap_titles(raw):\n",
    "    #get rid of chapter titles that use Chinese numbers with or without surronding parantheses\n",
    "    raw = re.sub(r'（*([一二三四五六七八九十])+(）)*\\n', '', raw)\n",
    "    #get rid of chapter titles that use utf-8 alpha-numeric numbers\n",
    "    raw = re.sub(r'[１-９]+\\n', '', raw)\n",
    "    raw = re.sub(r'[第弐拾章参壱一二三四五六七八九十]+\\n', '', raw)\n",
    "    raw = re.sub(r'『', r'「', raw)   #replace all 『 with 「\n",
    "    raw = re.sub(r'』', r'」', raw)   #replace all 』 with 」\n",
    "    raw = re.sub(r'\\n', '', raw)  #strips all newlines\n",
    "    raw = re.sub(r'\\r', '', raw)  #strips all returns\n",
    "    #raw = re.sub(r'\\s', '', raw)\n",
    "    return raw\n",
    "\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','<','〔','〕',\n",
    "         '＃','△','※','＊']\n",
    "\n",
    "def cleaner(text):\n",
    "    for punc in puncs:\n",
    "        text = re.sub(punc, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)                         #get rid of double spaces\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    new_list = [token for token in tokens if token not in stopwords]\n",
    "    return new_list\n",
    "\n",
    "def get_stopwords(path):\n",
    "    f = open(path, encoding='utf-8')\n",
    "    words = f.read()\n",
    "    return re.split(r'\\n', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8867, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD in metadata for the kindai zasshi corpus\n",
    "\n",
    "df = pd.read_excel(r'Kindai_Meta.xlsx', sheetname='Sheet1')\n",
    "#df = df[df['YEAR'] < 1960]\n",
    "#df = df[df['YEAR'] > 1875]\n",
    "df = df[df['FILTER'] != 'YES']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>MAGAZINE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>GEO_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12373</th>\n",
       "      <td>12373.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-001_皇帝陛下の西京行幸_文語_b</td>\n",
       "      <td>212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12374</th>\n",
       "      <td>12374.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-002_長崎事件_文語_b</td>\n",
       "      <td>313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12375</th>\n",
       "      <td>12375.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-003_地方制度の改良_文語_b</td>\n",
       "      <td>507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>12376.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-004_畝傍艦何の辺に漂ふ_文語_b</td>\n",
       "      <td>131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12377</th>\n",
       "      <td>12377.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-005_国会議事堂_文語_b</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12378</th>\n",
       "      <td>12378.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-006_二十三年博覧会_文語_b</td>\n",
       "      <td>109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12379</th>\n",
       "      <td>12379.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-007_仏教の末路_文語_b</td>\n",
       "      <td>131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12380</th>\n",
       "      <td>12380.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-008_商業世界の波瀾_文語_b</td>\n",
       "      <td>435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12381</th>\n",
       "      <td>12381.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-009_鉄道事業_文語_b</td>\n",
       "      <td>245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12382</th>\n",
       "      <td>12382.txt</td>\n",
       "      <td>国民之友</td>\n",
       "      <td>1887</td>\n",
       "      <td>01-010_斉東野人の語に非らざる乎_文語_b</td>\n",
       "      <td>251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FILE_ID MAGAZINE  YEAR                     TITLE  LENGTH FILTER  \\\n",
       "12373  12373.txt     国民之友  1887     01-001_皇帝陛下の西京行幸_文語_b     212    NaN   \n",
       "12374  12374.txt     国民之友  1887          01-002_長崎事件_文語_b     313    NaN   \n",
       "12375  12375.txt     国民之友  1887       01-003_地方制度の改良_文語_b     507    NaN   \n",
       "12376  12376.txt     国民之友  1887     01-004_畝傍艦何の辺に漂ふ_文語_b     131    NaN   \n",
       "12377  12377.txt     国民之友  1887         01-005_国会議事堂_文語_b     100    NaN   \n",
       "12378  12378.txt     国民之友  1887       01-006_二十三年博覧会_文語_b     109    NaN   \n",
       "12379  12379.txt     国民之友  1887         01-007_仏教の末路_文語_b     131    NaN   \n",
       "12380  12380.txt     国民之友  1887       01-008_商業世界の波瀾_文語_b     435    NaN   \n",
       "12381  12381.txt     国民之友  1887          01-009_鉄道事業_文語_b     245    NaN   \n",
       "12382  12382.txt     国民之友  1887  01-010_斉東野人の語に非らざる乎_文語_b     251    NaN   \n",
       "\n",
       "       GEO_WORDS  \n",
       "12373        0.0  \n",
       "12374        3.0  \n",
       "12375        0.0  \n",
       "12376        1.0  \n",
       "12377        0.0  \n",
       "12378        0.0  \n",
       "12379        0.0  \n",
       "12380        0.0  \n",
       "12381        0.0  \n",
       "12382        0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Significant Clusters and Race Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "#if you're working on unidic tokenized corpus\n",
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\KindaiLemmaMerge\\\\\"\n",
    "\n",
    "race_term = '中国人'\n",
    "atom_terms = ['言う','思う','考える'] #,'知る','分かる']\n",
    "\n",
    "#race_term = '西洋人'\n",
    "#atom_terms =['目','顔','笑う','様子','じっと','頬','見詰める','表情','唇','微笑','顔色','眉','瞳','見上げる','目付き','顔付き']\n",
    "#atom_terms = ['歩く','遊ぶ','散歩']\n",
    "\n",
    "#race_term = '日本人'\n",
    "#atom_terms = ['精神','道徳','観念','倫理','道義'] \n",
    "\n",
    "#race_term = '土人'\n",
    "#atom_terms = ['写真']\n",
    "#atom_terms = ['寝る','眠る','床','起きる','布団','寝']\n",
    "#atom_terms = ['雨','降る','雪']\n",
    "\n",
    "window_size = 20\n",
    "rmv_stopwords = False\n",
    "\n",
    "if rmv_stopwords == True:\n",
    "    stopwords = get_stopwords(r'C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\stopwords.txt')\n",
    "\n",
    "#prepare file to print out results\n",
    "\n",
    "#f = open(r\"C:\\Users\\Hoyt\\Dropbox\\SemanticsRace\\Results\\KindaiChinesePoem.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "#set up data structure to hold info\n",
    "metadata = {'text_id': [], 'race_term': [], 'index_pos': [], 'left_tokens': [], 'right_tokens': []}\n",
    "\n",
    "hits = 0\n",
    "\n",
    "#iterate through all texts and find instances where any atom_terms appears within 20 characters of race term\n",
    "for k in df.index:\n",
    "    current_position = 0\n",
    "    \n",
    "    #get the text\n",
    "    text_id = str(df.FILE_ID[k])\n",
    "    title = df.TITLE[k]\n",
    "    magazine = df.MAGAZINE[k]\n",
    "    year = df.YEAR[k]\n",
    "    source_text = CORPUS_PATH + str(df.FILE_ID[k])\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    text = raw_text.read()\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = cleaner(text)\n",
    "    \n",
    "    #tokenize the text\n",
    "    tokens = re.split(r' ', text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    if rmv_stopwords == True:\n",
    "        tokens = remove_stopwords(tokens, stopwords)\n",
    "    \n",
    "    #iterate through tokens in search of seed_terms\n",
    "    for token in tokens:\n",
    "        #check for seed term\n",
    "        if token == race_term:\n",
    "            #get words on either side of race term\n",
    "            \n",
    "            #check to make sure we are not at start of text\n",
    "            if current_position - window_size > 0:\n",
    "                #left_tokens = ' '.join(tokens[current_position-window_size:current_position])\n",
    "                left_tokens = tokens[current_position-window_size:current_position]\n",
    "            else:\n",
    "                #left_tokens = ' '.join(tokens[0:current_position])\n",
    "                left_tokens = tokens[0:current_position]\n",
    "            \n",
    "            #check to make sure we are not at end of text\n",
    "            if current_position + window_size < len(tokens):\n",
    "                #right_tokens = ' '.join(tokens[current_position+1:current_position+window_size])\n",
    "                right_tokens = tokens[current_position+1:current_position+window_size]\n",
    "            else:\n",
    "                #right_tokens = ' '.join(tokens[current_position+1:])\n",
    "                right_tokens = tokens[current_position+1:]\n",
    "                \n",
    "            #check to see if any atom words are in left and right tokens\n",
    "            for word in atom_terms:\n",
    "                if word in left_tokens or word in right_tokens:\n",
    "                    metadata['text_id'].append(text_id)\n",
    "                    metadata['race_term'].append(race_term)\n",
    "                    metadata['index_pos'].append(current_position)\n",
    "                    metadata['left_tokens'].append(left_tokens)\n",
    "                    metadata['right_tokens'].append(right_tokens)\n",
    "                    \n",
    "#                     #print the results to a file\n",
    "#                     f.write(text_id + ' ' + magazine + ' ' + str(year) + ' ' + str(title) + ' ' + word + '\\n')\n",
    "#                     f.write(' '.join(left_tokens) + ' ' + race_term + ' ' + ' '.join(right_tokens) + '\\n\\n')\n",
    "                    \n",
    "#                     #try to grab the corresponding passage in non-lemmatized version of text\n",
    "#                     CORPUS_DIR = r\"C:\\Users\\Hoyt\\Documents\\MyData\\SemanticsRace\\kindai_original_raw\\\\\"\n",
    "#                     #merge target passage into a single string\n",
    "#                     target_passage = ' '.join(left_tokens) + ' ' + race_term + ' ' + ' '.join(right_tokens)\n",
    "#                     target_passage = re.split(r' ', target_passage)\n",
    "                    \n",
    "#                     #open the corresponding tokenized text\n",
    "#                     source = CORPUS_DIR + str(df.FILE_ID[k])\n",
    "#                     raw = open(source, encoding=\"utf-8\")       #grab text\n",
    "#                     new_text = raw.read()\n",
    "                    \n",
    "#                     sim_scores = {}\n",
    "                    \n",
    "#                     #tokenize the text\n",
    "#                     new_tokens = re.split(r' ', new_text)\n",
    "                    \n",
    "#                     sliding_window_size = 2 * window_size\n",
    "                    \n",
    "#                     #calcuate similarity across all passages of a given window size\n",
    "#                     for i in range(len(new_tokens) - (sliding_window_size - 1)):\n",
    "#                         source_passage = new_tokens[i:i+sliding_window_size]\n",
    "#                         j_score = jaccard_similarity_score(target_passage, source_passage)\n",
    "#                         sim_scores[i] = j_score\n",
    "                        \n",
    "#                     #now rank by similarity score\n",
    "#                     sim_df = DataFrame.from_dict(sim_scores, orient='index')  #convert dict to dataframe\n",
    "#                     sim_df = sim_df.rename(columns={0:'j_score'})    #rename column\n",
    "#                     sim_df = sim_df.sort_values(by='j_score', ascending=False)   #sort by frequency\n",
    "#                     #get top most similar passage\n",
    "#                     top_sim_index = sim_df.index[0:1].tolist()[0]           \n",
    "#                     #grab the corresponding window (plus 20 words before and after)\n",
    "#                     matching_passage = new_tokens[top_sim_index - 20:top_sim_index + 60]\n",
    "#                     f.write('---' + ''.join(matching_passage) + '\\n\\n')\n",
    "                    \n",
    "                    hits+=1\n",
    "                    break\n",
    "                    \n",
    "        current_position += 1\n",
    "\n",
    "f.close()\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['まで',\n",
       " '見通す',\n",
       " '様',\n",
       " 'だ',\n",
       " '笑う',\n",
       " '方',\n",
       " 'を',\n",
       " '為る',\n",
       " 'か',\n",
       " 'だ',\n",
       " '無い',\n",
       " 'ば',\n",
       " '例',\n",
       " 'の',\n",
       " 'にやにや',\n",
       " '笑い',\n",
       " 'だ',\n",
       " '有る',\n",
       " '此れ',\n",
       " 'も',\n",
       " '西洋人',\n",
       " 'に',\n",
       " 'は',\n",
       " '分かる',\n",
       " 'ない',\n",
       " '態度',\n",
       " 'らしい',\n",
       " 'が',\n",
       " '私',\n",
       " 'の',\n",
       " '所',\n",
       " 'へ',\n",
       " '舞い込む',\n",
       " 'て',\n",
       " '来る',\n",
       " '葉書',\n",
       " 'など',\n",
       " 'に',\n",
       " 'も',\n",
       " '返事']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                    #try to grab the corresponding passage in non-lemmatized version of text\n",
    "                    CORPUS_DIR = r\"C:\\Users\\Hoyt\\Dropbox\\JapanCorpusTokenized\\\\\"\n",
    "                    #merge target passage into a single string\n",
    "                    target_passage = left_tokens + ' ' + race_term + ' ' + right_tokens\n",
    "                    target_passage = re.split(r' ', target_passage)\n",
    "                    \n",
    "                    #open the corresponding tokenized text\n",
    "                    source = CORPUS_DIR + str(df.WORK_ID[k]) + \".txt\"\n",
    "                    raw = open(source, encoding=\"utf-8\")       #grab text\n",
    "                    new_text = raw.read()\n",
    "    \n",
    "                    sim_scores = {}\n",
    "                    \n",
    "                    new_text = bracket_cleaner(new_text)  #clean brackets for non-unicode kanji; need to do this first\n",
    "                    #remove punctuation\n",
    "                    #new_text = cleaner(new_text)\n",
    "    \n",
    "                    #tokenize the text\n",
    "                    new_tokens = re.split(r' ', new_text)\n",
    "                    \n",
    "                    sliding_window_size = 2 * window_size\n",
    "                    \n",
    "                    #calcuate similarity across all passages of a given window size\n",
    "                    for i in range(len(new_tokens) - (sliding_window_size - 1)):\n",
    "                        source_passage = new_tokens[i:i+sliding_window_size]\n",
    "                        j_score = jaccard_similarity_score(target_passage, source_passage)\n",
    "                        sim_scores[i] = j_score\n",
    "    \n",
    "                    #now rank by similarity score\n",
    "                    sim_df = DataFrame.from_dict(sim_scores, orient='index')  #convert dict to dataframe\n",
    "                    sim_df = sim_df.rename(columns={0:'j_score'})    #rename column\n",
    "                    sim_df = sim_df.sort_values(by='j_score', ascending=False)   #sort by frequency\n",
    "                    #get top most similar passage\n",
    "                    top_sim_index = sim_df.index[0:1].tolist()[0]           \n",
    "                    #grab the corresponding window (plus 20 words before and after)\n",
    "                    matching_passage = new_tokens[top_sim_index - 20:top_sim_index + 60]\n",
    "                    f.write('---' + ''.join(matching_passage) + '\\n\\n')\n",
    "                    \n",
    "                    hits += 1\n",
    "                    break\n",
    "                    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
