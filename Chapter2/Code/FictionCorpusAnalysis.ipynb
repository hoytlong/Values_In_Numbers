{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Running Analysis on Aozora Fiction Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "import re\n",
    "import MeCab\n",
    "#mecab = MeCab.Tagger('-Ochasen')\n",
    "mecab = MeCab.Tagger(\"\")  #using unidic\n",
    "import collections\n",
    "import operator\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Functions used for extracting features\n",
    "#########################################\n",
    "\n",
    "#this first one should only be used for non-tokenized texts; basically cleans them for tokenization step\n",
    "def strip_chap_titles(raw):\n",
    "    #get rid of chapter titles that use Chinese numbers with or without surronding parantheses\n",
    "    raw = re.sub(r'（*([一二三四五六七八九十])+(）)*\\n', '', raw)\n",
    "    \n",
    "    #get rid of chapter titles that use utf-8 alpha-numeric numbers\n",
    "    raw = re.sub(r'[１-９]+\\n', '', raw)\n",
    "    raw = re.sub(r'[第弐拾章参壱一二三四五六七八九十]+\\n', '', raw)\n",
    "    \n",
    "    #normalize all quotation marks to singl bracket\n",
    "    raw = re.sub(r'『', r'「', raw)   #replace all 『 with 「\n",
    "    raw = re.sub(r'』', r'」', raw)   #replace all 』 with 」\n",
    "    \n",
    "    #remove newlines and spaces\n",
    "    raw = re.sub(r'\\n', '', raw)  #strips all newlines\n",
    "    raw = re.sub(r'\\r', '', raw)  #strips all returns\n",
    "    raw = re.sub(r'\\s', '', raw)  #strip spaces if text is not already tokenized\n",
    "    \n",
    "    raw = re.sub(u'\\ufeff', '', raw)\n",
    "    return raw\n",
    "\n",
    "puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','<','〔','〕',\n",
    "         '＃','△','※','＊']\n",
    "\n",
    "def remove_punc(text):\n",
    "    for punc in puncs:\n",
    "        text = re.sub(punc, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)                         #get rid of double spaces\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    new_list = [token for token in tokens if token not in stopwords]\n",
    "    return new_list\n",
    "\n",
    "def get_stopwords(path):\n",
    "    f = open(path, encoding='utf-8')\n",
    "    words = f.read()\n",
    "    return re.split(r'\\n', words)\n",
    "\n",
    "def bracket_cleaner(raw):\n",
    "    raw = re.sub(r'［[^］]+］', '', raw)   #replace annotations in brackets ([#...])\n",
    "    raw = re.sub(r'\\s+', ' ', raw)                         #get rid of double spaces\n",
    "    return raw\n",
    "\n",
    "def count_kanji(raw):\n",
    "    kanji_count = len(re.findall(r'[\\u4E00-\\u9FEF]', raw))\n",
    "    not_kanji_count = len(re.findall(r'[^\\u4E00-\\u9FEF]', raw))\n",
    "    return kanji_count, not_kanji_count\n",
    "\n",
    "def count_punc(raw):\n",
    "    return len(re.findall(r'[、。！？]', raw))\n",
    "\n",
    "def count_tags(raw):\n",
    "    nouns = 0\n",
    "    verbs = 0\n",
    "    adj = 0\n",
    "    tokens = 0\n",
    "\n",
    "    adj_tags = ['形容詞','副詞','連体詞']\n",
    "    puncs = ['、','。','「','」','…','！','――','？','ゝ','『','』','（','）','／','＼','々','ーーー','］','・','ゞ','［','<','〔','〕',\n",
    "         '＃','△','※','＊']\n",
    "    \n",
    "    raw = re.sub(r' ', '', raw)\n",
    "\n",
    "    node = mecab.parseToNode(raw)\n",
    "    node = node.next\n",
    "\n",
    "    while node:\n",
    "        head_tag = re.split(r',', node.feature)[0]\n",
    "        if head_tag == \"名詞\":\n",
    "            nouns += 1\n",
    "        elif head_tag == \"動詞\":\n",
    "            verbs += 1\n",
    "        elif head_tag in adj_tags:\n",
    "            adj += 1\n",
    "\n",
    "        #exclude punctuation  from token count    \n",
    "        if head_tag not in puncs:\n",
    "            tokens += 1\n",
    "        \n",
    "        #go to next item\n",
    "        node = node.next\n",
    "    \n",
    "    return nouns, verbs, adj, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata and Select Texts to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1829, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in metadata spreadsheet for the corpus you are working with\n",
    "df = pd.read_excel(r'C:\\Users\\Hoyt\\Dropbox\\CodeDataForBook\\Chapter2\\Data\\Corpus_Metadata_Clean.xlsx', sheet_name='metadata')\n",
    "\n",
    "#select only the Aozora texts in your fiction corpus\n",
    "df = df[df['FICTION_CORPUS'] == True]\n",
    "df = df[df['SOURCE'] == 'aozora']\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\r"
     ]
    }
   ],
   "source": [
    "#add new columns to store collected features\n",
    "df['KANJI_COUNT'] = Series('',index=df.index)\n",
    "df['NON_KANJI_COUNT'] = Series('',index=df.index)\n",
    "df['PUNCT'] = Series('',index=df.index)\n",
    "df['NOUNS'] = Series('',index=df.index)\n",
    "df['VERBS'] = Series('',index=df.index)\n",
    "df['ADJS'] = Series('',index=df.index)\n",
    "df['N_RATIO'] = Series('',index=df.index)\n",
    "df['MVR'] = Series('',index=df.index)\n",
    "\n",
    "#point to where all tokenized texts are stored\n",
    "CORPUS_PATH = r\"C:\\Users\\Hoyt\\Dropbox\\JapanCorpusTokenized\\\\\"\n",
    "\n",
    "for k in df.index:\n",
    "    source_text = CORPUS_PATH + str(df.WORK_ID[k]) + \".txt\"\n",
    "    raw_text = open(source_text, encoding=\"utf-8\")       #grab text\n",
    "    raw = raw_text.read()\n",
    "    \n",
    "    raw = bracket_cleaner(raw)\n",
    "    \n",
    "    df.PUNCT.at[k] = count_punc(raw)\n",
    "    \n",
    "    #exclude punctuation before counting Kanji\n",
    "    no_punc_raw = remove_punc(raw)    \n",
    "    df.KANJI_COUNT.at[k], df.NON_KANJI_COUNT.at[k] = count_kanji(no_punc_raw)\n",
    "    \n",
    "    #counts POS tags on raw text\n",
    "    nouns, verbs, adj, tokens = count_tags(raw)\n",
    "    \n",
    "    df.NOUNS.at[k] = nouns\n",
    "    df.VERBS.at[k] = verbs\n",
    "    df.ADJS.at[k] = adj\n",
    "    df.N_RATIO.at[k] = 100 * (nouns/tokens)\n",
    "    df.MVR.at[k] = 100 * (adj/verbs)\n",
    "    \n",
    "    print(str(k), end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export results to an excel file\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\Hoyt\\Dropbox\\JAPAN_CORPUS\\Counts.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
